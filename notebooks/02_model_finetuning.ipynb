{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":77902,"status":"ok","timestamp":1746930774245,"user":{"displayName":"Nithin Keshavamurthy","userId":"17129339458019700030"},"user_tz":420},"id":"gqZU6zkFagqt","outputId":"4689fe2a-58b7-4f0c-8f8f-b4f1074bdb79"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m127.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["pip install -q bitsandbytes accelerate transformers datasets peft torch"]},{"cell_type":"markdown","metadata":{"id":"l_-pK4UobcvJ"},"source":["## Load Mistral-7B-Instruct in 4-bit"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yndhZed6cEMu"},"outputs":[],"source":["from huggingface_hub import login\n","from google.colab import userdata\n","\n","# Fetch your Hugging Face token from Colab secrets\n","hf_token = userdata.get(\"HF_TOKEN\")\n","\n","# Log in to Hugging Face\n","login(token=hf_token)"]},{"cell_type":"markdown","metadata":{"id":"kEGvhsO8jX5e"},"source":["Tokenize Prompt–Response Pairs for Causal LM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241,"referenced_widgets":["1fd90877809c4d3fa188208d482d471e","0f21ad596f35469382c9cb5a34afa5f6","71f67fb5ef374e968430265697902a2d","43ccb3d88a704af3bb2c5279e35a25f0","9cb4db9595774712a95d7bf07d36c5b9","b9694e9d89614df6bd6594d6cca75893","b4e962cde98a472f8f4777101055dd7d","0ae343eb44c14d5b9c0d72aa1e1485af","74383b60180f4bd79183cafeef38b69f","7ae9445a351a4f808289edf526ade9f0","a61712fe056d469bb2b2ee0cbe551405","ba8f12662e44493f913c00efbb1a040f","5b282bc24299412c9898678abb572bed","73d286ecc7754374a76742282182abe6","c13f40c1b3064c16be1ea8edee33ccf8","4af55c63d38c42889c3bc368c01331f1","f18378affc0b4a36ad62fe46bbf0cbe0","2d4777b6a6ac407ea0770d05ca2a0463","67ec3e58fe6d40eb954d61a824f88764","b4dfc7afa28e44f8b0b67c01f0ba8d48","305b27e4512343e89cb7dae487a35bab","8633d1a8f29241c3a35faa8e534cd0a2","3961b5a9b2194fae804f74f473d7570a","403e61620ca6478a86d308d6ae1bb511","a8c27af0ba2446b7badb36141c433678","4298287b165a4c6a8c7618bcbd077990","68a031a5321c4f8aab675d743e0b4194","22e29a8f9b4c48f8b51146af9e60c2ff","d7caff773345470fae32963b2d57bbd5","6f7ae8a0187d4108bc0ed0c4f3e3c625","c83cc202629f4698adf2f6efb2fa7719","5aab09e802d64408afbd2dc41909f41c","8add316c69d54eadb81b4b29dfda9834","6e1901e5291c4ab189349487cf3a7561","c087096f16874b4290b7766816de3178","91b8a5fc38954c7c81dfc9365cb9972f","c1bb0e4f92cf4827821869c8e84a451d","6b4a50af84204149a3b33820f248a0b6","aa78ded68d144fbcae450dd5230783b1","b8027b18e6f544e1bb267b93f20a09ad","22fc35b725ed4f0787f6525c198a1ef2","2bdf89de641b4515a12970f3bc565463","f897f199b876470c8a11ef6aef3aec4d","8e91b5d4c95b44faa9130b413a51ed58","bb9a970b351c48d5b2b6ff0d217c378d","2c013ad55c8849d79e4d02f293d94412","a364ac3947ae4a5abc0ed61e27adbe84","bc3ef9c61be4481babd58e28d46504c1","b26b0f4320a24496a7c16a6368c67950","0a06e2fb938c4317a0871d64c0195ce7","b03dca684c6b4fe2894178f833916d22","3bee6fa43d0d43f08317f4d06fa978b5","2bd2bca8d7ad4804ac2057c650cfecec","eeb12acb25fa4dcab7429df8aabf3d9e","99891fee5dea4a0c828a284c156aecef","7ba72f221a0648959029042fcd66907d","3566f3fe91c14e37984c27ddbf9005a3","49971dfc54f3474fbabd7643f95e3aa2","6bcc7ad499a34d2dbc00e2d045efacce","3db18f2bb64d468e8e2030c5e2bc4489","40283a9320a9452bb89bf67633ce4f10","12903c76c2ed414188c49c7c85973854","666057b15e2f46d5b107a313d42d6ae4","7908972bc9884675b584e71a90a73e38","19871a03a2824d01b4a054c3964daf7f","202bf597fca244eeb1bc1cd07c9bc61e","264b2dc6c0074b1ab91a0093183dc8b0","594fbb00ed1d489a8f092d5f67deb9ae","c9d61939b2af43f383ff75185d300b39","7e3bbb555d524d31b8ad8cc2f8f46619","f2c019dc2037458492c3e321a404e1c2","07f9ed2963664b99a85228a31bc13b0a","8a15dbc9558847cfb1c1d64b6a61a4a1","743c8b2360574e61a6b29d811619a14a","262d6c0f7d564e2889d7037ec910b459","111630b83f38422f87fd95ef74d2180c","6d6f11a2854d4cfb87f002cd75d8c8d9"]},"executionInfo":{"elapsed":84612,"status":"ok","timestamp":1746416493980,"user":{"displayName":"Nithin Keshavamurthy","userId":"17129339458019700030"},"user_tz":420},"id":"HemjLo-HaoJb","outputId":"97dda6ba-b8c5-4f79-c90b-a539c369b9af"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1fd90877809c4d3fa188208d482d471e","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ba8f12662e44493f913c00efbb1a040f","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3961b5a9b2194fae804f74f473d7570a","version_major":2,"version_minor":0},"text/plain":["Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6e1901e5291c4ab189349487cf3a7561","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bb9a970b351c48d5b2b6ff0d217c378d","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7ba72f221a0648959029042fcd66907d","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"264b2dc6c0074b1ab91a0093183dc8b0","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import BitsAndBytesConfig\n","import torch\n","from transformers import AutoModelForCausalLM\n","\n","# Define quantization configuration for 4-bit loading\n","quant_config = BitsAndBytesConfig(\n","    load_in_4bit=True,              # enable 4-bit quantization\n","    bnb_4bit_use_double_quant=True, # use nested quantization\n","    bnb_4bit_compute_dtype=torch.float16,  # compute dtype\n","    bnb_4bit_quant_type=\"nf4\"       # quantization type\n",")\n","\n","# Reload model with quantization config\n","model = AutoModelForCausalLM.from_pretrained(\n","    \"mistralai/Mistral-7B-Instruct-v0.1\",  # model name\n","    quantization_config=quant_config,     # updated quantization method\n","    device_map=\"auto\"                     # place model layers on available GPU\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":876,"status":"ok","timestamp":1746416494859,"user":{"displayName":"Nithin Keshavamurthy","userId":"17129339458019700030"},"user_tz":420},"id":"wH-QC1MCbgoD","outputId":"972961f7-31d8-4e1d-82ef-bad969ba712e"},"outputs":[{"name":"stdout","output_type":"stream","text":["trainable params: 6,815,744 || all params: 7,248,547,840 || trainable%: 0.0940\n"]}],"source":["from peft import LoraConfig, get_peft_model, TaskType\n","\n","# Enhanced LoRA configuration for A100 (40GB)\n","peft_config = LoraConfig(\n","    r=16,                         # increase LoRA rank for better expressiveness\n","    lora_alpha=32,               # scale factor adjusted for higher rank\n","    target_modules=[\"q_proj\", \"v_proj\"],  # attention projection layers\n","    lora_dropout=0.05,           # regularization\n","    bias=\"all\",                  # train bias parameters as well\n","    task_type=TaskType.CAUSAL_LM # task type for language modeling\n",")\n","\n","# Attach LoRA to base model\n","model = get_peft_model(model, peft_config)\n","\n","# Print updated trainable parameter count\n","model.print_trainable_parameters()"]},{"cell_type":"markdown","metadata":{"id":"5In1urv2jT-C"},"source":["Load Dataset and Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["49fbe299f4054b6f9b1697a30c9a254d","bb0acead0dbc4054bf1d0d5cfe0bee53","4cac8726bc1441689305d629aced7432","68894f169f224c76893717dc96acbcea","a97c761270b14fbd956b80a0c617824b","75212ec16971472ab3951d05761f453b","b64247d2a080466e9349222342cc9c13","668f216dc79845fc90082c8598bafda3","08053ea2f8df444c9df1d0822c2efb25","75b35b9e20cc42f189ad40cb1f0fc7e1","cee9a0dde2ec4ef7b42e6dad83e122e0","1ab7fbbe6b2f495a9f12b2eb3851d76c","0cbcad0a37f74db6b020ba59e5032ed8","2c9b98ef8888475b807c4aaf5ba5263b","d2e6433f5c5042c38216a3056355730e","e60e10a138f3490da1b6cd569ae491d3","00afba2659c84efeb0394178d369ce2c","e2570f5c57d24733a9398856f330b502","b9f2653d840448959b890c7c21089e45","6d1e4d70f4824ee28ae8b596783e904f","8984bcb2dfd147fb80038daa8498ac27","2aaa370015fb49e5ac8328353e0bdbfa","8db3e662f139486b994788bb8dc3bf3c","7a894847dc1643078751dc8b9c564181","ac7638fe7bb54bf9884020d16675aff9","92c92d55fcac4969829e42b7554074d0","2ad6cb8b52ad422d8d49bd4f1197f964","b2bda070080146aea8a248a34737690b","cbf3344ee6a54075a647e73e28506d93","52b0617b9364437485879fed5f37139b","844fb2dbe0724d9d9838e6b13788e791","2d0eddb9678e497193bd63f9dbba6fd7","8c29bf3515f44aa59b90628591145d08","9c2c72f3d53440e3b7fb0f7c2f04b3e1","9399851214fb40799909a0c5eb7e07eb","e81ae95c48d54895b77748d145887f8f","c8ee929e264142868a8776cafed21b17","9ed02748d22a43aa8121b17093974312","1f2ec9c292c349bebcb1364c2479ed6b","96ff2624aa6c45f2bcb7e06662d01d38","bc78026910314080873bd06b0b4934ce","c1986e65eba64f008ffdae264c92f904","ff1dc317a75c4b418e67433b29f8ca7f","58cd6ff3599d4bddb49a9d3ecd23225b"]},"executionInfo":{"elapsed":2327,"status":"ok","timestamp":1746416613194,"user":{"displayName":"Nithin Keshavamurthy","userId":"17129339458019700030"},"user_tz":420},"id":"QhXTc8roiji7","outputId":"72cfe017-909a-4cfc-ed3a-91b0611d6e44"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"49fbe299f4054b6f9b1697a30c9a254d","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1ab7fbbe6b2f495a9f12b2eb3851d76c","version_major":2,"version_minor":0},"text/plain":["tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8db3e662f139486b994788bb8dc3bf3c","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9c2c72f3d53440e3b7fb0f7c2f04b3e1","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import json\n","from transformers import AutoTokenizer\n","\n","# Load formatted dataset from Drive\n","with open(\"/content/drive/Shareddrives/DATA266_Project/Data/spider_data/spider_instruction_data.json\", \"r\") as f:\n","    dataset = json.load(f)\n","\n","# Reload tokenizer for Mistral\n","tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")"]},{"cell_type":"markdown","metadata":{"id":"JZGE3F1AjQD4"},"source":[" Set pad_token for Mistral Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JQyAMOOPjAHQ"},"outputs":[],"source":["# Set the pad_token to eos_token (common fix for decoder-only models)\n","tokenizer.pad_token = tokenizer.eos_token"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a6nGmDVJgmlq"},"outputs":[],"source":["from transformers import PreTrainedTokenizerBase\n","\n","# Tokenize each example into input_ids and labels\n","def tokenize_sample(sample):\n","    # Combine prompt and response for causal LM\n","    full_prompt = sample[\"prompt\"] + \"\\n\" + sample[\"response\"]\n","\n","    # Tokenize the entire string\n","    tokenized = tokenizer(\n","        full_prompt,\n","        truncation=True,\n","        padding=\"max_length\",\n","        max_length=512,\n","        return_tensors=\"pt\"\n","    )\n","\n","    # Labels are the same as input_ids for causal LM\n","    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n","    return {key: val.squeeze() for key, val in tokenized.items()}\n","\n","# Apply tokenizer to dataset\n","tokenized_dataset = [tokenize_sample(example) for example in dataset]"]},{"cell_type":"markdown","metadata":{"id":"QTsAwIJhjM6t"},"source":[" Wrap Tokenized Data in a Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PaZFOP6sibdQ"},"outputs":[],"source":["from torch.utils.data import Dataset\n","\n","# Define a simple custom dataset class\n","class PromptDataset(Dataset):\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            \"input_ids\": self.data[idx][\"input_ids\"],\n","            \"attention_mask\": self.data[idx][\"attention_mask\"],\n","            \"labels\": self.data[idx][\"labels\"]\n","        }\n","\n","# Create dataset instance\n","train_dataset = PromptDataset(tokenized_dataset)"]},{"cell_type":"markdown","metadata":{"id":"PA-6R8aDjs7B"},"source":["Set Up Training Arguments and Trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UKRLSss1jKuM"},"outputs":[],"source":["from transformers import TrainingArguments, Trainer\n","from torch.utils.data import DataLoader\n","import os\n","\n","# Define training output directory\n","output_dir = \"/content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_spider_lora\"\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=output_dir,             # directory to save checkpoints\n","    per_device_train_batch_size=4,     # batch size per device\n","    gradient_accumulation_steps=4,     # effective batch size = 4 x 4 = 16\n","    num_train_epochs=3,                # number of epochs\n","    learning_rate=2e-4,                # learning rate\n","    fp16=True,                         # use float16 training\n","    logging_steps=10,                  # log every 10 steps\n","    save_strategy=\"epoch\",             # save at each epoch\n","    report_to=\"none\"                   # no W&B or TensorBoard\n",")"]},{"cell_type":"markdown","metadata":{"id":"bS1NXLbBjvy0"},"source":[" Initialize Trainer and Start Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2464813,"status":"ok","timestamp":1746419370382,"user":{"displayName":"Nithin Keshavamurthy","userId":"17129339458019700030"},"user_tz":420},"id":"usCXR9cXjnMP","outputId":"24acf15c-68ed-4c85-df9d-c43036f7278c"},"outputs":[{"name":"stderr","output_type":"stream","text":["No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1311' max='1311' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1311/1311 41:01, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>10</td>\n","      <td>0.658000</td>\n","    </tr>\n","    <tr>\n","      <td>20</td>\n","      <td>0.474600</td>\n","    </tr>\n","    <tr>\n","      <td>30</td>\n","      <td>0.392300</td>\n","    </tr>\n","    <tr>\n","      <td>40</td>\n","      <td>0.355100</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.305800</td>\n","    </tr>\n","    <tr>\n","      <td>60</td>\n","      <td>0.273800</td>\n","    </tr>\n","    <tr>\n","      <td>70</td>\n","      <td>0.244400</td>\n","    </tr>\n","    <tr>\n","      <td>80</td>\n","      <td>0.211100</td>\n","    </tr>\n","    <tr>\n","      <td>90</td>\n","      <td>0.205100</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.164000</td>\n","    </tr>\n","    <tr>\n","      <td>110</td>\n","      <td>0.160100</td>\n","    </tr>\n","    <tr>\n","      <td>120</td>\n","      <td>0.160300</td>\n","    </tr>\n","    <tr>\n","      <td>130</td>\n","      <td>0.136200</td>\n","    </tr>\n","    <tr>\n","      <td>140</td>\n","      <td>0.136100</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.133000</td>\n","    </tr>\n","    <tr>\n","      <td>160</td>\n","      <td>0.125300</td>\n","    </tr>\n","    <tr>\n","      <td>170</td>\n","      <td>0.121900</td>\n","    </tr>\n","    <tr>\n","      <td>180</td>\n","      <td>0.119100</td>\n","    </tr>\n","    <tr>\n","      <td>190</td>\n","      <td>0.109300</td>\n","    </tr>\n","    <tr>\n","      <td>200</td>\n","      <td>0.100500</td>\n","    </tr>\n","    <tr>\n","      <td>210</td>\n","      <td>0.104300</td>\n","    </tr>\n","    <tr>\n","      <td>220</td>\n","      <td>0.100600</td>\n","    </tr>\n","    <tr>\n","      <td>230</td>\n","      <td>0.105700</td>\n","    </tr>\n","    <tr>\n","      <td>240</td>\n","      <td>0.098500</td>\n","    </tr>\n","    <tr>\n","      <td>250</td>\n","      <td>0.095800</td>\n","    </tr>\n","    <tr>\n","      <td>260</td>\n","      <td>0.094400</td>\n","    </tr>\n","    <tr>\n","      <td>270</td>\n","      <td>0.093100</td>\n","    </tr>\n","    <tr>\n","      <td>280</td>\n","      <td>0.096100</td>\n","    </tr>\n","    <tr>\n","      <td>290</td>\n","      <td>0.097500</td>\n","    </tr>\n","    <tr>\n","      <td>300</td>\n","      <td>0.092000</td>\n","    </tr>\n","    <tr>\n","      <td>310</td>\n","      <td>0.095200</td>\n","    </tr>\n","    <tr>\n","      <td>320</td>\n","      <td>0.090000</td>\n","    </tr>\n","    <tr>\n","      <td>330</td>\n","      <td>0.090900</td>\n","    </tr>\n","    <tr>\n","      <td>340</td>\n","      <td>0.091400</td>\n","    </tr>\n","    <tr>\n","      <td>350</td>\n","      <td>0.094400</td>\n","    </tr>\n","    <tr>\n","      <td>360</td>\n","      <td>0.086200</td>\n","    </tr>\n","    <tr>\n","      <td>370</td>\n","      <td>0.089200</td>\n","    </tr>\n","    <tr>\n","      <td>380</td>\n","      <td>0.083600</td>\n","    </tr>\n","    <tr>\n","      <td>390</td>\n","      <td>0.086100</td>\n","    </tr>\n","    <tr>\n","      <td>400</td>\n","      <td>0.088000</td>\n","    </tr>\n","    <tr>\n","      <td>410</td>\n","      <td>0.086600</td>\n","    </tr>\n","    <tr>\n","      <td>420</td>\n","      <td>0.085600</td>\n","    </tr>\n","    <tr>\n","      <td>430</td>\n","      <td>0.085500</td>\n","    </tr>\n","    <tr>\n","      <td>440</td>\n","      <td>0.082600</td>\n","    </tr>\n","    <tr>\n","      <td>450</td>\n","      <td>0.085500</td>\n","    </tr>\n","    <tr>\n","      <td>460</td>\n","      <td>0.080800</td>\n","    </tr>\n","    <tr>\n","      <td>470</td>\n","      <td>0.081500</td>\n","    </tr>\n","    <tr>\n","      <td>480</td>\n","      <td>0.079300</td>\n","    </tr>\n","    <tr>\n","      <td>490</td>\n","      <td>0.086800</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.085000</td>\n","    </tr>\n","    <tr>\n","      <td>510</td>\n","      <td>0.078900</td>\n","    </tr>\n","    <tr>\n","      <td>520</td>\n","      <td>0.081500</td>\n","    </tr>\n","    <tr>\n","      <td>530</td>\n","      <td>0.079600</td>\n","    </tr>\n","    <tr>\n","      <td>540</td>\n","      <td>0.081600</td>\n","    </tr>\n","    <tr>\n","      <td>550</td>\n","      <td>0.080800</td>\n","    </tr>\n","    <tr>\n","      <td>560</td>\n","      <td>0.083800</td>\n","    </tr>\n","    <tr>\n","      <td>570</td>\n","      <td>0.078900</td>\n","    </tr>\n","    <tr>\n","      <td>580</td>\n","      <td>0.083800</td>\n","    </tr>\n","    <tr>\n","      <td>590</td>\n","      <td>0.080700</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>0.079600</td>\n","    </tr>\n","    <tr>\n","      <td>610</td>\n","      <td>0.080100</td>\n","    </tr>\n","    <tr>\n","      <td>620</td>\n","      <td>0.078200</td>\n","    </tr>\n","    <tr>\n","      <td>630</td>\n","      <td>0.079400</td>\n","    </tr>\n","    <tr>\n","      <td>640</td>\n","      <td>0.083700</td>\n","    </tr>\n","    <tr>\n","      <td>650</td>\n","      <td>0.082300</td>\n","    </tr>\n","    <tr>\n","      <td>660</td>\n","      <td>0.078100</td>\n","    </tr>\n","    <tr>\n","      <td>670</td>\n","      <td>0.081900</td>\n","    </tr>\n","    <tr>\n","      <td>680</td>\n","      <td>0.077200</td>\n","    </tr>\n","    <tr>\n","      <td>690</td>\n","      <td>0.078300</td>\n","    </tr>\n","    <tr>\n","      <td>700</td>\n","      <td>0.079600</td>\n","    </tr>\n","    <tr>\n","      <td>710</td>\n","      <td>0.080900</td>\n","    </tr>\n","    <tr>\n","      <td>720</td>\n","      <td>0.079300</td>\n","    </tr>\n","    <tr>\n","      <td>730</td>\n","      <td>0.079500</td>\n","    </tr>\n","    <tr>\n","      <td>740</td>\n","      <td>0.076000</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>0.076000</td>\n","    </tr>\n","    <tr>\n","      <td>760</td>\n","      <td>0.080200</td>\n","    </tr>\n","    <tr>\n","      <td>770</td>\n","      <td>0.077700</td>\n","    </tr>\n","    <tr>\n","      <td>780</td>\n","      <td>0.075600</td>\n","    </tr>\n","    <tr>\n","      <td>790</td>\n","      <td>0.077400</td>\n","    </tr>\n","    <tr>\n","      <td>800</td>\n","      <td>0.072500</td>\n","    </tr>\n","    <tr>\n","      <td>810</td>\n","      <td>0.078500</td>\n","    </tr>\n","    <tr>\n","      <td>820</td>\n","      <td>0.080700</td>\n","    </tr>\n","    <tr>\n","      <td>830</td>\n","      <td>0.075000</td>\n","    </tr>\n","    <tr>\n","      <td>840</td>\n","      <td>0.073700</td>\n","    </tr>\n","    <tr>\n","      <td>850</td>\n","      <td>0.075700</td>\n","    </tr>\n","    <tr>\n","      <td>860</td>\n","      <td>0.073300</td>\n","    </tr>\n","    <tr>\n","      <td>870</td>\n","      <td>0.072700</td>\n","    </tr>\n","    <tr>\n","      <td>880</td>\n","      <td>0.068200</td>\n","    </tr>\n","    <tr>\n","      <td>890</td>\n","      <td>0.073400</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>0.070400</td>\n","    </tr>\n","    <tr>\n","      <td>910</td>\n","      <td>0.069600</td>\n","    </tr>\n","    <tr>\n","      <td>920</td>\n","      <td>0.070900</td>\n","    </tr>\n","    <tr>\n","      <td>930</td>\n","      <td>0.069500</td>\n","    </tr>\n","    <tr>\n","      <td>940</td>\n","      <td>0.071300</td>\n","    </tr>\n","    <tr>\n","      <td>950</td>\n","      <td>0.074000</td>\n","    </tr>\n","    <tr>\n","      <td>960</td>\n","      <td>0.068600</td>\n","    </tr>\n","    <tr>\n","      <td>970</td>\n","      <td>0.071300</td>\n","    </tr>\n","    <tr>\n","      <td>980</td>\n","      <td>0.069500</td>\n","    </tr>\n","    <tr>\n","      <td>990</td>\n","      <td>0.069800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.071600</td>\n","    </tr>\n","    <tr>\n","      <td>1010</td>\n","      <td>0.070200</td>\n","    </tr>\n","    <tr>\n","      <td>1020</td>\n","      <td>0.068700</td>\n","    </tr>\n","    <tr>\n","      <td>1030</td>\n","      <td>0.068300</td>\n","    </tr>\n","    <tr>\n","      <td>1040</td>\n","      <td>0.068200</td>\n","    </tr>\n","    <tr>\n","      <td>1050</td>\n","      <td>0.070200</td>\n","    </tr>\n","    <tr>\n","      <td>1060</td>\n","      <td>0.070400</td>\n","    </tr>\n","    <tr>\n","      <td>1070</td>\n","      <td>0.070300</td>\n","    </tr>\n","    <tr>\n","      <td>1080</td>\n","      <td>0.071900</td>\n","    </tr>\n","    <tr>\n","      <td>1090</td>\n","      <td>0.067300</td>\n","    </tr>\n","    <tr>\n","      <td>1100</td>\n","      <td>0.071200</td>\n","    </tr>\n","    <tr>\n","      <td>1110</td>\n","      <td>0.070300</td>\n","    </tr>\n","    <tr>\n","      <td>1120</td>\n","      <td>0.065900</td>\n","    </tr>\n","    <tr>\n","      <td>1130</td>\n","      <td>0.067800</td>\n","    </tr>\n","    <tr>\n","      <td>1140</td>\n","      <td>0.068800</td>\n","    </tr>\n","    <tr>\n","      <td>1150</td>\n","      <td>0.074000</td>\n","    </tr>\n","    <tr>\n","      <td>1160</td>\n","      <td>0.067800</td>\n","    </tr>\n","    <tr>\n","      <td>1170</td>\n","      <td>0.067200</td>\n","    </tr>\n","    <tr>\n","      <td>1180</td>\n","      <td>0.068100</td>\n","    </tr>\n","    <tr>\n","      <td>1190</td>\n","      <td>0.067100</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>0.068600</td>\n","    </tr>\n","    <tr>\n","      <td>1210</td>\n","      <td>0.069500</td>\n","    </tr>\n","    <tr>\n","      <td>1220</td>\n","      <td>0.070800</td>\n","    </tr>\n","    <tr>\n","      <td>1230</td>\n","      <td>0.070300</td>\n","    </tr>\n","    <tr>\n","      <td>1240</td>\n","      <td>0.068900</td>\n","    </tr>\n","    <tr>\n","      <td>1250</td>\n","      <td>0.069700</td>\n","    </tr>\n","    <tr>\n","      <td>1260</td>\n","      <td>0.067200</td>\n","    </tr>\n","    <tr>\n","      <td>1270</td>\n","      <td>0.064900</td>\n","    </tr>\n","    <tr>\n","      <td>1280</td>\n","      <td>0.067300</td>\n","    </tr>\n","    <tr>\n","      <td>1290</td>\n","      <td>0.071900</td>\n","    </tr>\n","    <tr>\n","      <td>1300</td>\n","      <td>0.072400</td>\n","    </tr>\n","    <tr>\n","      <td>1310</td>\n","      <td>0.069700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=1311, training_loss=0.10129983225321607, metrics={'train_runtime': 2464.4129, 'train_samples_per_second': 8.521, 'train_steps_per_second': 0.532, 'total_flos': 4.582880059588608e+17, 'train_loss': 0.10129983225321607, 'epoch': 2.994285714285714})"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["from transformers import Trainer\n","\n","# Initialize Trainer with model, training args, and dataset\n","trainer = Trainer(\n","    model=model,                         # LoRA-wrapped Mistral model\n","    args=training_args,                  # training configuration\n","    train_dataset=train_dataset,         # our prompt-tokenized dataset\n",")\n","\n","# Start training\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241,"referenced_widgets":["57e76f642aa1462286f414e7bea8fbc2","4ed764a3d85b47aca7efc5e211f4310b","f43294d261ad44079e629ac7e6230fe2","ff84ef2b27874632ad20e5c67f147642","f3ebbaa8308448f59701026b6e3bb2be","7b8feb8f2cd744a384b2209cd521a7f1","6aad7e933ade46208bc4f5d5e445189f","37cd8e0fc3d946d19e67f0837ddb0cfa","fbecc65cb11e49e1b6a938c9c1d33495","8c5001403647468baf40f3baf7e7de7f","1ff99957d87744e89b551e4591cc506b","53ac75ce9e36472d9927f081bdea63cb","f7df2463aaf645af8272efb3b6005272","dd8c6be9107341929774d33c1bb8eaab","76f641deb3664f3bacae3db0207f354a","550e741da721446d9054c8791fe80226","869193acdbe64fef90244877113e8acf","9e06c025a31a4d618c046fe0ed346536","7ebf961332e84f30a5ac2460e159f10f","a0ace417ec6546cc82b7a0707b0a7568","e8350f7a6f6849c2b3ed6e554ac47f39","d5417346ff71485aa4a20ccab21fa863","171c55a07689492683d628186000ad9a","efb94644f94c4b918288b446113d71e9","ede67b2da89f496c8b8ea215381dd033","516dba2f22574e9cbccf4a8805c2a1fc","ae5fe91105a0415fa45a9d777b531561","b463641b455447039c543cd946e41e54","1516acf51bc44e51a95648f065cb11fa","05103c7ac83f4de3b77d3f3c6f35343b","079ed7cfb8314fceb0eb5ee97217dc93","f531854dc6b942b8ad7ada5212535615","79873f151e074c2ab0d9af4abc2b815e","8110f7f5d424481dadc6c0293632109b","b4fbfeb442174b6395fd3a9d31e78b9d","45f8616d6f4241f7931d27e45dc1477b","7fbe3ab6050f4d4189e6fc08363d81a1","18d2548f09fe4e888f6ef6bedaa72aea","6db982977a4f4fafbb8e890cdb17b68f","dd87f2dfbda74d939fe2679b5e190e80","934d24ceb6b14326a973d7da34d56e14","6081b74a7a124e36aca740e89012a14b","2a7076f701da41eeaf41dacc0a9181f2","99791a5b668b473c9d89b81c1d5e0d61","00d2fde3440e4ce9ab7bb192844df86c","a235c24fc0a54a8ca862fad5cd2628d7","06e5a1455128437680f9ddcc4f6d5d8f","4521c387d5d14183b8f9d12622e5babd","2d26084c66a1406b822a75b7762f93e9","208402e4aa9c43518bfde6a8c7fd5c8e","06ee60dd2d004ad4a7323eafef2b61f6","dd1a1637768443b880f85f043fbf0b09","82e321797fe54f329a86625abe5dd659","dd773445f75c45e68cef6b31639957af","84603fa02e6646ebba3e8d1c1f9a22cc","bbb40760b25842eba2faf41f092429f2","db74ed0735c44376a8ad184236a9b862","a3ae599d752242c9a047012cc725606a","9e129efce33246f4b52ec887f61956aa","8bb957cc36d5445b8090e8450ab436db","e6fef3e69e38435bac7d7887c0fb17ef","ef53355a1c5648279c4bfc987f9164be","9eda7d6c71224b63871b5e8ec6ea3d0f","f907fae1c2e74dab8439be26d0e46138","e2b1c4797827417aba95652af02970c2","ce619f7c363743a1a1f851dc4aca58d5","0c13050e213f420587733f5c48ff6c45","a91caf45c46d486fabed3bc98b660584","9c5d23db00d14c7885f59dfe5f261f25","5bf1d2d4dbd04ea4aa98f00fc23a480f","fb1ab124747c4f33a0bd9bd0b2d09cec","23083db9243546e2b8fbefb1113a826b","b12aa2ed67d04c1c8c31e624081cb16c","414480d2fd6c44ec943882b87fcaa71d","f2d64957e8c3424d831e1233b70b2062","6e2175fc59cf458ab55cc0c5f7d940c0","d4d5ba6783994e57805cb0f81bcd12d3"]},"executionInfo":{"elapsed":70936,"status":"ok","timestamp":1746930938406,"user":{"displayName":"Nithin Keshavamurthy","userId":"17129339458019700030"},"user_tz":420},"id":"R76Csx8syHXm","outputId":"a9f8565e-8f23-4f6b-9c68-2c731b10872e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57e76f642aa1462286f414e7bea8fbc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53ac75ce9e36472d9927f081bdea63cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"171c55a07689492683d628186000ad9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8110f7f5d424481dadc6c0293632109b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00d2fde3440e4ce9ab7bb192844df86c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbb40760b25842eba2faf41f092429f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c13050e213f420587733f5c48ff6c45"}},"metadata":{}}],"source":["from transformers import AutoModelForCausalLM\n","import torch\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    \"mistralai/Mistral-7B-Instruct-v0.1\",\n","    device_map=\"auto\",\n","    torch_dtype=torch.float16  # or .bfloat16 based on your runtime\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["4a3f359de7f544079ffe42da9391171f","ab9aefd400a148ec81bbc07be130c8ff","ed8aab732d934226ae5c2d771d4fadf6","b66771093eff4c7590a216d55f14b5e5","3ce709d2010f40fcbbcf9c4193668f61","16520616b28943728f42c4bdc80e415c","1ac6c7ec63c94ca1b59d34bfda2e85d5","0dfe83bcd9b044979f8c4a204ab7ea17","35f1587df63f49e795627c80c3da998d","06f4439d8056410981fce1d63bb0dc2d","c0fb6d4abf8f40bcae015c9df32297f9","cd748f04f3bd48f28de2146c328fb9d7","91434fd6bc3e4080b7c801b9d9a20bfb","acdd42f714fe4fa288b7ae18963e1a47","e85c5698c8ca4139a77fc42f8b8513ff","d51026308aa042f7804dbccb14717bbe","c9b92a25ccc149ad8357246e60e8acee","0bd83c226c53485ab87a503370462de7","6c140af28974443e80b077821928691f","53000cde2e3e40ebb27925ec54f5fd54","32c68e9ef8c941b08a4a0ee021b7161a","7540f59215ca4d589cad8bc876bf59d3","816a9c5ce7f147e2829a8d797a827222","184fa66ad80540c3951e9de0b3f8376e","4e84dc56235341b49c9c2bed8a1d86e4","6ca722bf80cd403eb689d0302f52a28b","d1a452030d63469e85b34a7aff108435","b14d65eb4b1d4b72b085b9d5e36359ec","945d37cdf60743559ae91a2940b130ba","c3223638fcb44f3880d4c4b20c0eda27","41b23cff945b44c4a89ba37188e34090","588f201fd5cc4090af398a1091f21be5","0430e059cf2e4871a8a983b4195d51f6","e33c008d387a4741bbe6b1d3fd1c1f54","469bf1a1ff7f403fa090db0c29677784","bf7052dd23e945889ae09367e07b7d1f","9bb5bac46f8b4b7c853010b5b529ebd1","c11c416041e54bc1abb0250fb754d78f","76397838ca244ebbb101fef83b921190","d56d2d84b4f44e03b23b4e9d1928d07c","8b4b31fc1a0d4270a90c98b498feb09b","c30ce2d604cf4b10a6abeb86055463ac","fc3ca9901d504a29b9ed19b570f11d61","37e497f27de44846a34ae3bd46fed312"]},"executionInfo":{"elapsed":2642,"status":"ok","timestamp":1746930947946,"user":{"displayName":"Nithin Keshavamurthy","userId":"17129339458019700030"},"user_tz":420},"id":"6VEzvUPPztQv","outputId":"ba54cd7d-2128-405f-b5b4-683e26677188"},"outputs":[{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a3f359de7f544079ffe42da9391171f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd748f04f3bd48f28de2146c328fb9d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"816a9c5ce7f147e2829a8d797a827222"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e33c008d387a4741bbe6b1d3fd1c1f54"}},"metadata":{}}],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n","tokenizer.pad_token = tokenizer.eos_token  # required for generation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QiYKB0vmjyzu"},"outputs":[],"source":["from peft import PeftModel\n","\n","# Load your fine-tuned adapter into the base model\n","peft_model = PeftModel.from_pretrained(\n","    base_model,\n","    \"/content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_spider_lora/checkpoint-1311\"\n",")\n","# Merge LoRA into base model\n","merged_model = peft_model.merge_and_unload()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":96385,"status":"ok","timestamp":1746555398528,"user":{"displayName":"Nithin Keshavamurthy","userId":"17129339458019700030"},"user_tz":420},"id":"QL_2UWZpxgLE","outputId":"1797f17e-f3ef-45cb-f409-d1d624ece532"},"outputs":[{"data":{"text/plain":["('/content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_merged_full/tokenizer_config.json',\n"," '/content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_merged_full/special_tokens_map.json',\n"," '/content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_merged_full/tokenizer.model',\n"," '/content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_merged_full/added_tokens.json',\n"," '/content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_merged_full/tokenizer.json')"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["merged_model.save_pretrained(\"/content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_merged_full\")\n","tokenizer.save_pretrained(\"/content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_merged_full\")"]},{"cell_type":"code","source":["from peft import PeftModel\n","\n","# Load your fine-tuned adapter into the base model\n","peft_model = PeftModel.from_pretrained(\n","    base_model,\n","    \"/content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_complex_sql_training_4090/checkpoint-14500\"\n",")\n","# Merge LoRA into base model\n","merged_model = peft_model.merge_and_unload()"],"metadata":{"id":"r_NQErrjLg_5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["merged_model.save_pretrained(\"/content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_merged_full_4090\")\n","tokenizer.save_pretrained(\"/content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_merged_full_4090\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-WMUD5_CM2Sh","executionInfo":{"status":"ok","timestamp":1746931076680,"user_tz":420,"elapsed":60160,"user":{"displayName":"Nithin Keshavamurthy","userId":"17129339458019700030"}},"outputId":"8fe50a32-20ad-43b1-b3c1-047083a36cc7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('/content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_merged_full_4090/tokenizer_config.json',\n"," '/content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_merged_full_4090/special_tokens_map.json',\n"," '/content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_merged_full_4090/tokenizer.model',\n"," '/content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_merged_full_4090/added_tokens.json',\n"," '/content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_merged_full_4090/tokenizer.json')"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"P-tDtPw28kNQ"},"source":["## seeklhy/SynSQL-2.5M - Fine tuning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tN6PldzwLUWa"},"outputs":[],"source":["import torch\n","import json\n","import os\n","from datasets import load_dataset\n","from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n","from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n","from torch.utils.data import Dataset\n","from huggingface_hub import login\n","from google.colab import userdata\n","import re\n","import random\n","from tqdm.auto import tqdm\n","\n","# Set random seed for reproducibility\n","torch.manual_seed(42)\n","random.seed(42)\n","\n","# Fetch your Hugging Face token from Colab secrets (if needed)\n","hf_token = userdata.get(\"HF_TOKEN\")\n","if hf_token:\n","    login(token=hf_token)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Og0Zp2y3UOYA"},"outputs":[],"source":["def is_complex_query(sql_query):\n","    #Determine if a SQL query is complex based on specific patterns.\n","    sql_lower = sql_query.lower()\n","\n","    # Check for multiple joins\n","    join_count = sql_lower.count(\" join \")\n","\n","    # Check for subqueries\n","    has_subquery = \"select\" in sql_lower[sql_lower.find(\"from\"):] if \"from\" in sql_lower else False\n","\n","    # Check for complex functions and operations\n","    complex_patterns = [\n","        \"group by\", \"having\", \"order by limit\", \"distinct\",\n","        \"case when\", \"with\", \"union\", \"intersect\", \"except\",\n","        \"sum(\", \"avg(\", \"max(\", \"min(\", \"count(\"\n","    ]\n","    has_complex_funcs = any(pattern in sql_lower for pattern in complex_patterns)\n","\n","    # Check for multiple conditions\n","    condition_count = sql_lower.count(\" and \") + sql_lower.count(\" or \")\n","\n","    # Count the number of tables involved (approximate)\n","    from_parts = sql_lower.split(\"from\")[1:]\n","    table_references = 0\n","    for part in from_parts:\n","        if \"where\" in part:\n","            part = part.split(\"where\")[0]\n","        elif \"group by\" in part:\n","            part = part.split(\"group by\")[0]\n","        table_references += part.count(\",\") + 1  # Count commas + 1 for table references\n","\n","    # Define complexity threshold\n","    is_complex = (join_count >= 1) or has_subquery or (condition_count >= 2) or \\\n","                 has_complex_funcs or (table_references >= 2)\n","\n","    return is_complex"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":808151,"status":"ok","timestamp":1746873030148,"user":{"displayName":"Nithin Keshavamurthy","userId":"17129339458019700030"},"user_tz":420},"id":"9DRalxD1UTuQ","outputId":"2b34799b-f615-45d6-a1bb-f40d78ddc33c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading SynSQL dataset...\n"]},{"output_type":"stream","name":"stderr","text":["\rProcessing examples: 0it [00:00, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Collecting 35000 complex and 25000 simple examples...\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 3000it [03:33, 18.03it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 927/35000 complex queries and 73/25000 simple queries\n","\n","Found 1869/35000 complex queries and 131/25000 simple queries\n","\n","Found 2832/35000 complex queries and 168/25000 simple queries\n","\n","Found 3773/35000 complex queries and 227/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 7000it [03:34, 64.19it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 4732/35000 complex queries and 268/25000 simple queries\n","\n","Found 5686/35000 complex queries and 314/25000 simple queries\n","\n","Found 6620/35000 complex queries and 380/25000 simple queries\n","\n","Found 7565/35000 complex queries and 435/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 11000it [03:34, 157.54it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 8470/35000 complex queries and 530/25000 simple queries\n","\n","Found 9404/35000 complex queries and 596/25000 simple queries\n","\n","Found 10322/35000 complex queries and 678/25000 simple queries\n","\n","Found 11240/35000 complex queries and 760/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 15000it [03:34, 344.71it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 12187/35000 complex queries and 813/25000 simple queries\n","\n","Found 13117/35000 complex queries and 883/25000 simple queries\n","\n","Found 14068/35000 complex queries and 932/25000 simple queries\n","\n","Found 15013/35000 complex queries and 987/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 19000it [03:34, 713.30it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 15934/35000 complex queries and 1066/25000 simple queries\n","\n","Found 16862/35000 complex queries and 1138/25000 simple queries\n","\n","Found 17809/35000 complex queries and 1191/25000 simple queries\n","\n","Found 18745/35000 complex queries and 1255/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 23000it [03:35, 1412.64it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 19683/35000 complex queries and 1317/25000 simple queries\n","\n","Found 20610/35000 complex queries and 1390/25000 simple queries\n","\n","Found 21543/35000 complex queries and 1457/25000 simple queries\n","\n","Found 22496/35000 complex queries and 1504/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 27000it [03:35, 2655.94it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 23431/35000 complex queries and 1569/25000 simple queries\n","\n","Found 24383/35000 complex queries and 1617/25000 simple queries\n","\n","Found 25312/35000 complex queries and 1688/25000 simple queries\n","\n","Found 26251/35000 complex queries and 1749/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 31000it [03:35, 4639.24it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 27218/35000 complex queries and 1782/25000 simple queries\n","\n","Found 28176/35000 complex queries and 1824/25000 simple queries\n","\n","Found 29126/35000 complex queries and 1874/25000 simple queries\n","\n","Found 30048/35000 complex queries and 1952/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 35000it [03:35, 7278.28it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 30985/35000 complex queries and 2015/25000 simple queries\n","\n","Found 31940/35000 complex queries and 2060/25000 simple queries\n","\n","Found 32879/35000 complex queries and 2121/25000 simple queries\n","\n","Found 33823/35000 complex queries and 2177/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 40000it [03:36, 11721.34it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 34765/35000 complex queries and 2235/25000 simple queries\n","\n","Found 35000/35000 complex queries and 2281/25000 simple queries\n","\n","Found 35000/35000 complex queries and 2327/25000 simple queries\n","\n","Found 35000/35000 complex queries and 2368/25000 simple queries\n","\n","Found 35000/35000 complex queries and 2415/25000 simple queries\n","\n","Found 35000/35000 complex queries and 2469/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 46000it [03:36, 17493.54it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 2562/25000 simple queries\n","\n","Found 35000/35000 complex queries and 2605/25000 simple queries\n","\n","Found 35000/35000 complex queries and 2655/25000 simple queries\n","\n","Found 35000/35000 complex queries and 2753/25000 simple queries\n","\n","Found 35000/35000 complex queries and 2816/25000 simple queries\n","\n","Found 35000/35000 complex queries and 2896/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 52000it [03:36, 21787.87it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 2966/25000 simple queries\n","\n","Found 35000/35000 complex queries and 3040/25000 simple queries\n","\n","Found 35000/35000 complex queries and 3105/25000 simple queries\n","\n","Found 35000/35000 complex queries and 3163/25000 simple queries\n","\n","Found 35000/35000 complex queries and 3237/25000 simple queries\n","\n","Found 35000/35000 complex queries and 3259/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 58000it [03:36, 24223.82it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 3325/25000 simple queries\n","\n","Found 35000/35000 complex queries and 3404/25000 simple queries\n","\n","Found 35000/35000 complex queries and 3490/25000 simple queries\n","\n","Found 35000/35000 complex queries and 3605/25000 simple queries\n","\n","Found 35000/35000 complex queries and 3659/25000 simple queries\n","\n","Found 35000/35000 complex queries and 3710/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 64000it [03:36, 25685.94it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 3774/25000 simple queries\n","\n","Found 35000/35000 complex queries and 3851/25000 simple queries\n","\n","Found 35000/35000 complex queries and 3915/25000 simple queries\n","\n","Found 35000/35000 complex queries and 3987/25000 simple queries\n","\n","Found 35000/35000 complex queries and 4084/25000 simple queries\n","\n","Found 35000/35000 complex queries and 4180/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 70000it [03:37, 26151.36it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 4256/25000 simple queries\n","\n","Found 35000/35000 complex queries and 4341/25000 simple queries\n","\n","Found 35000/35000 complex queries and 4401/25000 simple queries\n","\n","Found 35000/35000 complex queries and 4493/25000 simple queries\n","\n","Found 35000/35000 complex queries and 4579/25000 simple queries\n","\n","Found 35000/35000 complex queries and 4657/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 76000it [03:37, 26726.85it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 4733/25000 simple queries\n","\n","Found 35000/35000 complex queries and 4822/25000 simple queries\n","\n","Found 35000/35000 complex queries and 4915/25000 simple queries\n","\n","Found 35000/35000 complex queries and 5002/25000 simple queries\n","\n","Found 35000/35000 complex queries and 5059/25000 simple queries\n","\n","Found 35000/35000 complex queries and 5153/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 82000it [03:37, 27168.62it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 5211/25000 simple queries\n","\n","Found 35000/35000 complex queries and 5319/25000 simple queries\n","\n","Found 35000/35000 complex queries and 5432/25000 simple queries\n","\n","Found 35000/35000 complex queries and 5512/25000 simple queries\n","\n","Found 35000/35000 complex queries and 5608/25000 simple queries\n","\n","Found 35000/35000 complex queries and 5720/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 88000it [03:37, 27013.07it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 5824/25000 simple queries\n","\n","Found 35000/35000 complex queries and 5945/25000 simple queries\n","\n","Found 35000/35000 complex queries and 6033/25000 simple queries\n","\n","Found 35000/35000 complex queries and 6088/25000 simple queries\n","\n","Found 35000/35000 complex queries and 6166/25000 simple queries\n","\n","Found 35000/35000 complex queries and 6282/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 94000it [03:38, 27290.78it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 6343/25000 simple queries\n","\n","Found 35000/35000 complex queries and 6403/25000 simple queries\n","\n","Found 35000/35000 complex queries and 6490/25000 simple queries\n","\n","Found 35000/35000 complex queries and 6576/25000 simple queries\n","\n","Found 35000/35000 complex queries and 6639/25000 simple queries\n","\n","Found 35000/35000 complex queries and 6742/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 100000it [03:38, 27296.65it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 6822/25000 simple queries\n","\n","Found 35000/35000 complex queries and 6896/25000 simple queries\n","\n","Found 35000/35000 complex queries and 6973/25000 simple queries\n","\n","Found 35000/35000 complex queries and 7048/25000 simple queries\n","\n","Found 35000/35000 complex queries and 7135/25000 simple queries\n","\n","Found 35000/35000 complex queries and 7231/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 106000it [03:38, 27282.74it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 7310/25000 simple queries\n","\n","Found 35000/35000 complex queries and 7388/25000 simple queries\n","\n","Found 35000/35000 complex queries and 7467/25000 simple queries\n","\n","Found 35000/35000 complex queries and 7558/25000 simple queries\n","\n","Found 35000/35000 complex queries and 7671/25000 simple queries\n","\n","Found 35000/35000 complex queries and 7768/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 112000it [03:38, 27213.43it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 7856/25000 simple queries\n","\n","Found 35000/35000 complex queries and 7954/25000 simple queries\n","\n","Found 35000/35000 complex queries and 8016/25000 simple queries\n","\n","Found 35000/35000 complex queries and 8098/25000 simple queries\n","\n","Found 35000/35000 complex queries and 8203/25000 simple queries\n","\n","Found 35000/35000 complex queries and 8284/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 118000it [03:38, 27131.88it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 8379/25000 simple queries\n","\n","Found 35000/35000 complex queries and 8473/25000 simple queries\n","\n","Found 35000/35000 complex queries and 8547/25000 simple queries\n","\n","Found 35000/35000 complex queries and 8638/25000 simple queries\n","\n","Found 35000/35000 complex queries and 8728/25000 simple queries\n","\n","Found 35000/35000 complex queries and 8802/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 124000it [03:39, 26892.98it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 8887/25000 simple queries\n","\n","Found 35000/35000 complex queries and 8960/25000 simple queries\n","\n","Found 35000/35000 complex queries and 9078/25000 simple queries\n","\n","Found 35000/35000 complex queries and 9184/25000 simple queries\n","\n","Found 35000/35000 complex queries and 9275/25000 simple queries\n","\n","Found 35000/35000 complex queries and 9381/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 130000it [03:39, 26511.65it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 9462/25000 simple queries\n","\n","Found 35000/35000 complex queries and 9561/25000 simple queries\n","\n","Found 35000/35000 complex queries and 9726/25000 simple queries\n","\n","Found 35000/35000 complex queries and 9864/25000 simple queries\n","\n","Found 35000/35000 complex queries and 9930/25000 simple queries\n","\n","Found 35000/35000 complex queries and 9962/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 136000it [03:39, 26227.75it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 10018/25000 simple queries\n","\n","Found 35000/35000 complex queries and 10066/25000 simple queries\n","\n","Found 35000/35000 complex queries and 10131/25000 simple queries\n","\n","Found 35000/35000 complex queries and 10181/25000 simple queries\n","\n","Found 35000/35000 complex queries and 10244/25000 simple queries\n","\n","Found 35000/35000 complex queries and 10302/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 142000it [03:39, 26122.85it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 10354/25000 simple queries\n","\n","Found 35000/35000 complex queries and 10397/25000 simple queries\n","\n","Found 35000/35000 complex queries and 10459/25000 simple queries\n","\n","Found 35000/35000 complex queries and 10560/25000 simple queries\n","\n","Found 35000/35000 complex queries and 10596/25000 simple queries\n","\n","Found 35000/35000 complex queries and 10645/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 148000it [03:40, 26334.05it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 10694/25000 simple queries\n","\n","Found 35000/35000 complex queries and 10739/25000 simple queries\n","\n","Found 35000/35000 complex queries and 10785/25000 simple queries\n","\n","Found 35000/35000 complex queries and 10852/25000 simple queries\n","\n","Found 35000/35000 complex queries and 10925/25000 simple queries\n","\n","Found 35000/35000 complex queries and 10981/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 154000it [03:40, 26296.09it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 11029/25000 simple queries\n","\n","Found 35000/35000 complex queries and 11084/25000 simple queries\n","\n","Found 35000/35000 complex queries and 11143/25000 simple queries\n","\n","Found 35000/35000 complex queries and 11198/25000 simple queries\n","\n","Found 35000/35000 complex queries and 11228/25000 simple queries\n","\n","Found 35000/35000 complex queries and 11273/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 160000it [03:40, 26632.41it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 11338/25000 simple queries\n","\n","Found 35000/35000 complex queries and 11371/25000 simple queries\n","\n","Found 35000/35000 complex queries and 11420/25000 simple queries\n","\n","Found 35000/35000 complex queries and 11502/25000 simple queries\n","\n","Found 35000/35000 complex queries and 11555/25000 simple queries\n","\n","Found 35000/35000 complex queries and 11651/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 166000it [03:40, 26150.82it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 11734/25000 simple queries\n","\n","Found 35000/35000 complex queries and 11814/25000 simple queries\n","\n","Found 35000/35000 complex queries and 11864/25000 simple queries\n","\n","Found 35000/35000 complex queries and 11899/25000 simple queries\n","\n","Found 35000/35000 complex queries and 11956/25000 simple queries\n","\n","Found 35000/35000 complex queries and 12019/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 172000it [03:41, 26313.53it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 12080/25000 simple queries\n","\n","Found 35000/35000 complex queries and 12141/25000 simple queries\n","\n","Found 35000/35000 complex queries and 12195/25000 simple queries\n","\n","Found 35000/35000 complex queries and 12304/25000 simple queries\n","\n","Found 35000/35000 complex queries and 12363/25000 simple queries\n","\n","Found 35000/35000 complex queries and 12422/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 178000it [03:41, 26457.96it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 12505/25000 simple queries\n","\n","Found 35000/35000 complex queries and 12533/25000 simple queries\n","\n","Found 35000/35000 complex queries and 12604/25000 simple queries\n","\n","Found 35000/35000 complex queries and 12664/25000 simple queries\n","\n","Found 35000/35000 complex queries and 12739/25000 simple queries\n","\n","Found 35000/35000 complex queries and 12766/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 184000it [03:41, 26859.17it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 12812/25000 simple queries\n","\n","Found 35000/35000 complex queries and 12854/25000 simple queries\n","\n","Found 35000/35000 complex queries and 12918/25000 simple queries\n","\n","Found 35000/35000 complex queries and 12987/25000 simple queries\n","\n","Found 35000/35000 complex queries and 13036/25000 simple queries\n","\n","Found 35000/35000 complex queries and 13104/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 190000it [03:41, 26435.62it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 13162/25000 simple queries\n","\n","Found 35000/35000 complex queries and 13252/25000 simple queries\n","\n","Found 35000/35000 complex queries and 13308/25000 simple queries\n","\n","Found 35000/35000 complex queries and 13359/25000 simple queries\n","\n","Found 35000/35000 complex queries and 13427/25000 simple queries\n","\n","Found 35000/35000 complex queries and 13465/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 196000it [03:41, 26457.29it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 13528/25000 simple queries\n","\n","Found 35000/35000 complex queries and 13582/25000 simple queries\n","\n","Found 35000/35000 complex queries and 13624/25000 simple queries\n","\n","Found 35000/35000 complex queries and 13684/25000 simple queries\n","\n","Found 35000/35000 complex queries and 13793/25000 simple queries\n","\n","Found 35000/35000 complex queries and 13875/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 202000it [03:42, 26031.61it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 13949/25000 simple queries\n","\n","Found 35000/35000 complex queries and 14042/25000 simple queries\n","\n","Found 35000/35000 complex queries and 14126/25000 simple queries\n","\n","Found 35000/35000 complex queries and 14197/25000 simple queries\n","\n","Found 35000/35000 complex queries and 14244/25000 simple queries\n","\n","Found 35000/35000 complex queries and 14288/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 208000it [03:42, 26304.80it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 14344/25000 simple queries\n","\n","Found 35000/35000 complex queries and 14402/25000 simple queries\n","\n","Found 35000/35000 complex queries and 14513/25000 simple queries\n","\n","Found 35000/35000 complex queries and 14571/25000 simple queries\n","\n","Found 35000/35000 complex queries and 14632/25000 simple queries\n","\n","Found 35000/35000 complex queries and 14699/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 214000it [03:42, 26348.71it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 14758/25000 simple queries\n","\n","Found 35000/35000 complex queries and 14814/25000 simple queries\n","\n","Found 35000/35000 complex queries and 14868/25000 simple queries\n","\n","Found 35000/35000 complex queries and 14922/25000 simple queries\n","\n","Found 35000/35000 complex queries and 14972/25000 simple queries\n","\n","Found 35000/35000 complex queries and 15038/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 220000it [03:42, 26126.75it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 15103/25000 simple queries\n","\n","Found 35000/35000 complex queries and 15156/25000 simple queries\n","\n","Found 35000/35000 complex queries and 15213/25000 simple queries\n","\n","Found 35000/35000 complex queries and 15283/25000 simple queries\n","\n","Found 35000/35000 complex queries and 15343/25000 simple queries\n","\n","Found 35000/35000 complex queries and 15406/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 226000it [03:43, 26412.57it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 15456/25000 simple queries\n","\n","Found 35000/35000 complex queries and 15510/25000 simple queries\n","\n","Found 35000/35000 complex queries and 15578/25000 simple queries\n","\n","Found 35000/35000 complex queries and 15651/25000 simple queries\n","\n","Found 35000/35000 complex queries and 15691/25000 simple queries\n","\n","Found 35000/35000 complex queries and 15722/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 232000it [03:43, 26425.06it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 15776/25000 simple queries\n","\n","Found 35000/35000 complex queries and 15823/25000 simple queries\n","\n","Found 35000/35000 complex queries and 15853/25000 simple queries\n","\n","Found 35000/35000 complex queries and 15916/25000 simple queries\n","\n","Found 35000/35000 complex queries and 15996/25000 simple queries\n","\n","Found 35000/35000 complex queries and 16042/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 238000it [03:43, 26566.40it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 16085/25000 simple queries\n","\n","Found 35000/35000 complex queries and 16136/25000 simple queries\n","\n","Found 35000/35000 complex queries and 16191/25000 simple queries\n","\n","Found 35000/35000 complex queries and 16276/25000 simple queries\n","\n","Found 35000/35000 complex queries and 16322/25000 simple queries\n","\n","Found 35000/35000 complex queries and 16371/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 244000it [03:43, 26652.38it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 16430/25000 simple queries\n","\n","Found 35000/35000 complex queries and 16484/25000 simple queries\n","\n","Found 35000/35000 complex queries and 16547/25000 simple queries\n","\n","Found 35000/35000 complex queries and 16573/25000 simple queries\n","\n","Found 35000/35000 complex queries and 16629/25000 simple queries\n","\n","Found 35000/35000 complex queries and 16670/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 250000it [03:43, 26554.69it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 16732/25000 simple queries\n","\n","Found 35000/35000 complex queries and 16798/25000 simple queries\n","\n","Found 35000/35000 complex queries and 16851/25000 simple queries\n","\n","Found 35000/35000 complex queries and 16897/25000 simple queries\n","\n","Found 35000/35000 complex queries and 16960/25000 simple queries\n","\n","Found 35000/35000 complex queries and 17007/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 256000it [03:44, 26713.58it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 17085/25000 simple queries\n","\n","Found 35000/35000 complex queries and 17122/25000 simple queries\n","\n","Found 35000/35000 complex queries and 17187/25000 simple queries\n","\n","Found 35000/35000 complex queries and 17235/25000 simple queries\n","\n","Found 35000/35000 complex queries and 17306/25000 simple queries\n","\n","Found 35000/35000 complex queries and 17376/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 262000it [03:44, 26740.92it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 17443/25000 simple queries\n","\n","Found 35000/35000 complex queries and 17493/25000 simple queries\n","\n","Found 35000/35000 complex queries and 17545/25000 simple queries\n","\n","Found 35000/35000 complex queries and 17629/25000 simple queries\n","\n","Found 35000/35000 complex queries and 17692/25000 simple queries\n","\n","Found 35000/35000 complex queries and 17777/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 268000it [03:44, 26402.41it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 17843/25000 simple queries\n","\n","Found 35000/35000 complex queries and 17907/25000 simple queries\n","\n","Found 35000/35000 complex queries and 17980/25000 simple queries\n","\n","Found 35000/35000 complex queries and 18032/25000 simple queries\n","\n","Found 35000/35000 complex queries and 18072/25000 simple queries\n","\n","Found 35000/35000 complex queries and 18098/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 274000it [03:44, 26278.35it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 18193/25000 simple queries\n","\n","Found 35000/35000 complex queries and 18261/25000 simple queries\n","\n","Found 35000/35000 complex queries and 18349/25000 simple queries\n","\n","Found 35000/35000 complex queries and 18410/25000 simple queries\n","\n","Found 35000/35000 complex queries and 18477/25000 simple queries\n","\n","Found 35000/35000 complex queries and 18548/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 280000it [03:45, 26728.83it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 18605/25000 simple queries\n","\n","Found 35000/35000 complex queries and 18674/25000 simple queries\n","\n","Found 35000/35000 complex queries and 18738/25000 simple queries\n","\n","Found 35000/35000 complex queries and 18810/25000 simple queries\n","\n","Found 35000/35000 complex queries and 18860/25000 simple queries\n","\n","Found 35000/35000 complex queries and 18919/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 286000it [03:45, 26839.60it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 18967/25000 simple queries\n","\n","Found 35000/35000 complex queries and 19027/25000 simple queries\n","\n","Found 35000/35000 complex queries and 19105/25000 simple queries\n","\n","Found 35000/35000 complex queries and 19150/25000 simple queries\n","\n","Found 35000/35000 complex queries and 19196/25000 simple queries\n","\n","Found 35000/35000 complex queries and 19260/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 292000it [03:45, 26518.50it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 19334/25000 simple queries\n","\n","Found 35000/35000 complex queries and 19403/25000 simple queries\n","\n","Found 35000/35000 complex queries and 19460/25000 simple queries\n","\n","Found 35000/35000 complex queries and 19515/25000 simple queries\n","\n","Found 35000/35000 complex queries and 19568/25000 simple queries\n","\n","Found 35000/35000 complex queries and 19617/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 298000it [03:45, 26247.80it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 19652/25000 simple queries\n","\n","Found 35000/35000 complex queries and 19705/25000 simple queries\n","\n","Found 35000/35000 complex queries and 19761/25000 simple queries\n","\n","Found 35000/35000 complex queries and 19829/25000 simple queries\n","\n","Found 35000/35000 complex queries and 19909/25000 simple queries\n","\n","Found 35000/35000 complex queries and 19991/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 304000it [03:46, 26125.07it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 20050/25000 simple queries\n","\n","Found 35000/35000 complex queries and 20143/25000 simple queries\n","\n","Found 35000/35000 complex queries and 20190/25000 simple queries\n","\n","Found 35000/35000 complex queries and 20257/25000 simple queries\n","\n","Found 35000/35000 complex queries and 20287/25000 simple queries\n","\n","Found 35000/35000 complex queries and 20339/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 310000it [03:46, 26504.10it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 20420/25000 simple queries\n","\n","Found 35000/35000 complex queries and 20477/25000 simple queries\n","\n","Found 35000/35000 complex queries and 20523/25000 simple queries\n","\n","Found 35000/35000 complex queries and 20568/25000 simple queries\n","\n","Found 35000/35000 complex queries and 20618/25000 simple queries\n","\n","Found 35000/35000 complex queries and 20681/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 316000it [03:46, 26593.22it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 20751/25000 simple queries\n","\n","Found 35000/35000 complex queries and 20797/25000 simple queries\n","\n","Found 35000/35000 complex queries and 20840/25000 simple queries\n","\n","Found 35000/35000 complex queries and 20896/25000 simple queries\n","\n","Found 35000/35000 complex queries and 20958/25000 simple queries\n","\n","Found 35000/35000 complex queries and 20998/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 322000it [03:46, 26428.42it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 21057/25000 simple queries\n","\n","Found 35000/35000 complex queries and 21111/25000 simple queries\n","\n","Found 35000/35000 complex queries and 21191/25000 simple queries\n","\n","Found 35000/35000 complex queries and 21247/25000 simple queries\n","\n","Found 35000/35000 complex queries and 21314/25000 simple queries\n","\n","Found 35000/35000 complex queries and 21352/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 328000it [03:46, 26607.29it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 21402/25000 simple queries\n","\n","Found 35000/35000 complex queries and 21463/25000 simple queries\n","\n","Found 35000/35000 complex queries and 21525/25000 simple queries\n","\n","Found 35000/35000 complex queries and 21571/25000 simple queries\n","\n","Found 35000/35000 complex queries and 21615/25000 simple queries\n","\n","Found 35000/35000 complex queries and 21664/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 334000it [03:47, 26608.54it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 21725/25000 simple queries\n","\n","Found 35000/35000 complex queries and 21794/25000 simple queries\n","\n","Found 35000/35000 complex queries and 21876/25000 simple queries\n","\n","Found 35000/35000 complex queries and 21918/25000 simple queries\n","\n","Found 35000/35000 complex queries and 21978/25000 simple queries\n","\n","Found 35000/35000 complex queries and 22056/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 340000it [03:47, 26616.30it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 22119/25000 simple queries\n","\n","Found 35000/35000 complex queries and 22184/25000 simple queries\n","\n","Found 35000/35000 complex queries and 22224/25000 simple queries\n","\n","Found 35000/35000 complex queries and 22269/25000 simple queries\n","\n","Found 35000/35000 complex queries and 22307/25000 simple queries\n","\n","Found 35000/35000 complex queries and 22371/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 346000it [03:47, 26098.05it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 22414/25000 simple queries\n","\n","Found 35000/35000 complex queries and 22493/25000 simple queries\n","\n","Found 35000/35000 complex queries and 22562/25000 simple queries\n","\n","Found 35000/35000 complex queries and 22629/25000 simple queries\n","\n","Found 35000/35000 complex queries and 22685/25000 simple queries\n","\n","Found 35000/35000 complex queries and 22719/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 352000it [03:47, 25993.50it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 22777/25000 simple queries\n","\n","Found 35000/35000 complex queries and 22830/25000 simple queries\n","\n","Found 35000/35000 complex queries and 22903/25000 simple queries\n","\n","Found 35000/35000 complex queries and 22988/25000 simple queries\n","\n","Found 35000/35000 complex queries and 23039/25000 simple queries\n","\n","Found 35000/35000 complex queries and 23084/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 358000it [03:48, 26278.55it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 23130/25000 simple queries\n","\n","Found 35000/35000 complex queries and 23193/25000 simple queries\n","\n","Found 35000/35000 complex queries and 23227/25000 simple queries\n","\n","Found 35000/35000 complex queries and 23330/25000 simple queries\n","\n","Found 35000/35000 complex queries and 23388/25000 simple queries\n","\n","Found 35000/35000 complex queries and 23441/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 364000it [03:48, 26261.84it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 23492/25000 simple queries\n","\n","Found 35000/35000 complex queries and 23552/25000 simple queries\n","\n","Found 35000/35000 complex queries and 23620/25000 simple queries\n","\n","Found 35000/35000 complex queries and 23671/25000 simple queries\n","\n","Found 35000/35000 complex queries and 23748/25000 simple queries\n","\n","Found 35000/35000 complex queries and 23807/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 370000it [03:48, 26267.63it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 23883/25000 simple queries\n","\n","Found 35000/35000 complex queries and 23923/25000 simple queries\n","\n","Found 35000/35000 complex queries and 23963/25000 simple queries\n","\n","Found 35000/35000 complex queries and 24007/25000 simple queries\n","\n","Found 35000/35000 complex queries and 24064/25000 simple queries\n","\n","Found 35000/35000 complex queries and 24153/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 376000it [03:48, 26020.04it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 24200/25000 simple queries\n","\n","Found 35000/35000 complex queries and 24256/25000 simple queries\n","\n","Found 35000/35000 complex queries and 24319/25000 simple queries\n","\n","Found 35000/35000 complex queries and 24384/25000 simple queries\n","\n","Found 35000/35000 complex queries and 24424/25000 simple queries\n","\n","Found 35000/35000 complex queries and 24463/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 382000it [03:48, 25975.34it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 24512/25000 simple queries\n","\n","Found 35000/35000 complex queries and 24563/25000 simple queries\n","\n","Found 35000/35000 complex queries and 24626/25000 simple queries\n","\n","Found 35000/35000 complex queries and 24678/25000 simple queries\n","\n","Found 35000/35000 complex queries and 24705/25000 simple queries\n","\n","Found 35000/35000 complex queries and 24723/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 388000it [03:49, 25749.48it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 24734/25000 simple queries\n","\n","Found 35000/35000 complex queries and 24771/25000 simple queries\n","\n","Found 35000/35000 complex queries and 24805/25000 simple queries\n","\n","Found 35000/35000 complex queries and 24824/25000 simple queries\n","\n","Found 35000/35000 complex queries and 24848/25000 simple queries\n","\n","Found 35000/35000 complex queries and 24865/25000 simple queries\n"]},{"output_type":"stream","name":"stderr","text":["Processing examples: 394000it [03:49, 1717.11it/s] \n"]},{"output_type":"stream","name":"stdout","text":["\n","Found 35000/35000 complex queries and 24895/25000 simple queries\n","\n","Found 35000/35000 complex queries and 24926/25000 simple queries\n","\n","Found 35000/35000 complex queries and 24956/25000 simple queries\n","\n","Found 35000/35000 complex queries and 24999/25000 simple queries\n","\n","Creating high-quality randomized dataset...\n","Saving properly randomized dataset in 6 chunks...\n","Saved chunk 0 with 10000 examples\n","Saved chunk 1 with 10000 examples\n","Saved chunk 2 with 10000 examples\n","Saved chunk 3 with 10000 examples\n","Saved chunk 4 with 10000 examples\n","Saved chunk 5 with 10000 examples\n","Dataset creation complete. Manifest saved to sql_data/manifest.json\n","Final dataset composition:\n"," - Complex queries: 35000\n"," - Simple queries added: 25000\n"," - Total training examples: 60000\n"]}],"source":["import os\n","import json\n","import random\n","import numpy as np\n","from tqdm import tqdm\n","from datasets import load_dataset\n","\n","# Load dataset in streaming mode to save memory\n","print(\"Loading SynSQL dataset...\")\n","synsql_stream = load_dataset(\"seeklhy/SynSQL-2.5M\", split=\"train\", streaming=True)\n","\n","# Set target numbers\n","target_complex = 35000  # Target number of complex queries\n","target_simple = 25000   # Target number of simple queries\n","final_dataset_size = target_complex + target_simple\n","\n","# Create output directory\n","os.makedirs(\"sql_data\", exist_ok=True)\n","\n","# Files for storing complex and simple queries\n","complex_file = \"sql_data/complex_queries.jsonl\"\n","simple_file = \"sql_data/simple_queries.jsonl\"\n","\n","# Remove files if they exist\n","if os.path.exists(complex_file):\n","    os.remove(complex_file)\n","if os.path.exists(simple_file):\n","    os.remove(simple_file)\n","\n","# Tracking variables\n","complex_count = 0\n","simple_count = 0\n","processed_count = 0\n","\n","# Track progress\n","progress_bar = tqdm(desc=\"Processing examples\")\n","\n","# Create an iterator for the stream\n","synsql_iterator = iter(synsql_stream)\n","\n","print(f\"Collecting {target_complex} complex and {target_simple} simple examples...\")\n","\n","# Process the stream and write directly to files\n","while complex_count < target_complex or simple_count < target_simple:\n","    try:\n","        sample = next(synsql_iterator)\n","        processed_count += 1\n","\n","        question = sample.get(\"question\", \"\").strip()\n","        sql = sample.get(\"sql\", \"\").strip()\n","\n","        if not question or not sql:\n","            continue\n","\n","        prompt = f\"\"\"### Instruction:\n","Generate an SQL query for the following question.\n","### Question:\n","{question}\n","### SQL:\n","\"\"\"\n","        formatted_example = {\n","            \"prompt\": prompt,\n","            \"response\": sql\n","        }\n","\n","        # Write to appropriate file (JSONL format - one JSON object per line)\n","        if is_complex_query(sql) and complex_count < target_complex:\n","            with open(complex_file, 'a') as f:\n","                f.write(json.dumps(formatted_example) + '\\n')\n","            complex_count += 1\n","\n","        elif not is_complex_query(sql) and simple_count < target_simple:\n","            with open(simple_file, 'a') as f:\n","                f.write(json.dumps(formatted_example) + '\\n')\n","            simple_count += 1\n","\n","        # Update progress every 1000 examples\n","        if processed_count % 1000 == 0:\n","            progress_bar.update(1000)\n","            print(f\"\\nFound {complex_count}/{target_complex} complex queries and {simple_count}/{target_simple} simple queries\")\n","\n","        # Safety check\n","        if processed_count > 1000000:\n","            print(\"Warning: Processed too many examples without finding enough of both types.\")\n","            break\n","\n","    except StopIteration:\n","        print(\"\\nReached the end of the dataset before finding enough examples.\")\n","        break\n","\n","progress_bar.close()\n","\n","print(\"\\nCreating high-quality randomized dataset...\")\n","\n","# Create index arrays for both files\n","complex_indices = list(range(min(complex_count, target_complex)))\n","simple_indices = list(range(min(simple_count, target_simple)))\n","\n","# Shuffle the indices\n","random.shuffle(complex_indices)\n","random.shuffle(simple_indices)\n","\n","# Calculate how many examples we can include from each type\n","actual_complex = min(complex_count, target_complex)\n","actual_simple = min(simple_count, target_simple)\n","total_examples = actual_complex + actual_simple\n","\n","# Determine indices for interleaving complex and simple examples\n","# This creates a proper randomized dataset while respecting the target ratio\n","all_indices = []\n","for i in range(total_examples):\n","    # Weighted random selection based on remaining examples\n","    complex_weight = actual_complex / (actual_complex + actual_simple) if (actual_complex + actual_simple) > 0 else 0\n","    if random.random() < complex_weight and complex_indices:\n","        all_indices.append((\"complex\", complex_indices.pop()))\n","        actual_complex -= 1\n","    elif simple_indices:\n","        all_indices.append((\"simple\", simple_indices.pop()))\n","        actual_simple -= 1\n","    else:\n","        all_indices.append((\"complex\", complex_indices.pop()))\n","        actual_complex -= 1\n","\n","# Function to get example at a specific index from a file\n","def get_example_at_index(file_path, index):\n","    with open(file_path, 'r') as f:\n","        for i, line in enumerate(f):\n","            if i == index:\n","                return json.loads(line)\n","    return None\n","\n","# Save the interleaved dataset in chunks\n","chunk_size = 10000\n","total_chunks = (total_examples - 1) // chunk_size + 1\n","print(f\"Saving properly randomized dataset in {total_chunks} chunks...\")\n","\n","for chunk_idx in range(total_chunks):\n","    start_idx = chunk_idx * chunk_size\n","    end_idx = min(start_idx + chunk_size, total_examples)\n","\n","    chunk = []\n","    for i in range(start_idx, end_idx):\n","        file_type, file_idx = all_indices[i]\n","        if file_type == \"complex\":\n","            example = get_example_at_index(complex_file, file_idx)\n","        else:\n","            example = get_example_at_index(simple_file, file_idx)\n","        if example:\n","            chunk.append(example)\n","\n","    chunk_file = f\"sql_data/training_data_chunk_{chunk_idx}.json\"\n","    with open(chunk_file, 'w') as f:\n","        json.dump(chunk, f)\n","    print(f\"Saved chunk {chunk_idx} with {len(chunk)} examples\")\n","\n","# Create a manifest file for training\n","manifest = {\n","    \"total_examples\": total_examples,\n","    \"complex_examples\": min(complex_count, target_complex),\n","    \"simple_examples\": min(simple_count, target_simple),\n","    \"chunks\": total_chunks,\n","    \"chunk_size\": chunk_size\n","}\n","\n","with open(\"sql_data/manifest.json\", 'w') as f:\n","    json.dump(manifest, f)\n","\n","print(f\"Dataset creation complete. Manifest saved to sql_data/manifest.json\")\n","print(f\"Final dataset composition:\")\n","print(f\" - Complex queries: {min(complex_count, target_complex)}\")\n","print(f\" - Simple queries added: {min(simple_count, target_simple)}\")\n","print(f\" - Total training examples: {total_examples}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":83640,"status":"ok","timestamp":1746876450766,"user":{"displayName":"Nithin Keshavamurthy","userId":"17129339458019700030"},"user_tz":420},"id":"Gw-94WjwUfbw","outputId":"af29b9f7-4ab5-4d88-8542-48e2c0acf830"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Setting up dataset...\n","Total dataset size: 60000\n","Training dataset size: 58000\n","Validation dataset size: 1000\n","Test dataset size: 1000\n","Saving train dataset (58000 examples) in 58 chunks...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 58/58 [36:55<00:00, 38.21s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Saved train dataset to /content/drive/Shareddrives/DATA266_Project/Data/sql_data\n","Saving val dataset (1000 examples) in 1 chunks...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:39<00:00, 39.53s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Saved val dataset to /content/drive/Shareddrives/DATA266_Project/Data/sql_data\n","Saving test dataset (1000 examples) in 1 chunks...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:39<00:00, 39.11s/it]"]},{"output_type":"stream","name":"stdout","text":["Saved test dataset to /content/drive/Shareddrives/DATA266_Project/Data/sql_data\n","All datasets saved successfully.\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import os\n","import json\n","import torch\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from transformers import AutoTokenizer\n","from tqdm import tqdm\n","\n","# Load the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n","tokenizer.pad_token = tokenizer.eos_token  # Ensure padding works correctly\n","\n","# Define the correct data directory\n","data_dir = \"/content/drive/Shareddrives/DATA266_Project/Data/sql_data\"\n","\n","# Define a dataset class that loads data from chunks\n","class ChunkedSQLDataset(Dataset):\n","    def __init__(self, data_dir, tokenizer, max_length=512):\n","        self.data_dir = data_dir\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","        # Load the manifest file\n","        with open(os.path.join(data_dir, \"manifest.json\"), 'r') as f:\n","            self.manifest = json.load(f)\n","\n","        self.total_examples = self.manifest[\"total_examples\"]\n","        self.chunks = self.manifest[\"chunks\"]\n","        self.chunk_size = self.manifest[\"chunk_size\"]\n","\n","        # Create a mapping of example index to chunk index and position\n","        self.example_map = {}\n","        for i in range(self.total_examples):\n","            chunk_idx = i // self.chunk_size\n","            position_in_chunk = i % self.chunk_size\n","            self.example_map[i] = (chunk_idx, position_in_chunk)\n","\n","        # Cache for loaded chunks to improve efficiency\n","        self.chunk_cache = {}\n","        self.current_cached_chunks = []\n","        self.max_cached_chunks = 2  # Adjust based on memory constraints\n","\n","    def __len__(self):\n","        return self.total_examples\n","\n","    def load_chunk(self, chunk_idx):\n","        \"\"\"Load a chunk into memory if not already cached\"\"\"\n","        if chunk_idx in self.chunk_cache:\n","            return self.chunk_cache[chunk_idx]\n","\n","        # Load the chunk\n","        chunk_file = os.path.join(self.data_dir, f\"training_data_chunk_{chunk_idx}.json\")\n","        with open(chunk_file, 'r') as f:\n","            chunk_data = json.load(f)\n","\n","        # Update cache\n","        self.chunk_cache[chunk_idx] = chunk_data\n","        self.current_cached_chunks.append(chunk_idx)\n","\n","        # If cache is full, remove the oldest chunk\n","        if len(self.current_cached_chunks) > self.max_cached_chunks:\n","            oldest_chunk = self.current_cached_chunks.pop(0)\n","            if oldest_chunk in self.chunk_cache:\n","                del self.chunk_cache[oldest_chunk]\n","\n","        return chunk_data\n","\n","    def __getitem__(self, idx):\n","        # Get the chunk index and position\n","        chunk_idx, position_in_chunk = self.example_map[idx]\n","\n","        # Load the chunk if necessary\n","        chunk_data = self.load_chunk(chunk_idx)\n","\n","        # Get the example\n","        sample = chunk_data[position_in_chunk]\n","\n","        # Combine prompt and response\n","        full_text = sample[\"prompt\"] + sample[\"response\"]\n","\n","        # Tokenize with proper padding and truncation\n","        tokenized = self.tokenizer(\n","            full_text,\n","            truncation=True,\n","            padding=\"max_length\",\n","            max_length=self.max_length,\n","            return_tensors=\"pt\"\n","        )\n","\n","        # For causal LM, labels are the same as input_ids\n","        tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n","\n","        return {key: val.squeeze(0) for key, val in tokenized.items()}\n","\n","# Create the dataset\n","print(\"Setting up dataset...\")\n","full_dataset = ChunkedSQLDataset(data_dir=data_dir, tokenizer=tokenizer)\n","\n","# Create train/val/test split using indices\n","dataset_size = len(full_dataset)\n","test_size = min(1000, int(dataset_size * 0.05))\n","val_size = min(1000, int(dataset_size * 0.05))\n","train_size = dataset_size - val_size - test_size\n","\n","# Use PyTorch's random_split to create the splits\n","train_dataset, val_dataset, test_dataset = random_split(\n","    full_dataset,\n","    [train_size, val_size, test_size],\n","    generator=torch.Generator().manual_seed(42)  # For reproducibility\n",")\n","\n","print(f\"Total dataset size: {dataset_size}\")\n","print(f\"Training dataset size: {len(train_dataset)}\")\n","print(f\"Validation dataset size: {len(val_dataset)}\")\n","print(f\"Test dataset size: {len(test_dataset)}\")\n","\n","# Output directory is the same as input directory\n","output_dir = data_dir\n","\n","# Function to save dataset splits\n","def save_tokenized_dataset(dataset, split_name, output_dir, chunk_size=1000):\n","    indices = dataset.indices if hasattr(dataset, 'indices') else range(len(dataset))\n","    num_chunks = (len(indices) - 1) // chunk_size + 1\n","\n","    print(f\"Saving {split_name} dataset ({len(indices)} examples) in {num_chunks} chunks...\")\n","\n","    for chunk_idx in tqdm(range(num_chunks)):\n","        start_idx = chunk_idx * chunk_size\n","        end_idx = min(start_idx + chunk_size, len(indices))\n","        chunk_indices = indices[start_idx:end_idx]\n","\n","        tokenized_examples = []\n","        for idx in chunk_indices:\n","            example = full_dataset[idx]\n","            tokenized_examples.append(example)\n","\n","        # Save the chunk\n","        output_file = os.path.join(output_dir, f\"{split_name}_chunk_{chunk_idx}.pt\")\n","        torch.save(tokenized_examples, output_file)\n","\n","    # Save manifest for this split\n","    manifest_file = os.path.join(output_dir, f\"{split_name}_manifest.json\")\n","    with open(manifest_file, 'w') as f:\n","        json.dump({\n","            \"num_examples\": len(indices),\n","            \"num_chunks\": num_chunks,\n","            \"chunk_size\": chunk_size\n","        }, f)\n","\n","    print(f\"Saved {split_name} dataset to {output_dir}\")\n","\n","# Save each split\n","save_tokenized_dataset(train_dataset, \"train\", output_dir)\n","save_tokenized_dataset(val_dataset, \"val\", output_dir)\n","save_tokenized_dataset(test_dataset, \"test\", output_dir)\n","\n","print(\"All datasets saved successfully.\")"]},{"cell_type":"code","source":["import os\n","import torch\n","import json\n","from tqdm import tqdm\n","\n","# Define paths\n","data_dir = \"/content/drive/Shareddrives/DATA266_Project/Data/sql_data\"\n","preprocessed_data_path = \"/content/drive/Shareddrives/DATA266_Project/Data/sql_data/preprocessed_data\"\n","\n","# Create preprocessed data directory if it doesn't exist\n","os.makedirs(preprocessed_data_path, exist_ok=True)\n","\n","# Function to load all tokenized examples from chunks\n","def load_all_tokenized_data(data_dir, split_names=[\"train\", \"val\", \"test\"]):\n","   print(\"Loading all tokenized data...\")\n","   all_examples = []\n","   splits_indices = {\"train_indices\": [], \"val_indices\": [], \"test_indices\": []}\n","\n","   current_index = 0\n","\n","   # Process each split\n","   for split_name in split_names:\n","       # Load the manifest for this split\n","       manifest_path = os.path.join(data_dir, f\"{split_name}_manifest.json\")\n","       if not os.path.exists(manifest_path):\n","           print(f\"Warning: Manifest not found for {split_name} split\")\n","           continue\n","\n","       with open(manifest_path, \"r\") as f:\n","           manifest = json.load(f)\n","\n","       num_chunks = manifest[\"num_chunks\"]\n","       split_start_index = current_index\n","\n","       # Load all chunks for this split\n","       for chunk_idx in tqdm(range(num_chunks), desc=f\"Loading {split_name} chunks\"):\n","           chunk_path = os.path.join(data_dir, f\"{split_name}_chunk_{chunk_idx}.pt\")\n","           if not os.path.exists(chunk_path):\n","               print(f\"Warning: Chunk file not found: {chunk_path}\")\n","               continue\n","\n","           # Load the chunk\n","           chunk_data = torch.load(chunk_path)\n","\n","           # Add current indices to the appropriate split\n","           chunk_indices = list(range(current_index, current_index + len(chunk_data)))\n","           splits_indices[f\"{split_name}_indices\"].extend(chunk_indices)\n","\n","           # Add examples to the combined list\n","           all_examples.extend(chunk_data)\n","           current_index += len(chunk_data)\n","\n","       print(f\"Loaded {current_index - split_start_index} examples for {split_name} split\")\n","\n","   print(f\"Total loaded examples: {len(all_examples)}\")\n","   return all_examples, splits_indices\n","\n","# Load all tokenized data and track indices for each split\n","tokenized_data, splits_indices = load_all_tokenized_data(data_dir)\n","\n","# Save the combined tokenized data\n","combined_file = os.path.join(preprocessed_data_path, \"tokenized_combined_data.pt\")\n","print(f\"Saving combined tokenized data to {combined_file}\")\n","torch.save(tokenized_data, combined_file)\n","\n","# Save the split indices\n","splits_file = os.path.join(preprocessed_data_path, \"dataset_splits.json\")\n","print(f\"Saving dataset splits to {splits_file}\")\n","with open(splits_file, \"w\") as f:\n","   json.dump(splits_indices, f)\n","\n","print(\"Data preparation complete.\")\n","print(f\"Training examples: {len(splits_indices['train_indices'])}\")\n","print(f\"Validation examples: {len(splits_indices['val_indices'])}\")\n","print(f\"Test examples: {len(splits_indices['test_indices'])}\")\n","\n","# Memory cleanup\n","import gc\n","gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jPxfgWEu85Id","executionInfo":{"status":"ok","timestamp":1746876534533,"user_tz":420,"elapsed":41312,"user":{"displayName":"Nithin Keshavamurthy","userId":"17129339458019700030"}},"outputId":"6c715cde-d293-4e3b-f134-af07011513c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading all tokenized data...\n"]},{"output_type":"stream","name":"stderr","text":["Loading train chunks: 100%|██████████| 58/58 [00:18<00:00,  3.15it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loaded 58000 examples for train split\n"]},{"output_type":"stream","name":"stderr","text":["Loading val chunks: 100%|██████████| 1/1 [00:00<00:00,  1.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loaded 1000 examples for val split\n"]},{"output_type":"stream","name":"stderr","text":["Loading test chunks: 100%|██████████| 1/1 [00:00<00:00,  3.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loaded 1000 examples for test split\n","Total loaded examples: 60000\n","Saving combined tokenized data to /content/drive/Shareddrives/DATA266_Project/Data/sql_data/preprocessed_data/tokenized_combined_data.pt\n","Saving dataset splits to /content/drive/Shareddrives/DATA266_Project/Data/sql_data/preprocessed_data/dataset_splits.json\n","Data preparation complete.\n","Training examples: 58000\n","Validation examples: 1000\n","Test examples: 1000\n"]},{"output_type":"execute_result","data":{"text/plain":["180046"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["import os\n","import json\n","import torch\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from transformers import AutoTokenizer\n","\n","# Load the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n","tokenizer.pad_token = tokenizer.eos_token  # Ensure padding works correctly\n","\n","# Define a dataset class that loads data from chunks\n","class ChunkedSQLDataset(Dataset):\n","   def __init__(self, data_dir, tokenizer, max_length=512):\n","       self.data_dir = data_dir\n","       self.tokenizer = tokenizer\n","       self.max_length = max_length\n","\n","       # Load the manifest file\n","       with open(os.path.join(data_dir, \"manifest.json\"), 'r') as f:\n","           self.manifest = json.load(f)\n","\n","       self.total_examples = self.manifest[\"total_examples\"]\n","       self.chunks = self.manifest[\"chunks\"]\n","       self.chunk_size = self.manifest[\"chunk_size\"]\n","\n","       # Create a mapping of example index to chunk index and position\n","       self.example_map = {}\n","       for i in range(self.total_examples):\n","           chunk_idx = i // self.chunk_size\n","           position_in_chunk = i % self.chunk_size\n","           self.example_map[i] = (chunk_idx, position_in_chunk)\n","\n","       # Cache for loaded chunks to improve efficiency\n","       self.chunk_cache = {}\n","       self.current_cached_chunks = []\n","       self.max_cached_chunks = 2  # Adjust based on memory constraints\n","\n","   def __len__(self):\n","       return self.total_examples\n","\n","   def load_chunk(self, chunk_idx):\n","       #Load a chunk into memory if not already cached\n","       if chunk_idx in self.chunk_cache:\n","           return self.chunk_cache[chunk_idx]\n","\n","       # Load the chunk\n","       chunk_file = os.path.join(self.data_dir, f\"training_data_chunk_{chunk_idx}.json\")\n","       with open(chunk_file, 'r') as f:\n","           chunk_data = json.load(f)\n","\n","       # Update cache\n","       self.chunk_cache[chunk_idx] = chunk_data\n","       self.current_cached_chunks.append(chunk_idx)\n","\n","       # If cache is full, remove the oldest chunk\n","       if len(self.current_cached_chunks) > self.max_cached_chunks:\n","           oldest_chunk = self.current_cached_chunks.pop(0)\n","           if oldest_chunk in self.chunk_cache:\n","               del self.chunk_cache[oldest_chunk]\n","\n","       return chunk_data\n","\n","   def __getitem__(self, idx):\n","       # Get the chunk index and position\n","       chunk_idx, position_in_chunk = self.example_map[idx]\n","\n","       # Load the chunk if necessary\n","       chunk_data = self.load_chunk(chunk_idx)\n","\n","       # Get the example\n","       sample = chunk_data[position_in_chunk]\n","\n","       # Combine prompt and response\n","       full_text = sample[\"prompt\"] + sample[\"response\"]\n","\n","       # Tokenize with proper padding and truncation\n","       tokenized = self.tokenizer(\n","           full_text,\n","           truncation=True,\n","           padding=\"max_length\",\n","           max_length=self.max_length,\n","           return_tensors=\"pt\"\n","       )\n","\n","       # For causal LM, labels are the same as input_ids\n","       tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n","\n","       return {key: val.squeeze(0) for key, val in tokenized.items()}\n","\n","# Create the dataset\n","print(\"Setting up dataset...\")\n","full_dataset = ChunkedSQLDataset(data_dir=\"sql_data\", tokenizer=tokenizer)\n","\n","# Create train/val/test split using indices\n","dataset_size = len(full_dataset)\n","test_size = min(1000, int(dataset_size * 0.05))\n","val_size = min(1000, int(dataset_size * 0.05))\n","train_size = dataset_size - val_size - test_size\n","\n","# Use PyTorch's random_split to create the splits\n","train_dataset, val_dataset, test_dataset = random_split(\n","   full_dataset,\n","   [train_size, val_size, test_size],\n","   generator=torch.Generator().manual_seed(42)  # For reproducibility\n",")\n","\n","print(f\"Total dataset size: {dataset_size}\")\n","print(f\"Training dataset size: {len(train_dataset)}\")\n","print(f\"Validation dataset size: {len(val_dataset)}\")\n","print(f\"Test dataset size: {len(test_dataset)}\")\n","\n","# Create data loaders\n","train_dataloader = DataLoader(\n","   train_dataset,\n","   batch_size=8,\n","   shuffle=True,\n","   num_workers=2\n",")\n","\n","val_dataloader = DataLoader(\n","   val_dataset,\n","   batch_size=8,\n","   shuffle=False,\n","   num_workers=2\n",")\n","\n","test_dataloader = DataLoader(\n","   test_dataset,\n","   batch_size=8,\n","   shuffle=False,\n","   num_workers=2\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qT1q3TgyxIE_","executionInfo":{"status":"ok","timestamp":1746873388701,"user_tz":420,"elapsed":358,"user":{"displayName":"Nithin Keshavamurthy","userId":"17129339458019700030"}},"outputId":"2c587845-ba0c-4c38-a1c6-9a09b9e6723e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Setting up dataset...\n","Total dataset size: 60000\n","Training dataset size: 58000\n","Validation dataset size: 1000\n","Test dataset size: 1000\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40,"status":"ok","timestamp":1746667255614,"user":{"displayName":"Nithin Keshavamurthy","userId":"17129339458019700030"},"user_tz":420},"id":"4CT9kh3IXIuf","outputId":"58a258bf-5021-4786-f63f-b4ee14c004c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training dataset size: 39000\n","Validation dataset size: 1000\n"]}],"source":["class SQLDataset(Dataset):\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            \"input_ids\": self.data[idx][\"input_ids\"],\n","            \"attention_mask\": self.data[idx][\"attention_mask\"],\n","            \"labels\": self.data[idx][\"labels\"]\n","        }\n","\n","# Create train and validation splits\n","val_size = min(1000, int(len(tokenized_dataset) * 0.05))\n","train_data = tokenized_dataset[:-val_size]\n","val_data = tokenized_dataset[-val_size:]\n","\n","train_dataset = SQLDataset(train_data)\n","val_dataset = SQLDataset(val_data)\n","\n","print(f\"Training dataset size: {len(train_dataset)}\")\n","print(f\"Validation dataset size: {len(val_dataset)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["91a56fba160641d38cb4ad430f2e4024","c6fd0be956bb4e67b15d4660f9b1a43f","b4d5879255f8459398ba0d7ac9be4452","8d89a3b943c6467682f4537f916dd23f","1b83ba1506f74950a4f9d6aa136cf034","69ddcd6fff75492fbfa0631913b3ecff","282ab77adf624e6487f5b9faf2f145c9","d8f7a4946eb04b0d949c20afae1ab6a1","0566c57a70b845cda3feafc8174a0104","f73b0966a54f4b7db04117257c61646c","c37f4e128ec34f01b7ba1189eaeecd41","502d64ae43b5449fac7fb0a616d34b22","297a0837f39045319fe339a53b8a7152","fb2a7c9a0fb549dd9f872a317acb95e5","ef6ad329d90345c3908409eb085b3cca","a9eb869104d7405e96ce40abd2ff91da","47c4ad4741d74fdb8eaf1e2323915107","e7efd3833a6d4e71863df2ae481f2bbe","7444377c63994813880f0f470f10c430","fbe255c006664403811c3119c46cda0c","791433f1ee67498aab8604c4f4bb2a3d","c0e14ad7daed451cafe97f8ac1cc9d46","ce31bf68bdbc4597959d8cb603ee6744","c532da6460c94acf88b07eb88b37687f","537476b6435a4c1a9a7ed2f236528e57","2fbca29b97494d3da19730fa1674d4b6","71cb62f9b44146f4b2c62f0f913d83dd","b4969fe334c343d6b19eb446414dc164","fbc92ca12e69442eb2f58f9b028cd945","5865ff53ad204e398fcc31d353661e29","a17733980ea8472c8878a56187a26ea1","6895e8451a6f40f2b812b5fb902d9108","53130449395d4db88f80464d2a8c10b5","fad1bfa6ab1c4bbfa6a576b3e4d27ed2","e480fee0c6ea46f4bfa21d5e67b10b2d","7b1a8018a9d3445c92973aae1e297710","91bd7169bc54413a935d5a129ebb2384","2ad7308f3f8e4122be1ed2c6a5ed1343","c95e6b746cf1463783536f0f5dd84713","349d455be24b4d439828b7034f073f9a","a7dac64f02bd4715ba21d8ebee16456f","589240a331fc4f99ad8263492911573d","405049a9c262426994b0b26f21aac54b","efa540d6d9434a2fbc3dc9da75524ec5","7490b1d2d1bd44359b3260117e5f86d9","2770df2a72eb46cba7f6a3f84c66cfee","87972a49b6834c708eedc8eb5957398f","5f72a381666c4c9bb6458e4cf0f63298","de35ef99e93c4e4ab6eb484474f3c5f7","71e3f59480ff4dc5ac4f0a2e793ee131","50ea2f0b62674f118e0617479729fe96","771f598928e3423f931b0a2adf0d07be","c4c3a800295544498a98699d73dfef5b","746daa80ded442ecb03865b932b08f2b","173b91dc9b244237866692da1a7452c7","56f504d9ee964609b42f0b56df0b0dd2","43c4463f9d5d49a0a21e90a2922134fb","e23b4fb1841a43d6ae2d4acd3c663465","6897b5710b9a446bb9d752c2fa4263b8","3ed7cfc48a4c4e8c98b506e9f132cdb6","f603f3cc30084b8baa3ab63dd6b13cd4","61d1db653a804bcbad06a1aaa30c314a","4efcc1d3020f4470b266cf71644bad6f","549e968c25034051a0604328e9279e9a","f4a6353654834f45a01aa324b97d6c3c","bced6d64ab8449a98a3d9751db89bdd1","b2dc6d84489c4ac6a390be8baa51c16d","cf1ee0f2698c40bdbd85ab6f1f94edf1","0842ecbcd5c34a34ab66a666bc837434","e50889d4d8a4425680a57d404bd2d86b","385a3517b7694055a2d546a920702d63","62930ee9985a46fcab640762a28a1a0d","c32b752bc45d44d080be128d0d94ed82","8d24166ec711435ca53e0f6abf6dde3c","17ed2d961a3f44b0a8a68421d0b3ef9b","ea2ec46b51e04a4a8e17415e99c2d1f9","000fbab5c4264019a01be791335fc28d"]},"executionInfo":{"elapsed":70591,"status":"ok","timestamp":1746667331582,"user":{"displayName":"Nithin Keshavamurthy","userId":"17129339458019700030"},"user_tz":420},"id":"whCkhRXDXiql","outputId":"f2a6e062-e070-4897-c0f7-5d6642977fa1"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"91a56fba160641d38cb4ad430f2e4024","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"502d64ae43b5449fac7fb0a616d34b22","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ce31bf68bdbc4597959d8cb603ee6744","version_major":2,"version_minor":0},"text/plain":["Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fad1bfa6ab1c4bbfa6a576b3e4d27ed2","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7490b1d2d1bd44359b3260117e5f86d9","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"56f504d9ee964609b42f0b56df0b0dd2","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b2dc6d84489c4ac6a390be8baa51c16d","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Enabled gradients for base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight\n"]}],"source":["import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from peft import PeftModel, LoraConfig, get_peft_model, TaskType\n","\n","# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","# Load base model without device mapping\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    \"mistralai/Mistral-7B-Instruct-v0.1\",\n","    torch_dtype=torch.float16,\n","    # Don't use device_map here\n",")\n","\n","# Move to CUDA\n","base_model = base_model.cuda()\n","\n","# Create fresh LoRA configuration\n","peft_config = LoraConfig(\n","    r=16,\n","\n","    lora_alpha=32,\n","    target_modules=[\"q_proj\", \"v_proj\"],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=TaskType.CAUSAL_LM\n",")\n","\n","# Apply LoRA\n","model = get_peft_model(base_model, peft_config)\n","\n","# Load the weights\n","from safetensors.torch import load_file\n","import os\n","\n","checkpoint_path = \"/content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_spider_lora/checkpoint-1311\"\n","safetensors_path = os.path.join(checkpoint_path, \"adapter_model.safetensors\")\n","\n","state_dict = load_file(safetensors_path)\n","model.load_state_dict(state_dict, strict=False)\n","\n","# CRITICAL: Make sure parameters are set to require gradients\n","for name, param in model.named_parameters():\n","    if 'lora' in name:  # Only LoRA parameters should be trained\n","        param.requires_grad = True\n","        print(f\"Enabled gradients for {name}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1746667337937,"user":{"displayName":"Nithin Keshavamurthy","userId":"17129339458019700030"},"user_tz":420},"id":"aF7zyFrGXnKY","outputId":"2242cfb9-7699-4354-fd83-2bd1690fd577"},"outputs":[{"name":"stdout","output_type":"stream","text":["Set evaluation parameters\n"]}],"source":["# Set up training arguments\n","from transformers import TrainingArguments\n","\n","# Define output directory\n","output_dir = \"/content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_complex_sql_continued\"\n","\n","# Define training arguments optimized for A100\n","training_args = TrainingArguments(\n","    output_dir=output_dir,\n","    per_device_train_batch_size=4,      # Balance between speed and memory\n","    per_device_eval_batch_size=4,       # Evaluation batch size\n","    gradient_accumulation_steps=8,      # Effective batch size = 4 x 8 = 32\n","    num_train_epochs=2,                 # Two epochs should be sufficient\n","    learning_rate=5e-5,                 # Lower learning rate for continuing training\n","    lr_scheduler_type=\"cosine\",         # Cosine schedule with warmup\n","    warmup_ratio=0.1,                   # Longer warmup phase\n","    weight_decay=0.01,                  # Regularization\n","    fp16=True,                          # Mixed precision for speed\n","    gradient_checkpointing=True,        # Memory optimization\n","    logging_steps=10,                   # Log frequently\n","    save_strategy=\"steps\",              # Save periodically\n","    save_steps=500,                     # Save every 500 steps\n","    # Removed evaluation_strategy and eval_steps\n","    load_best_model_at_end=False,       # Changed to False since we're not evaluating\n","    # Removed metric_for_best_model\n","    # Removed greater_is_better\n","    report_to=\"none\",                   # Disable reporting\n","    remove_unused_columns=False,        # Keep all columns\n","    optim=\"adamw_torch\",                # Use AdamW optimizer\n","    max_grad_norm=0.3,                  # Gradient clipping for stability\n",")\n","\n","# If you still want evaluation, check if your version supports these parameters\n","try:\n","    training_args.eval_strategy = \"steps\"\n","    training_args.eval_steps = 500\n","    print(\"Set evaluation parameters\")\n","except:\n","    print(\"Error\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4kJDFn-lZ17a"},"outputs":[],"source":["# Manual training loop approach\n","import torch\n","from torch.utils.data import DataLoader\n","\n","# Create a simple dataloader with a smaller batch size\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=1,  # Start with batch size 1 for debugging\n","    shuffle=True\n",")\n","\n","# Set up optimizer just for the LoRA parameters\n","optimizer = torch.optim.AdamW(\n","    [p for p in model.parameters() if p.requires_grad],\n","    lr=2e-5,\n","    weight_decay=0.01\n",")\n","\n","# Put model in training mode\n","model.train()\n","\n","# Simple training loop\n","print(\"Starting manual training loop\")\n","for epoch in range(2):  # 2 epochs\n","    running_loss = 0.0\n","    for step, batch in enumerate(train_dataloader):\n","        # Move batch to GPU\n","        batch = {k: v.cuda() for k, v in batch.items()}\n","\n","        # Forward pass with explicit gradient tracking\n","        outputs = model(\n","            input_ids=batch[\"input_ids\"],\n","            attention_mask=batch[\"attention_mask\"],\n","            labels=batch[\"labels\"],\n","            use_cache=False  # Disable cache to avoid issues\n","        )\n","\n","        loss = outputs.loss\n","\n","        # Check if loss has gradients\n","        if loss.requires_grad:\n","            print(f\"Step {step}: Loss requires grad: {loss.requires_grad}\")\n","        else:\n","            print(f\"Step {step}: Loss does NOT require grad!\")\n","\n","        # Manual backward pass\n","        loss.backward()\n","\n","        # Check if any gradients were actually computed\n","        has_grad = False\n","        for name, param in model.named_parameters():\n","            if param.requires_grad and param.grad is not None:\n","                has_grad = True\n","                break\n","\n","        if has_grad:\n","            print(f\"Step {step}: Gradients computed successfully\")\n","        else:\n","            print(f\"Step {step}: No gradients were computed!\")\n","\n","        # Update weights\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","        # Track loss\n","        running_loss += loss.item()\n","\n","        # Log progress\n","        if step % 10 == 0:\n","            print(f\"Epoch {epoch}, Step {step}, Loss {loss.item():.4f}, Avg Loss: {running_loss/(step+1):.4f}\")\n","\n","        # Save checkpoint periodically\n","        if step % 500 == 0 and step > 0:\n","            checkpoint_dir = f\"/content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_complex_sql_continued/manual-epoch-{epoch}-step-{step}\"\n","            model.save_pretrained(checkpoint_dir)\n","            tokenizer.save_pretrained(checkpoint_dir)\n","            print(f\"Saved checkpoint to {checkpoint_dir}\")\n","\n","    # Save at end of epoch\n","    epoch_dir = f\"/content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_complex_sql_continued/manual-epoch-{epoch}\"\n","    model.save_pretrained(epoch_dir)\n","    tokenizer.save_pretrained(epoch_dir)\n","    print(f\"Epoch {epoch} completed. Average loss: {running_loss / len(train_dataloader):.4f}\")\n","\n","# Save final model\n","final_dir = f\"/content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_complex_sql_continued/spider_x_synsql_model\"\n","model.save_pretrained(final_dir)\n","tokenizer.save_pretrained(final_dir)\n","print(f\"Training complete! Final model saved to {final_dir}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["12e1b1799bfb473a99813719acd57636","030b649008bb451694d77d5741b6f1d0","cb984e030eda435ea8b8a1d7d0e58ee5","bac8ed097fcf4d9a9dbaf2307bbaec07","ce644f40f2744f018aff76fc8c202c5d","fd2b51237efe4d3da125ffd83ae71696","112cb17bfd85490aba0571bf0b5ee143","bc2be97ff9e6412db9f42b6feb213a1d","659ad925b9e14dc587d8cd54383a468f","b52b9a20f2b545c689d9d4cd7c94a531","c6170d9782d5417d96885e88151c1e11"]},"executionInfo":{"elapsed":2058917,"status":"error","timestamp":1746669549047,"user":{"displayName":"Nithin Keshavamurthy","userId":"17129339458019700030"},"user_tz":420},"id":"6XKArUm6aa6a","outputId":"f6cb58b5-63a2-4754-ff9b-a63eb6088748"},"outputs":[{"name":"stdout","output_type":"stream","text":["Merging adapter into base model...\n","Saved merged model to /content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_merged_ready\n","Loading merged model...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"12e1b1799bfb473a99813719acd57636","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Step 6593: Gradients computed successfully\n","Step 6594: Loss: 0.1017, requires_grad: True\n","Step 6594: Gradients computed successfully\n","Step 6595: Loss: 0.2723, requires_grad: True\n","Step 6595: Gradients computed successfully\n","Step 6596: Loss: 0.3878, requires_grad: True\n","Step 6596: Gradients computed successfully\n","Step 6597: Loss: 0.3190, requires_grad: True\n","Step 6597: Gradients computed successfully\n","Step 6598: Loss: 0.2701, requires_grad: True\n","Step 6598: Gradients computed successfully\n","Step 6599: Loss: 0.1377, requires_grad: True\n","Step 6599: Gradients computed successfully\n","Step 6600: Loss: 0.1214, requires_grad: True\n","Step 6600: Gradients computed successfully\n","Epoch 0, Step 6600, Loss 0.1214, Avg Loss: 0.2576\n","Step 6601: Loss: 0.2008, requires_grad: True\n","Step 6601: Gradients computed successfully\n","Step 6602: Loss: 0.1291, requires_grad: True\n","Step 6602: Gradients computed successfully\n","Step 6603: Loss: 0.3323, requires_grad: True\n","Step 6603: Gradients computed successfully\n","Step 6604: Loss: 0.1830, requires_grad: True\n","Step 6604: Gradients computed successfully\n","Step 6605: Loss: 0.2423, requires_grad: True\n","Step 6605: Gradients computed successfully\n","Step 6606: Loss: 0.2410, requires_grad: True\n","Step 6606: Gradients computed successfully\n","Step 6607: Loss: 0.1251, requires_grad: True\n","Step 6607: Gradients computed successfully\n","Step 6608: Loss: 0.1907, requires_grad: True\n","Step 6608: Gradients computed successfully\n","Step 6609: Loss: 0.1645, requires_grad: True\n","Step 6609: Gradients computed successfully\n","Step 6610: Loss: 0.1284, requires_grad: True\n","Step 6610: Gradients computed successfully\n","Epoch 0, Step 6610, Loss 0.1284, Avg Loss: 0.2575\n","Step 6611: Loss: 0.2363, requires_grad: True\n","Step 6611: Gradients computed successfully\n","Step 6612: Loss: 0.1613, requires_grad: True\n","Step 6612: Gradients computed successfully\n","Step 6613: Loss: 0.1362, requires_grad: True\n","Step 6613: Gradients computed successfully\n","Step 6614: Loss: 0.1636, requires_grad: True\n","Step 6614: Gradients computed successfully\n","Step 6615: Loss: 0.2246, requires_grad: True\n","Step 6615: Gradients computed successfully\n","Step 6616: Loss: 0.1504, requires_grad: True\n","Step 6616: Gradients computed successfully\n","Step 6617: Loss: 0.3885, requires_grad: True\n","Step 6617: Gradients computed successfully\n","Step 6618: Loss: 0.1872, requires_grad: True\n","Step 6618: Gradients computed successfully\n","Step 6619: Loss: 0.3420, requires_grad: True\n","Step 6619: Gradients computed successfully\n","Step 6620: Loss: 0.2279, requires_grad: True\n","Step 6620: Gradients computed successfully\n","Epoch 0, Step 6620, Loss 0.2279, Avg Loss: 0.2575\n","Step 6621: Loss: 0.0929, requires_grad: True\n","Step 6621: Gradients computed successfully\n","Step 6622: Loss: 0.2154, requires_grad: True\n","Step 6622: Gradients computed successfully\n","Step 6623: Loss: 0.7075, requires_grad: True\n","Step 6623: Gradients computed successfully\n","Step 6624: Loss: 0.1610, requires_grad: True\n","Step 6624: Gradients computed successfully\n","Step 6625: Loss: 0.2578, requires_grad: True\n","Step 6625: Gradients computed successfully\n","Step 6626: Loss: 0.0661, requires_grad: True\n","Step 6626: Gradients computed successfully\n","Step 6627: Loss: 0.3126, requires_grad: True\n","Step 6627: Gradients computed successfully\n","Step 6628: Loss: 0.3409, requires_grad: True\n","Step 6628: Gradients computed successfully\n","Step 6629: Loss: 0.1679, requires_grad: True\n","Step 6629: Gradients computed successfully\n","Step 6630: Loss: 0.1668, requires_grad: True\n","Step 6630: Gradients computed successfully\n","Epoch 0, Step 6630, Loss 0.1668, Avg Loss: 0.2574\n","Step 6631: Loss: 0.1358, requires_grad: True\n","Step 6631: Gradients computed successfully\n","Step 6632: Loss: 0.2749, requires_grad: True\n","Step 6632: Gradients computed successfully\n","Step 6633: Loss: 0.1184, requires_grad: True\n","Step 6633: Gradients computed successfully\n","Step 6634: Loss: 0.1359, requires_grad: True\n","Step 6634: Gradients computed successfully\n","Step 6635: Loss: 0.2819, requires_grad: True\n","Step 6635: Gradients computed successfully\n","Step 6636: Loss: 0.0953, requires_grad: True\n","Step 6636: Gradients computed successfully\n","Step 6637: Loss: 0.3229, requires_grad: True\n","Step 6637: Gradients computed successfully\n","Step 6638: Loss: 0.1826, requires_grad: True\n","Step 6638: Gradients computed successfully\n","Step 6639: Loss: 0.3013, requires_grad: True\n","Step 6639: Gradients computed successfully\n","Step 6640: Loss: 0.2816, requires_grad: True\n","Step 6640: Gradients computed successfully\n","Epoch 0, Step 6640, Loss 0.2816, Avg Loss: 0.2574\n","Step 6641: Loss: 0.1343, requires_grad: True\n","Step 6641: Gradients computed successfully\n","Step 6642: Loss: 0.1512, requires_grad: True\n","Step 6642: Gradients computed successfully\n","Step 6643: Loss: 0.3341, requires_grad: True\n","Step 6643: Gradients computed successfully\n","Step 6644: Loss: 0.3650, requires_grad: True\n","Step 6644: Gradients computed successfully\n","Step 6645: Loss: 0.2307, requires_grad: True\n","Step 6645: Gradients computed successfully\n","Step 6646: Loss: 0.3202, requires_grad: True\n","Step 6646: Gradients computed successfully\n","Step 6647: Loss: 0.2742, requires_grad: True\n","Step 6647: Gradients computed successfully\n","Step 6648: Loss: 0.2827, requires_grad: True\n","Step 6648: Gradients computed successfully\n","Step 6649: Loss: 0.2116, requires_grad: True\n","Step 6649: Gradients computed successfully\n","Step 6650: Loss: 0.0553, requires_grad: True\n","Step 6650: Gradients computed successfully\n","Epoch 0, Step 6650, Loss 0.0553, Avg Loss: 0.2574\n","Step 6651: Loss: 0.2923, requires_grad: True\n","Step 6651: Gradients computed successfully\n","Step 6652: Loss: 0.1277, requires_grad: True\n","Step 6652: Gradients computed successfully\n","Step 6653: Loss: 0.3966, requires_grad: True\n","Step 6653: Gradients computed successfully\n","Step 6654: Loss: 0.3204, requires_grad: True\n","Step 6654: Gradients computed successfully\n","Step 6655: Loss: 0.1501, requires_grad: True\n","Step 6655: Gradients computed successfully\n","Step 6656: Loss: 0.1339, requires_grad: True\n","Step 6656: Gradients computed successfully\n","Step 6657: Loss: 0.3147, requires_grad: True\n","Step 6657: Gradients computed successfully\n","Step 6658: Loss: 0.1125, requires_grad: True\n","Step 6658: Gradients computed successfully\n","Step 6659: Loss: 0.0722, requires_grad: True\n","Step 6659: Gradients computed successfully\n","Step 6660: Loss: 0.2865, requires_grad: True\n","Step 6660: Gradients computed successfully\n","Epoch 0, Step 6660, Loss 0.2865, Avg Loss: 0.2573\n","Step 6661: Loss: 0.3630, requires_grad: True\n","Step 6661: Gradients computed successfully\n","Step 6662: Loss: 0.1955, requires_grad: True\n","Step 6662: Gradients computed successfully\n","Step 6663: Loss: 0.5111, requires_grad: True\n","Step 6663: Gradients computed successfully\n","Step 6664: Loss: 0.3003, requires_grad: True\n","Step 6664: Gradients computed successfully\n","Step 6665: Loss: 0.1539, requires_grad: True\n","Step 6665: Gradients computed successfully\n","Step 6666: Loss: 0.1829, requires_grad: True\n","Step 6666: Gradients computed successfully\n","Step 6667: Loss: 0.2259, requires_grad: True\n","Step 6667: Gradients computed successfully\n","Step 6668: Loss: 0.2824, requires_grad: True\n","Step 6668: Gradients computed successfully\n","Step 6669: Loss: 0.1816, requires_grad: True\n","Step 6669: Gradients computed successfully\n","Step 6670: Loss: 0.1793, requires_grad: True\n","Step 6670: Gradients computed successfully\n","Epoch 0, Step 6670, Loss 0.1793, Avg Loss: 0.2573\n","Step 6671: Loss: 0.2401, requires_grad: True\n","Step 6671: Gradients computed successfully\n","Step 6672: Loss: 0.3548, requires_grad: True\n","Step 6672: Gradients computed successfully\n","Step 6673: Loss: 0.3263, requires_grad: True\n","Step 6673: Gradients computed successfully\n","Step 6674: Loss: 0.1057, requires_grad: True\n","Step 6674: Gradients computed successfully\n","Step 6675: Loss: 0.2464, requires_grad: True\n","Step 6675: Gradients computed successfully\n","Step 6676: Loss: 0.1731, requires_grad: True\n","Step 6676: Gradients computed successfully\n","Step 6677: Loss: 0.1731, requires_grad: True\n","Step 6677: Gradients computed successfully\n","Step 6678: Loss: 0.1390, requires_grad: True\n","Step 6678: Gradients computed successfully\n","Step 6679: Loss: 0.4735, requires_grad: True\n","Step 6679: Gradients computed successfully\n","Step 6680: Loss: 0.1101, requires_grad: True\n","Step 6680: Gradients computed successfully\n","Epoch 0, Step 6680, Loss 0.1101, Avg Loss: 0.2573\n","Step 6681: Loss: 0.1278, requires_grad: True\n","Step 6681: Gradients computed successfully\n","Step 6682: Loss: 0.2965, requires_grad: True\n","Step 6682: Gradients computed successfully\n","Step 6683: Loss: 0.1639, requires_grad: True\n","Step 6683: Gradients computed successfully\n","Step 6684: Loss: 0.2833, requires_grad: True\n","Step 6684: Gradients computed successfully\n","Step 6685: Loss: 0.2536, requires_grad: True\n","Step 6685: Gradients computed successfully\n","Step 6686: Loss: 0.2546, requires_grad: True\n","Step 6686: Gradients computed successfully\n","Step 6687: Loss: 0.3334, requires_grad: True\n","Step 6687: Gradients computed successfully\n","Step 6688: Loss: 0.4421, requires_grad: True\n","Step 6688: Gradients computed successfully\n","Step 6689: Loss: 0.1698, requires_grad: True\n","Step 6689: Gradients computed successfully\n","Step 6690: Loss: 0.1467, requires_grad: True\n","Step 6690: Gradients computed successfully\n","Epoch 0, Step 6690, Loss 0.1467, Avg Loss: 0.2572\n","Step 6691: Loss: 0.1894, requires_grad: True\n","Step 6691: Gradients computed successfully\n","Step 6692: Loss: 0.2501, requires_grad: True\n","Step 6692: Gradients computed successfully\n","Step 6693: Loss: 0.2662, requires_grad: True\n","Step 6693: Gradients computed successfully\n","Step 6694: Loss: 0.1051, requires_grad: True\n","Step 6694: Gradients computed successfully\n","Step 6695: Loss: 0.0877, requires_grad: True\n","Step 6695: Gradients computed successfully\n","Step 6696: Loss: 0.1478, requires_grad: True\n","Step 6696: Gradients computed successfully\n","Step 6697: Loss: 0.0941, requires_grad: True\n","Step 6697: Gradients computed successfully\n","Step 6698: Loss: 0.1842, requires_grad: True\n","Step 6698: Gradients computed successfully\n","Step 6699: Loss: 0.1312, requires_grad: True\n","Step 6699: Gradients computed successfully\n","Step 6700: Loss: 0.2039, requires_grad: True\n","Step 6700: Gradients computed successfully\n","Epoch 0, Step 6700, Loss 0.2039, Avg Loss: 0.2571\n","Step 6701: Loss: 0.1697, requires_grad: True\n","Step 6701: Gradients computed successfully\n","Step 6702: Loss: 0.1572, requires_grad: True\n","Step 6702: Gradients computed successfully\n","Step 6703: Loss: 0.1423, requires_grad: True\n","Step 6703: Gradients computed successfully\n","Step 6704: Loss: 0.1577, requires_grad: True\n","Step 6704: Gradients computed successfully\n","Step 6705: Loss: 0.2519, requires_grad: True\n","Step 6705: Gradients computed successfully\n","Step 6706: Loss: 0.2541, requires_grad: True\n","Step 6706: Gradients computed successfully\n","Step 6707: Loss: 0.3019, requires_grad: True\n","Step 6707: Gradients computed successfully\n","Step 6708: Loss: 0.1765, requires_grad: True\n","Step 6708: Gradients computed successfully\n","Step 6709: Loss: 0.1533, requires_grad: True\n","Step 6709: Gradients computed successfully\n","Step 6710: Loss: 0.1970, requires_grad: True\n","Step 6710: Gradients computed successfully\n","Epoch 0, Step 6710, Loss 0.1970, Avg Loss: 0.2570\n","Step 6711: Loss: 0.1880, requires_grad: True\n","Step 6711: Gradients computed successfully\n","Step 6712: Loss: 0.3188, requires_grad: True\n","Step 6712: Gradients computed successfully\n","Step 6713: Loss: 0.7129, requires_grad: True\n","Step 6713: Gradients computed successfully\n","Step 6714: Loss: 0.1251, requires_grad: True\n","Step 6714: Gradients computed successfully\n","Step 6715: Loss: 0.0996, requires_grad: True\n","Step 6715: Gradients computed successfully\n","Step 6716: Loss: 0.2791, requires_grad: True\n","Step 6716: Gradients computed successfully\n","Step 6717: Loss: 0.3052, requires_grad: True\n","Step 6717: Gradients computed successfully\n","Step 6718: Loss: 0.3751, requires_grad: True\n","Step 6718: Gradients computed successfully\n","Step 6719: Loss: 0.4082, requires_grad: True\n","Step 6719: Gradients computed successfully\n","Step 6720: Loss: 0.1755, requires_grad: True\n","Step 6720: Gradients computed successfully\n","Epoch 0, Step 6720, Loss 0.1755, Avg Loss: 0.2571\n","Step 6721: Loss: 0.2886, requires_grad: True\n","Step 6721: Gradients computed successfully\n","Step 6722: Loss: 0.1312, requires_grad: True\n","Step 6722: Gradients computed successfully\n","Step 6723: Loss: 0.3240, requires_grad: True\n","Step 6723: Gradients computed successfully\n","Step 6724: Loss: 0.2466, requires_grad: True\n","Step 6724: Gradients computed successfully\n","Step 6725: Loss: 0.2898, requires_grad: True\n","Step 6725: Gradients computed successfully\n","Step 6726: Loss: 0.1599, requires_grad: True\n","Step 6726: Gradients computed successfully\n","Step 6727: Loss: 0.4267, requires_grad: True\n","Step 6727: Gradients computed successfully\n","Step 6728: Loss: 0.2743, requires_grad: True\n","Step 6728: Gradients computed successfully\n","Step 6729: Loss: 0.0665, requires_grad: True\n","Step 6729: Gradients computed successfully\n","Step 6730: Loss: 0.3146, requires_grad: True\n","Step 6730: Gradients computed successfully\n","Epoch 0, Step 6730, Loss 0.3146, Avg Loss: 0.2571\n","Step 6731: Loss: 0.1910, requires_grad: True\n","Step 6731: Gradients computed successfully\n","Step 6732: Loss: 0.3791, requires_grad: True\n","Step 6732: Gradients computed successfully\n","Step 6733: Loss: 0.1195, requires_grad: True\n","Step 6733: Gradients computed successfully\n","Step 6734: Loss: 0.2924, requires_grad: True\n","Step 6734: Gradients computed successfully\n","Step 6735: Loss: 0.2176, requires_grad: True\n","Step 6735: Gradients computed successfully\n","Step 6736: Loss: 0.1361, requires_grad: True\n","Step 6736: Gradients computed successfully\n","Step 6737: Loss: 0.1719, requires_grad: True\n","Step 6737: Gradients computed successfully\n","Step 6738: Loss: 0.1783, requires_grad: True\n","Step 6738: Gradients computed successfully\n","Step 6739: Loss: 0.1525, requires_grad: True\n","Step 6739: Gradients computed successfully\n","Step 6740: Loss: 0.4998, requires_grad: True\n","Step 6740: Gradients computed successfully\n","Epoch 0, Step 6740, Loss 0.4998, Avg Loss: 0.2570\n","Step 6741: Loss: 0.2693, requires_grad: True\n","Step 6741: Gradients computed successfully\n","Step 6742: Loss: 0.1853, requires_grad: True\n","Step 6742: Gradients computed successfully\n","Step 6743: Loss: 0.2376, requires_grad: True\n","Step 6743: Gradients computed successfully\n","Step 6744: Loss: 0.2295, requires_grad: True\n","Step 6744: Gradients computed successfully\n","Step 6745: Loss: 0.2315, requires_grad: True\n","Step 6745: Gradients computed successfully\n","Step 6746: Loss: 0.1747, requires_grad: True\n","Step 6746: Gradients computed successfully\n","Step 6747: Loss: 0.2068, requires_grad: True\n","Step 6747: Gradients computed successfully\n","Step 6748: Loss: 0.4833, requires_grad: True\n","Step 6748: Gradients computed successfully\n","Step 6749: Loss: 0.1303, requires_grad: True\n","Step 6749: Gradients computed successfully\n","Step 6750: Loss: 0.3335, requires_grad: True\n","Step 6750: Gradients computed successfully\n","Epoch 0, Step 6750, Loss 0.3335, Avg Loss: 0.2570\n","Step 6751: Loss: 0.3820, requires_grad: True\n","Step 6751: Gradients computed successfully\n","Step 6752: Loss: 0.2262, requires_grad: True\n","Step 6752: Gradients computed successfully\n","Step 6753: Loss: 0.2492, requires_grad: True\n","Step 6753: Gradients computed successfully\n","Step 6754: Loss: 0.3266, requires_grad: True\n","Step 6754: Gradients computed successfully\n","Step 6755: Loss: 0.1519, requires_grad: True\n","Step 6755: Gradients computed successfully\n","Step 6756: Loss: 0.3490, requires_grad: True\n","Step 6756: Gradients computed successfully\n","Step 6757: Loss: 0.1863, requires_grad: True\n","Step 6757: Gradients computed successfully\n","Step 6758: Loss: 0.3219, requires_grad: True\n","Step 6758: Gradients computed successfully\n","Step 6759: Loss: 0.0904, requires_grad: True\n","Step 6759: Gradients computed successfully\n","Step 6760: Loss: 0.2360, requires_grad: True\n","Step 6760: Gradients computed successfully\n","Epoch 0, Step 6760, Loss 0.2360, Avg Loss: 0.2570\n","Step 6761: Loss: 0.1588, requires_grad: True\n","Step 6761: Gradients computed successfully\n","Step 6762: Loss: 0.3226, requires_grad: True\n","Step 6762: Gradients computed successfully\n","Step 6763: Loss: 0.5517, requires_grad: True\n","Step 6763: Gradients computed successfully\n","Step 6764: Loss: 0.1352, requires_grad: True\n","Step 6764: Gradients computed successfully\n","Step 6765: Loss: 0.2917, requires_grad: True\n","Step 6765: Gradients computed successfully\n","Step 6766: Loss: 0.2274, requires_grad: True\n","Step 6766: Gradients computed successfully\n","Step 6767: Loss: 0.3065, requires_grad: True\n","Step 6767: Gradients computed successfully\n","Step 6768: Loss: 0.3081, requires_grad: True\n","Step 6768: Gradients computed successfully\n","Step 6769: Loss: 0.3066, requires_grad: True\n","Step 6769: Gradients computed successfully\n","Step 6770: Loss: 0.2024, requires_grad: True\n","Step 6770: Gradients computed successfully\n","Epoch 0, Step 6770, Loss 0.2024, Avg Loss: 0.2571\n","Step 6771: Loss: 0.0890, requires_grad: True\n","Step 6771: Gradients computed successfully\n","Step 6772: Loss: 0.0877, requires_grad: True\n","Step 6772: Gradients computed successfully\n","Step 6773: Loss: 0.1754, requires_grad: True\n","Step 6773: Gradients computed successfully\n","Step 6774: Loss: 0.1989, requires_grad: True\n","Step 6774: Gradients computed successfully\n","Step 6775: Loss: 0.2482, requires_grad: True\n","Step 6775: Gradients computed successfully\n","Step 6776: Loss: 0.1781, requires_grad: True\n","Step 6776: Gradients computed successfully\n","Step 6777: Loss: 0.2781, requires_grad: True\n","Step 6777: Gradients computed successfully\n","Step 6778: Loss: 0.3480, requires_grad: True\n","Step 6778: Gradients computed successfully\n","Step 6779: Loss: 0.3150, requires_grad: True\n","Step 6779: Gradients computed successfully\n","Step 6780: Loss: 0.1160, requires_grad: True\n","Step 6780: Gradients computed successfully\n","Epoch 0, Step 6780, Loss 0.1160, Avg Loss: 0.2570\n","Step 6781: Loss: 0.2406, requires_grad: True\n","Step 6781: Gradients computed successfully\n","Step 6782: Loss: 0.1277, requires_grad: True\n","Step 6782: Gradients computed successfully\n","Step 6783: Loss: 0.4045, requires_grad: True\n","Step 6783: Gradients computed successfully\n","Step 6784: Loss: 0.2314, requires_grad: True\n","Step 6784: Gradients computed successfully\n","Step 6785: Loss: 0.1479, requires_grad: True\n","Step 6785: Gradients computed successfully\n","Step 6786: Loss: 0.3699, requires_grad: True\n","Step 6786: Gradients computed successfully\n","Step 6787: Loss: 0.2948, requires_grad: True\n","Step 6787: Gradients computed successfully\n","Step 6788: Loss: 0.2574, requires_grad: True\n","Step 6788: Gradients computed successfully\n","Step 6789: Loss: 0.0856, requires_grad: True\n","Step 6789: Gradients computed successfully\n","Step 6790: Loss: 0.2422, requires_grad: True\n","Step 6790: Gradients computed successfully\n","Epoch 0, Step 6790, Loss 0.2422, Avg Loss: 0.2570\n","Step 6791: Loss: 0.2057, requires_grad: True\n","Step 6791: Gradients computed successfully\n","Step 6792: Loss: 0.3551, requires_grad: True\n","Step 6792: Gradients computed successfully\n","Step 6793: Loss: 0.1996, requires_grad: True\n","Step 6793: Gradients computed successfully\n","Step 6794: Loss: 0.1524, requires_grad: True\n","Step 6794: Gradients computed successfully\n","Step 6795: Loss: 0.1076, requires_grad: True\n","Step 6795: Gradients computed successfully\n","Step 6796: Loss: 0.2665, requires_grad: True\n","Step 6796: Gradients computed successfully\n","Step 6797: Loss: 0.1645, requires_grad: True\n","Step 6797: Gradients computed successfully\n","Step 6798: Loss: 0.1673, requires_grad: True\n","Step 6798: Gradients computed successfully\n","Step 6799: Loss: 0.1279, requires_grad: True\n","Step 6799: Gradients computed successfully\n","Step 6800: Loss: 0.1617, requires_grad: True\n","Step 6800: Gradients computed successfully\n","Epoch 0, Step 6800, Loss 0.1617, Avg Loss: 0.2569\n","Step 6801: Loss: 0.0808, requires_grad: True\n","Step 6801: Gradients computed successfully\n","Step 6802: Loss: 0.1356, requires_grad: True\n","Step 6802: Gradients computed successfully\n","Step 6803: Loss: 0.0736, requires_grad: True\n","Step 6803: Gradients computed successfully\n","Step 6804: Loss: 0.1190, requires_grad: True\n","Step 6804: Gradients computed successfully\n","Step 6805: Loss: 0.1253, requires_grad: True\n","Step 6805: Gradients computed successfully\n","Step 6806: Loss: 0.1771, requires_grad: True\n","Step 6806: Gradients computed successfully\n","Step 6807: Loss: 0.1783, requires_grad: True\n","Step 6807: Gradients computed successfully\n","Step 6808: Loss: 0.2353, requires_grad: True\n","Step 6808: Gradients computed successfully\n","Step 6809: Loss: 0.1336, requires_grad: True\n","Step 6809: Gradients computed successfully\n","Step 6810: Loss: 0.2767, requires_grad: True\n","Step 6810: Gradients computed successfully\n","Epoch 0, Step 6810, Loss 0.2767, Avg Loss: 0.2567\n","Step 6811: Loss: 0.1310, requires_grad: True\n","Step 6811: Gradients computed successfully\n","Step 6812: Loss: 0.2350, requires_grad: True\n","Step 6812: Gradients computed successfully\n","Step 6813: Loss: 0.1607, requires_grad: True\n","Step 6813: Gradients computed successfully\n","Step 6814: Loss: 0.4192, requires_grad: True\n","Step 6814: Gradients computed successfully\n","Step 6815: Loss: 0.1300, requires_grad: True\n","Step 6815: Gradients computed successfully\n","Step 6816: Loss: 0.3452, requires_grad: True\n","Step 6816: Gradients computed successfully\n","Step 6817: Loss: 0.2079, requires_grad: True\n","Step 6817: Gradients computed successfully\n","Step 6818: Loss: 0.2008, requires_grad: True\n","Step 6818: Gradients computed successfully\n","Step 6819: Loss: 0.3598, requires_grad: True\n","Step 6819: Gradients computed successfully\n","Step 6820: Loss: 0.1300, requires_grad: True\n","Step 6820: Gradients computed successfully\n","Epoch 0, Step 6820, Loss 0.1300, Avg Loss: 0.2567\n","Step 6821: Loss: 0.2302, requires_grad: True\n","Step 6821: Gradients computed successfully\n","Step 6822: Loss: 0.3818, requires_grad: True\n","Step 6822: Gradients computed successfully\n","Step 6823: Loss: 0.2263, requires_grad: True\n","Step 6823: Gradients computed successfully\n","Step 6824: Loss: 0.1077, requires_grad: True\n","Step 6824: Gradients computed successfully\n","Step 6825: Loss: 0.3859, requires_grad: True\n","Step 6825: Gradients computed successfully\n","Step 6826: Loss: 0.1599, requires_grad: True\n","Step 6826: Gradients computed successfully\n","Step 6827: Loss: 0.2624, requires_grad: True\n","Step 6827: Gradients computed successfully\n","Step 6828: Loss: 0.1697, requires_grad: True\n","Step 6828: Gradients computed successfully\n","Step 6829: Loss: 0.3258, requires_grad: True\n","Step 6829: Gradients computed successfully\n","Step 6830: Loss: 0.1967, requires_grad: True\n","Step 6830: Gradients computed successfully\n","Epoch 0, Step 6830, Loss 0.1967, Avg Loss: 0.2566\n","Step 6831: Loss: 0.2224, requires_grad: True\n","Step 6831: Gradients computed successfully\n","Step 6832: Loss: 0.1735, requires_grad: True\n","Step 6832: Gradients computed successfully\n","Step 6833: Loss: 0.1225, requires_grad: True\n","Step 6833: Gradients computed successfully\n","Step 6834: Loss: 0.1502, requires_grad: True\n","Step 6834: Gradients computed successfully\n","Step 6835: Loss: 0.4561, requires_grad: True\n","Step 6835: Gradients computed successfully\n","Step 6836: Loss: 0.3417, requires_grad: True\n","Step 6836: Gradients computed successfully\n","Step 6837: Loss: 0.3754, requires_grad: True\n","Step 6837: Gradients computed successfully\n","Step 6838: Loss: 0.1823, requires_grad: True\n","Step 6838: Gradients computed successfully\n","Step 6839: Loss: 0.4157, requires_grad: True\n","Step 6839: Gradients computed successfully\n","Step 6840: Loss: 0.2977, requires_grad: True\n","Step 6840: Gradients computed successfully\n","Epoch 0, Step 6840, Loss 0.2977, Avg Loss: 0.2567\n","Step 6841: Loss: 0.2781, requires_grad: True\n","Step 6841: Gradients computed successfully\n","Step 6842: Loss: 0.3648, requires_grad: True\n","Step 6842: Gradients computed successfully\n","Step 6843: Loss: 0.1937, requires_grad: True\n","Step 6843: Gradients computed successfully\n","Step 6844: Loss: 0.3080, requires_grad: True\n","Step 6844: Gradients computed successfully\n","Step 6845: Loss: 0.4516, requires_grad: True\n","Step 6845: Gradients computed successfully\n","Step 6846: Loss: 0.1588, requires_grad: True\n","Step 6846: Gradients computed successfully\n","Step 6847: Loss: 0.2430, requires_grad: True\n","Step 6847: Gradients computed successfully\n","Step 6848: Loss: 0.1102, requires_grad: True\n","Step 6848: Gradients computed successfully\n","Step 6849: Loss: 0.1291, requires_grad: True\n","Step 6849: Gradients computed successfully\n","Step 6850: Loss: 0.2109, requires_grad: True\n","Step 6850: Gradients computed successfully\n","Epoch 0, Step 6850, Loss 0.2109, Avg Loss: 0.2567\n","Step 6851: Loss: 0.1422, requires_grad: True\n","Step 6851: Gradients computed successfully\n","Step 6852: Loss: 0.3950, requires_grad: True\n","Step 6852: Gradients computed successfully\n","Step 6853: Loss: 0.3209, requires_grad: True\n","Step 6853: Gradients computed successfully\n","Step 6854: Loss: 0.1296, requires_grad: True\n","Step 6854: Gradients computed successfully\n","Step 6855: Loss: 0.2571, requires_grad: True\n","Step 6855: Gradients computed successfully\n","Step 6856: Loss: 0.1237, requires_grad: True\n","Step 6856: Gradients computed successfully\n","Step 6857: Loss: 0.2278, requires_grad: True\n","Step 6857: Gradients computed successfully\n","Step 6858: Loss: 0.1373, requires_grad: True\n","Step 6858: Gradients computed successfully\n","Step 6859: Loss: 0.1153, requires_grad: True\n","Step 6859: Gradients computed successfully\n","Step 6860: Loss: 0.3831, requires_grad: True\n","Step 6860: Gradients computed successfully\n","Epoch 0, Step 6860, Loss 0.3831, Avg Loss: 0.2566\n","Step 6861: Loss: 0.2840, requires_grad: True\n","Step 6861: Gradients computed successfully\n","Step 6862: Loss: 0.1376, requires_grad: True\n","Step 6862: Gradients computed successfully\n","Step 6863: Loss: 0.1767, requires_grad: True\n","Step 6863: Gradients computed successfully\n","Step 6864: Loss: 0.2911, requires_grad: True\n","Step 6864: Gradients computed successfully\n","Step 6865: Loss: 0.0681, requires_grad: True\n","Step 6865: Gradients computed successfully\n","Step 6866: Loss: 0.3674, requires_grad: True\n","Step 6866: Gradients computed successfully\n","Step 6867: Loss: 0.2334, requires_grad: True\n","Step 6867: Gradients computed successfully\n","Step 6868: Loss: 0.3661, requires_grad: True\n","Step 6868: Gradients computed successfully\n","Step 6869: Loss: 0.2460, requires_grad: True\n","Step 6869: Gradients computed successfully\n","Step 6870: Loss: 0.1597, requires_grad: True\n","Step 6870: Gradients computed successfully\n","Epoch 0, Step 6870, Loss 0.1597, Avg Loss: 0.2566\n","Step 6871: Loss: 0.1792, requires_grad: True\n","Step 6871: Gradients computed successfully\n","Step 6872: Loss: 0.0882, requires_grad: True\n","Step 6872: Gradients computed successfully\n","Step 6873: Loss: 0.2032, requires_grad: True\n","Step 6873: Gradients computed successfully\n","Step 6874: Loss: 0.3427, requires_grad: True\n","Step 6874: Gradients computed successfully\n","Step 6875: Loss: 0.2090, requires_grad: True\n","Step 6875: Gradients computed successfully\n","Step 6876: Loss: 0.1770, requires_grad: True\n","Step 6876: Gradients computed successfully\n","Step 6877: Loss: 0.2020, requires_grad: True\n","Step 6877: Gradients computed successfully\n","Step 6878: Loss: 0.3363, requires_grad: True\n","Step 6878: Gradients computed successfully\n","Step 6879: Loss: 0.2049, requires_grad: True\n","Step 6879: Gradients computed successfully\n","Step 6880: Loss: 0.2854, requires_grad: True\n","Step 6880: Gradients computed successfully\n","Epoch 0, Step 6880, Loss 0.2854, Avg Loss: 0.2565\n","Step 6881: Loss: 0.2416, requires_grad: True\n","Step 6881: Gradients computed successfully\n","Step 6882: Loss: 0.2342, requires_grad: True\n","Step 6882: Gradients computed successfully\n","Step 6883: Loss: 0.2168, requires_grad: True\n","Step 6883: Gradients computed successfully\n","Step 6884: Loss: 0.2745, requires_grad: True\n","Step 6884: Gradients computed successfully\n","Step 6885: Loss: 0.1592, requires_grad: True\n","Step 6885: Gradients computed successfully\n","Step 6886: Loss: 0.2053, requires_grad: True\n","Step 6886: Gradients computed successfully\n","Step 6887: Loss: 0.1253, requires_grad: True\n","Step 6887: Gradients computed successfully\n","Step 6888: Loss: 0.2135, requires_grad: True\n","Step 6888: Gradients computed successfully\n","Step 6889: Loss: 0.1385, requires_grad: True\n","Step 6889: Gradients computed successfully\n","Step 6890: Loss: 0.3049, requires_grad: True\n","Step 6890: Gradients computed successfully\n","Epoch 0, Step 6890, Loss 0.3049, Avg Loss: 0.2565\n","Step 6891: Loss: 0.1887, requires_grad: True\n","Step 6891: Gradients computed successfully\n","Step 6892: Loss: 0.5061, requires_grad: True\n","Step 6892: Gradients computed successfully\n","Step 6893: Loss: 0.1373, requires_grad: True\n","Step 6893: Gradients computed successfully\n","Step 6894: Loss: 0.1158, requires_grad: True\n","Step 6894: Gradients computed successfully\n","Step 6895: Loss: 0.1131, requires_grad: True\n","Step 6895: Gradients computed successfully\n","Step 6896: Loss: 0.1341, requires_grad: True\n","Step 6896: Gradients computed successfully\n","Step 6897: Loss: 0.0995, requires_grad: True\n","Step 6897: Gradients computed successfully\n","Step 6898: Loss: 0.1293, requires_grad: True\n","Step 6898: Gradients computed successfully\n","Step 6899: Loss: 0.1619, requires_grad: True\n","Step 6899: Gradients computed successfully\n","Step 6900: Loss: 0.1351, requires_grad: True\n","Step 6900: Gradients computed successfully\n","Epoch 0, Step 6900, Loss 0.1351, Avg Loss: 0.2563\n","Step 6901: Loss: 0.1934, requires_grad: True\n","Step 6901: Gradients computed successfully\n","Step 6902: Loss: 0.1797, requires_grad: True\n","Step 6902: Gradients computed successfully\n","Step 6903: Loss: 0.1880, requires_grad: True\n","Step 6903: Gradients computed successfully\n","Step 6904: Loss: 0.1336, requires_grad: True\n","Step 6904: Gradients computed successfully\n","Step 6905: Loss: 0.0965, requires_grad: True\n","Step 6905: Gradients computed successfully\n","Step 6906: Loss: 0.2269, requires_grad: True\n","Step 6906: Gradients computed successfully\n","Step 6907: Loss: 0.1224, requires_grad: True\n","Step 6907: Gradients computed successfully\n","Step 6908: Loss: 0.2073, requires_grad: True\n","Step 6908: Gradients computed successfully\n","Step 6909: Loss: 0.1721, requires_grad: True\n","Step 6909: Gradients computed successfully\n","Step 6910: Loss: 0.3880, requires_grad: True\n","Step 6910: Gradients computed successfully\n","Epoch 0, Step 6910, Loss 0.3880, Avg Loss: 0.2562\n","Step 6911: Loss: 0.1154, requires_grad: True\n","Step 6911: Gradients computed successfully\n","Step 6912: Loss: 0.1898, requires_grad: True\n","Step 6912: Gradients computed successfully\n","Step 6913: Loss: 0.1010, requires_grad: True\n","Step 6913: Gradients computed successfully\n","Step 6914: Loss: 0.0866, requires_grad: True\n","Step 6914: Gradients computed successfully\n","Step 6915: Loss: 0.1912, requires_grad: True\n","Step 6915: Gradients computed successfully\n","Step 6916: Loss: 0.1494, requires_grad: True\n","Step 6916: Gradients computed successfully\n","Step 6917: Loss: 0.1204, requires_grad: True\n","Step 6917: Gradients computed successfully\n","Step 6918: Loss: 0.4177, requires_grad: True\n","Step 6918: Gradients computed successfully\n","Step 6919: Loss: 0.4481, requires_grad: True\n","Step 6919: Gradients computed successfully\n","Step 6920: Loss: 0.3628, requires_grad: True\n","Step 6920: Gradients computed successfully\n","Epoch 0, Step 6920, Loss 0.3628, Avg Loss: 0.2562\n","Step 6921: Loss: 0.4765, requires_grad: True\n","Step 6921: Gradients computed successfully\n","Step 6922: Loss: 0.1948, requires_grad: True\n","Step 6922: Gradients computed successfully\n","Step 6923: Loss: 0.2644, requires_grad: True\n","Step 6923: Gradients computed successfully\n","Step 6924: Loss: 0.1169, requires_grad: True\n","Step 6924: Gradients computed successfully\n","Step 6925: Loss: 0.1932, requires_grad: True\n","Step 6925: Gradients computed successfully\n","Step 6926: Loss: 0.2559, requires_grad: True\n","Step 6926: Gradients computed successfully\n","Step 6927: Loss: 0.2446, requires_grad: True\n","Step 6927: Gradients computed successfully\n","Step 6928: Loss: 0.1170, requires_grad: True\n","Step 6928: Gradients computed successfully\n","Step 6929: Loss: 0.4609, requires_grad: True\n","Step 6929: Gradients computed successfully\n","Step 6930: Loss: 0.2361, requires_grad: True\n","Step 6930: Gradients computed successfully\n","Epoch 0, Step 6930, Loss 0.2361, Avg Loss: 0.2562\n","Step 6931: Loss: 0.3062, requires_grad: True\n","Step 6931: Gradients computed successfully\n","Step 6932: Loss: 0.2295, requires_grad: True\n","Step 6932: Gradients computed successfully\n","Step 6933: Loss: 0.3805, requires_grad: True\n","Step 6933: Gradients computed successfully\n","Step 6934: Loss: 0.1728, requires_grad: True\n","Step 6934: Gradients computed successfully\n","Step 6935: Loss: 0.2635, requires_grad: True\n","Step 6935: Gradients computed successfully\n","Step 6936: Loss: 0.3434, requires_grad: True\n","Step 6936: Gradients computed successfully\n","Step 6937: Loss: 0.1939, requires_grad: True\n","Step 6937: Gradients computed successfully\n","Step 6938: Loss: 0.1488, requires_grad: True\n","Step 6938: Gradients computed successfully\n","Step 6939: Loss: 0.0930, requires_grad: True\n","Step 6939: Gradients computed successfully\n","Step 6940: Loss: 0.2159, requires_grad: True\n","Step 6940: Gradients computed successfully\n","Epoch 0, Step 6940, Loss 0.2159, Avg Loss: 0.2562\n","Step 6941: Loss: 0.1912, requires_grad: True\n","Step 6941: Gradients computed successfully\n","Step 6942: Loss: 0.3700, requires_grad: True\n","Step 6942: Gradients computed successfully\n","Step 6943: Loss: 0.0832, requires_grad: True\n","Step 6943: Gradients computed successfully\n","Step 6944: Loss: 0.3350, requires_grad: True\n","Step 6944: Gradients computed successfully\n","Step 6945: Loss: 0.1505, requires_grad: True\n","Step 6945: Gradients computed successfully\n","Step 6946: Loss: 0.3260, requires_grad: True\n","Step 6946: Gradients computed successfully\n","Step 6947: Loss: 0.2012, requires_grad: True\n","Step 6947: Gradients computed successfully\n","Step 6948: Loss: 0.2000, requires_grad: True\n","Step 6948: Gradients computed successfully\n","Step 6949: Loss: 0.1361, requires_grad: True\n","Step 6949: Gradients computed successfully\n","Step 6950: Loss: 0.3805, requires_grad: True\n","Step 6950: Gradients computed successfully\n","Epoch 0, Step 6950, Loss 0.3805, Avg Loss: 0.2561\n","Step 6951: Loss: 0.1278, requires_grad: True\n","Step 6951: Gradients computed successfully\n","Step 6952: Loss: 0.2736, requires_grad: True\n","Step 6952: Gradients computed successfully\n","Step 6953: Loss: 0.1572, requires_grad: True\n","Step 6953: Gradients computed successfully\n","Step 6954: Loss: 0.3062, requires_grad: True\n","Step 6954: Gradients computed successfully\n","Step 6955: Loss: 0.1410, requires_grad: True\n","Step 6955: Gradients computed successfully\n","Step 6956: Loss: 0.2421, requires_grad: True\n","Step 6956: Gradients computed successfully\n","Step 6957: Loss: 0.2212, requires_grad: True\n","Step 6957: Gradients computed successfully\n","Step 6958: Loss: 0.2210, requires_grad: True\n","Step 6958: Gradients computed successfully\n","Step 6959: Loss: 0.3493, requires_grad: True\n","Step 6959: Gradients computed successfully\n","Step 6960: Loss: 0.1313, requires_grad: True\n","Step 6960: Gradients computed successfully\n","Epoch 0, Step 6960, Loss 0.1313, Avg Loss: 0.2561\n","Step 6961: Loss: 0.2866, requires_grad: True\n","Step 6961: Gradients computed successfully\n","Step 6962: Loss: 0.1740, requires_grad: True\n","Step 6962: Gradients computed successfully\n","Step 6963: Loss: 0.1174, requires_grad: True\n","Step 6963: Gradients computed successfully\n","Step 6964: Loss: 0.5557, requires_grad: True\n","Step 6964: Gradients computed successfully\n","Step 6965: Loss: 0.2132, requires_grad: True\n","Step 6965: Gradients computed successfully\n","Step 6966: Loss: 0.2994, requires_grad: True\n","Step 6966: Gradients computed successfully\n","Step 6967: Loss: 0.1393, requires_grad: True\n","Step 6967: Gradients computed successfully\n","Step 6968: Loss: 0.2145, requires_grad: True\n","Step 6968: Gradients computed successfully\n","Step 6969: Loss: 0.2047, requires_grad: True\n","Step 6969: Gradients computed successfully\n","Step 6970: Loss: 0.8032, requires_grad: True\n","Step 6970: Gradients computed successfully\n","Epoch 0, Step 6970, Loss 0.8032, Avg Loss: 0.2561\n","Step 6971: Loss: 0.1171, requires_grad: True\n","Step 6971: Gradients computed successfully\n","Step 6972: Loss: 0.4691, requires_grad: True\n","Step 6972: Gradients computed successfully\n","Step 6973: Loss: 0.0882, requires_grad: True\n","Step 6973: Gradients computed successfully\n","Step 6974: Loss: 0.2123, requires_grad: True\n","Step 6974: Gradients computed successfully\n","Step 6975: Loss: 0.2225, requires_grad: True\n","Step 6975: Gradients computed successfully\n","Step 6976: Loss: 0.1282, requires_grad: True\n","Step 6976: Gradients computed successfully\n","Step 6977: Loss: 0.2493, requires_grad: True\n","Step 6977: Gradients computed successfully\n","Step 6978: Loss: 0.2291, requires_grad: True\n","Step 6978: Gradients computed successfully\n","Step 6979: Loss: 0.2920, requires_grad: True\n","Step 6979: Gradients computed successfully\n","Step 6980: Loss: 0.1698, requires_grad: True\n","Step 6980: Gradients computed successfully\n","Epoch 0, Step 6980, Loss 0.1698, Avg Loss: 0.2561\n","Step 6981: Loss: 0.3760, requires_grad: True\n","Step 6981: Gradients computed successfully\n","Step 6982: Loss: 0.1461, requires_grad: True\n","Step 6982: Gradients computed successfully\n","Step 6983: Loss: 0.2871, requires_grad: True\n","Step 6983: Gradients computed successfully\n","Step 6984: Loss: 0.3027, requires_grad: True\n","Step 6984: Gradients computed successfully\n","Step 6985: Loss: 0.2063, requires_grad: True\n","Step 6985: Gradients computed successfully\n","Step 6986: Loss: 0.1527, requires_grad: True\n","Step 6986: Gradients computed successfully\n","Step 6987: Loss: 0.1525, requires_grad: True\n","Step 6987: Gradients computed successfully\n","Step 6988: Loss: 0.1082, requires_grad: True\n","Step 6988: Gradients computed successfully\n","Step 6989: Loss: 0.2227, requires_grad: True\n","Step 6989: Gradients computed successfully\n","Step 6990: Loss: 0.2369, requires_grad: True\n","Step 6990: Gradients computed successfully\n","Epoch 0, Step 6990, Loss 0.2369, Avg Loss: 0.2560\n","Step 6991: Loss: 0.2809, requires_grad: True\n","Step 6991: Gradients computed successfully\n","Step 6992: Loss: 0.2366, requires_grad: True\n","Step 6992: Gradients computed successfully\n","Step 6993: Loss: 0.2232, requires_grad: True\n","Step 6993: Gradients computed successfully\n","Step 6994: Loss: 0.1798, requires_grad: True\n","Step 6994: Gradients computed successfully\n","Step 6995: Loss: 0.3717, requires_grad: True\n","Step 6995: Gradients computed successfully\n","Step 6996: Loss: 0.0895, requires_grad: True\n","Step 6996: Gradients computed successfully\n","Step 6997: Loss: 0.1245, requires_grad: True\n","Step 6997: Gradients computed successfully\n","Step 6998: Loss: 0.0594, requires_grad: True\n","Step 6998: Gradients computed successfully\n","Step 6999: Loss: 0.4945, requires_grad: True\n","Step 6999: Gradients computed successfully\n","Step 7000: Loss: 0.1462, requires_grad: True\n","Step 7000: Gradients computed successfully\n","Epoch 0, Step 7000, Loss 0.1462, Avg Loss: 0.2560\n","Saved checkpoint to /content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_complex_sql_continued/manual-epoch-0-step-7000\n","Step 7001: Loss: 0.2308, requires_grad: True\n","Step 7001: Gradients computed successfully\n","Step 7002: Loss: 0.0804, requires_grad: True\n","Step 7002: Gradients computed successfully\n","Step 7003: Loss: 0.2276, requires_grad: True\n","Step 7003: Gradients computed successfully\n","Step 7004: Loss: 0.1717, requires_grad: True\n","Step 7004: Gradients computed successfully\n","Step 7005: Loss: 0.2724, requires_grad: True\n","Step 7005: Gradients computed successfully\n","Step 7006: Loss: 0.2050, requires_grad: True\n","Step 7006: Gradients computed successfully\n","Step 7007: Loss: 0.1703, requires_grad: True\n","Step 7007: Gradients computed successfully\n","Step 7008: Loss: 0.1802, requires_grad: True\n","Step 7008: Gradients computed successfully\n","Step 7009: Loss: 0.2421, requires_grad: True\n","Step 7009: Gradients computed successfully\n","Step 7010: Loss: 0.1806, requires_grad: True\n","Step 7010: Gradients computed successfully\n","Epoch 0, Step 7010, Loss 0.1806, Avg Loss: 0.2559\n","Step 7011: Loss: 0.1631, requires_grad: True\n","Step 7011: Gradients computed successfully\n","Step 7012: Loss: 0.1763, requires_grad: True\n","Step 7012: Gradients computed successfully\n","Step 7013: Loss: 0.3340, requires_grad: True\n","Step 7013: Gradients computed successfully\n","Step 7014: Loss: 0.1224, requires_grad: True\n","Step 7014: Gradients computed successfully\n","Step 7015: Loss: 0.1543, requires_grad: True\n","Step 7015: Gradients computed successfully\n","Step 7016: Loss: 0.3365, requires_grad: True\n","Step 7016: Gradients computed successfully\n","Step 7017: Loss: 0.2463, requires_grad: True\n","Step 7017: Gradients computed successfully\n","Step 7018: Loss: 0.3567, requires_grad: True\n","Step 7018: Gradients computed successfully\n","Step 7019: Loss: 0.2237, requires_grad: True\n","Step 7019: Gradients computed successfully\n","Step 7020: Loss: 0.1542, requires_grad: True\n","Step 7020: Gradients computed successfully\n","Epoch 0, Step 7020, Loss 0.1542, Avg Loss: 0.2559\n","Step 7021: Loss: 0.3730, requires_grad: True\n","Step 7021: Gradients computed successfully\n","Step 7022: Loss: 0.1365, requires_grad: True\n","Step 7022: Gradients computed successfully\n","Step 7023: Loss: 0.1862, requires_grad: True\n","Step 7023: Gradients computed successfully\n","Step 7024: Loss: 0.2093, requires_grad: True\n","Step 7024: Gradients computed successfully\n","Step 7025: Loss: 0.7879, requires_grad: True\n","Step 7025: Gradients computed successfully\n","Step 7026: Loss: 0.3779, requires_grad: True\n","Step 7026: Gradients computed successfully\n","Step 7027: Loss: 0.4795, requires_grad: True\n","Step 7027: Gradients computed successfully\n","Step 7028: Loss: 0.1373, requires_grad: True\n","Step 7028: Gradients computed successfully\n","Step 7029: Loss: 0.2948, requires_grad: True\n","Step 7029: Gradients computed successfully\n","Step 7030: Loss: 0.2192, requires_grad: True\n","Step 7030: Gradients computed successfully\n","Epoch 0, Step 7030, Loss 0.2192, Avg Loss: 0.2559\n","Step 7031: Loss: 0.1383, requires_grad: True\n","Step 7031: Gradients computed successfully\n","Step 7032: Loss: 0.4168, requires_grad: True\n","Step 7032: Gradients computed successfully\n","Step 7033: Loss: 0.3904, requires_grad: True\n","Step 7033: Gradients computed successfully\n","Step 7034: Loss: 0.3421, requires_grad: True\n","Step 7034: Gradients computed successfully\n","Step 7035: Loss: 0.2530, requires_grad: True\n","Step 7035: Gradients computed successfully\n","Step 7036: Loss: 0.2034, requires_grad: True\n","Step 7036: Gradients computed successfully\n","Step 7037: Loss: 0.5972, requires_grad: True\n","Step 7037: Gradients computed successfully\n","Step 7038: Loss: 0.3940, requires_grad: True\n","Step 7038: Gradients computed successfully\n","Step 7039: Loss: 0.0845, requires_grad: True\n","Step 7039: Gradients computed successfully\n","Step 7040: Loss: 0.1999, requires_grad: True\n","Step 7040: Gradients computed successfully\n","Epoch 0, Step 7040, Loss 0.1999, Avg Loss: 0.2560\n","Step 7041: Loss: 0.2396, requires_grad: True\n","Step 7041: Gradients computed successfully\n","Step 7042: Loss: 0.2341, requires_grad: True\n","Step 7042: Gradients computed successfully\n","Step 7043: Loss: 0.1438, requires_grad: True\n","Step 7043: Gradients computed successfully\n","Step 7044: Loss: 0.4373, requires_grad: True\n","Step 7044: Gradients computed successfully\n","Step 7045: Loss: 0.2265, requires_grad: True\n","Step 7045: Gradients computed successfully\n","Step 7046: Loss: 0.4598, requires_grad: True\n","Step 7046: Gradients computed successfully\n","Step 7047: Loss: 0.1207, requires_grad: True\n","Step 7047: Gradients computed successfully\n","Step 7048: Loss: 0.1202, requires_grad: True\n","Step 7048: Gradients computed successfully\n","Step 7049: Loss: 0.4609, requires_grad: True\n","Step 7049: Gradients computed successfully\n","Step 7050: Loss: 0.2106, requires_grad: True\n","Step 7050: Gradients computed successfully\n","Epoch 0, Step 7050, Loss 0.2106, Avg Loss: 0.2560\n","Step 7051: Loss: 0.3301, requires_grad: True\n","Step 7051: Gradients computed successfully\n","Step 7052: Loss: 0.1141, requires_grad: True\n","Step 7052: Gradients computed successfully\n","Step 7053: Loss: 0.1269, requires_grad: True\n","Step 7053: Gradients computed successfully\n","Step 7054: Loss: 0.1575, requires_grad: True\n","Step 7054: Gradients computed successfully\n","Step 7055: Loss: 0.3343, requires_grad: True\n","Step 7055: Gradients computed successfully\n","Step 7056: Loss: 0.1489, requires_grad: True\n","Step 7056: Gradients computed successfully\n","Step 7057: Loss: 0.1712, requires_grad: True\n","Step 7057: Gradients computed successfully\n","Step 7058: Loss: 0.1586, requires_grad: True\n","Step 7058: Gradients computed successfully\n","Step 7059: Loss: 0.4860, requires_grad: True\n","Step 7059: Gradients computed successfully\n","Step 7060: Loss: 0.3302, requires_grad: True\n","Step 7060: Gradients computed successfully\n","Epoch 0, Step 7060, Loss 0.3302, Avg Loss: 0.2560\n","Step 7061: Loss: 0.1695, requires_grad: True\n","Step 7061: Gradients computed successfully\n","Step 7062: Loss: 0.2159, requires_grad: True\n","Step 7062: Gradients computed successfully\n","Step 7063: Loss: 0.2731, requires_grad: True\n","Step 7063: Gradients computed successfully\n","Step 7064: Loss: 0.1353, requires_grad: True\n","Step 7064: Gradients computed successfully\n","Step 7065: Loss: 0.2381, requires_grad: True\n","Step 7065: Gradients computed successfully\n","Step 7066: Loss: 0.1453, requires_grad: True\n","Step 7066: Gradients computed successfully\n","Step 7067: Loss: 0.1143, requires_grad: True\n","Step 7067: Gradients computed successfully\n","Step 7068: Loss: 0.3896, requires_grad: True\n","Step 7068: Gradients computed successfully\n","Step 7069: Loss: 0.3346, requires_grad: True\n","Step 7069: Gradients computed successfully\n","Step 7070: Loss: 0.2638, requires_grad: True\n","Step 7070: Gradients computed successfully\n","Epoch 0, Step 7070, Loss 0.2638, Avg Loss: 0.2560\n","Step 7071: Loss: 0.1045, requires_grad: True\n","Step 7071: Gradients computed successfully\n","Step 7072: Loss: 0.3253, requires_grad: True\n","Step 7072: Gradients computed successfully\n","Step 7073: Loss: 0.1013, requires_grad: True\n","Step 7073: Gradients computed successfully\n","Step 7074: Loss: 0.2461, requires_grad: True\n","Step 7074: Gradients computed successfully\n","Step 7075: Loss: 0.2110, requires_grad: True\n","Step 7075: Gradients computed successfully\n","Step 7076: Loss: 0.1675, requires_grad: True\n","Step 7076: Gradients computed successfully\n","Step 7077: Loss: 0.2523, requires_grad: True\n","Step 7077: Gradients computed successfully\n","Step 7078: Loss: 0.1985, requires_grad: True\n","Step 7078: Gradients computed successfully\n","Step 7079: Loss: 0.0580, requires_grad: True\n","Step 7079: Gradients computed successfully\n","Step 7080: Loss: 0.2413, requires_grad: True\n","Step 7080: Gradients computed successfully\n","Epoch 0, Step 7080, Loss 0.2413, Avg Loss: 0.2559\n","Step 7081: Loss: 0.4492, requires_grad: True\n","Step 7081: Gradients computed successfully\n","Step 7082: Loss: 0.4058, requires_grad: True\n","Step 7082: Gradients computed successfully\n","Step 7083: Loss: 0.2655, requires_grad: True\n","Step 7083: Gradients computed successfully\n","Step 7084: Loss: 0.2215, requires_grad: True\n","Step 7084: Gradients computed successfully\n","Step 7085: Loss: 0.1885, requires_grad: True\n","Step 7085: Gradients computed successfully\n","Step 7086: Loss: 0.1507, requires_grad: True\n","Step 7086: Gradients computed successfully\n","Step 7087: Loss: 0.1214, requires_grad: True\n","Step 7087: Gradients computed successfully\n","Step 7088: Loss: 0.1778, requires_grad: True\n","Step 7088: Gradients computed successfully\n","Step 7089: Loss: 0.2991, requires_grad: True\n","Step 7089: Gradients computed successfully\n","Step 7090: Loss: 0.0876, requires_grad: True\n","Step 7090: Gradients computed successfully\n","Epoch 0, Step 7090, Loss 0.0876, Avg Loss: 0.2558\n","Step 7091: Loss: 0.3293, requires_grad: True\n","Step 7091: Gradients computed successfully\n","Step 7092: Loss: 0.6548, requires_grad: True\n","Step 7092: Gradients computed successfully\n","Step 7093: Loss: 0.1990, requires_grad: True\n","Step 7093: Gradients computed successfully\n","Step 7094: Loss: 0.2659, requires_grad: True\n","Step 7094: Gradients computed successfully\n","Step 7095: Loss: 0.1651, requires_grad: True\n","Step 7095: Gradients computed successfully\n","Step 7096: Loss: 0.3145, requires_grad: True\n","Step 7096: Gradients computed successfully\n","Step 7097: Loss: 0.2668, requires_grad: True\n","Step 7097: Gradients computed successfully\n","Step 7098: Loss: 0.2073, requires_grad: True\n","Step 7098: Gradients computed successfully\n","Step 7099: Loss: 0.2235, requires_grad: True\n","Step 7099: Gradients computed successfully\n","Step 7100: Loss: 0.2730, requires_grad: True\n","Step 7100: Gradients computed successfully\n","Epoch 0, Step 7100, Loss 0.2730, Avg Loss: 0.2559\n","Step 7101: Loss: 0.3083, requires_grad: True\n","Step 7101: Gradients computed successfully\n","Step 7102: Loss: 0.2802, requires_grad: True\n","Step 7102: Gradients computed successfully\n","Step 7103: Loss: 0.2321, requires_grad: True\n","Step 7103: Gradients computed successfully\n","Step 7104: Loss: 0.2423, requires_grad: True\n","Step 7104: Gradients computed successfully\n","Step 7105: Loss: 0.1553, requires_grad: True\n","Step 7105: Gradients computed successfully\n","Step 7106: Loss: 0.6869, requires_grad: True\n","Step 7106: Gradients computed successfully\n","Step 7107: Loss: 0.4831, requires_grad: True\n","Step 7107: Gradients computed successfully\n","Step 7108: Loss: 0.1362, requires_grad: True\n","Step 7108: Gradients computed successfully\n","Step 7109: Loss: 0.2071, requires_grad: True\n","Step 7109: Gradients computed successfully\n","Step 7110: Loss: 0.1494, requires_grad: True\n","Step 7110: Gradients computed successfully\n","Epoch 0, Step 7110, Loss 0.1494, Avg Loss: 0.2559\n","Step 7111: Loss: 0.3694, requires_grad: True\n","Step 7111: Gradients computed successfully\n","Step 7112: Loss: 0.3688, requires_grad: True\n","Step 7112: Gradients computed successfully\n","Step 7113: Loss: 0.1662, requires_grad: True\n","Step 7113: Gradients computed successfully\n","Step 7114: Loss: 0.3665, requires_grad: True\n","Step 7114: Gradients computed successfully\n","Step 7115: Loss: 0.1575, requires_grad: True\n","Step 7115: Gradients computed successfully\n","Step 7116: Loss: 0.1614, requires_grad: True\n","Step 7116: Gradients computed successfully\n","Step 7117: Loss: 0.2283, requires_grad: True\n","Step 7117: Gradients computed successfully\n","Step 7118: Loss: 0.1571, requires_grad: True\n","Step 7118: Gradients computed successfully\n","Step 7119: Loss: 0.2782, requires_grad: True\n","Step 7119: Gradients computed successfully\n","Step 7120: Loss: 0.0831, requires_grad: True\n","Step 7120: Gradients computed successfully\n","Epoch 0, Step 7120, Loss 0.0831, Avg Loss: 0.2559\n","Step 7121: Loss: 0.2305, requires_grad: True\n","Step 7121: Gradients computed successfully\n","Step 7122: Loss: 0.4517, requires_grad: True\n","Step 7122: Gradients computed successfully\n","Step 7123: Loss: 0.3246, requires_grad: True\n","Step 7123: Gradients computed successfully\n","Step 7124: Loss: 0.1557, requires_grad: True\n","Step 7124: Gradients computed successfully\n","Step 7125: Loss: 0.1860, requires_grad: True\n","Step 7125: Gradients computed successfully\n","Step 7126: Loss: 0.3450, requires_grad: True\n","Step 7126: Gradients computed successfully\n","Step 7127: Loss: 0.2498, requires_grad: True\n","Step 7127: Gradients computed successfully\n","Step 7128: Loss: 0.2577, requires_grad: True\n","Step 7128: Gradients computed successfully\n","Step 7129: Loss: 0.3173, requires_grad: True\n","Step 7129: Gradients computed successfully\n","Step 7130: Loss: 0.5640, requires_grad: True\n","Step 7130: Gradients computed successfully\n","Epoch 0, Step 7130, Loss 0.5640, Avg Loss: 0.2560\n","Step 7131: Loss: 0.2938, requires_grad: True\n","Step 7131: Gradients computed successfully\n","Step 7132: Loss: 0.2160, requires_grad: True\n","Step 7132: Gradients computed successfully\n","Step 7133: Loss: 0.1891, requires_grad: True\n","Step 7133: Gradients computed successfully\n","Step 7134: Loss: 0.2100, requires_grad: True\n","Step 7134: Gradients computed successfully\n","Step 7135: Loss: 0.2767, requires_grad: True\n","Step 7135: Gradients computed successfully\n","Step 7136: Loss: 0.0765, requires_grad: True\n","Step 7136: Gradients computed successfully\n","Step 7137: Loss: 0.3196, requires_grad: True\n","Step 7137: Gradients computed successfully\n","Step 7138: Loss: 0.3093, requires_grad: True\n","Step 7138: Gradients computed successfully\n","Step 7139: Loss: 0.2895, requires_grad: True\n","Step 7139: Gradients computed successfully\n","Step 7140: Loss: 0.2049, requires_grad: True\n","Step 7140: Gradients computed successfully\n","Epoch 0, Step 7140, Loss 0.2049, Avg Loss: 0.2559\n","Step 7141: Loss: 0.4297, requires_grad: True\n","Step 7141: Gradients computed successfully\n","Step 7142: Loss: 0.0980, requires_grad: True\n","Step 7142: Gradients computed successfully\n","Step 7143: Loss: 0.1426, requires_grad: True\n","Step 7143: Gradients computed successfully\n","Step 7144: Loss: 0.4107, requires_grad: True\n","Step 7144: Gradients computed successfully\n","Step 7145: Loss: 0.1036, requires_grad: True\n","Step 7145: Gradients computed successfully\n","Step 7146: Loss: 0.1125, requires_grad: True\n","Step 7146: Gradients computed successfully\n","Step 7147: Loss: 0.1034, requires_grad: True\n","Step 7147: Gradients computed successfully\n","Step 7148: Loss: 0.1585, requires_grad: True\n","Step 7148: Gradients computed successfully\n","Step 7149: Loss: 0.5273, requires_grad: True\n","Step 7149: Gradients computed successfully\n","Step 7150: Loss: 0.1220, requires_grad: True\n","Step 7150: Gradients computed successfully\n","Epoch 0, Step 7150, Loss 0.1220, Avg Loss: 0.2559\n","Step 7151: Loss: 0.2158, requires_grad: True\n","Step 7151: Gradients computed successfully\n","Step 7152: Loss: 0.3480, requires_grad: True\n","Step 7152: Gradients computed successfully\n","Step 7153: Loss: 0.2152, requires_grad: True\n","Step 7153: Gradients computed successfully\n","Step 7154: Loss: 0.0638, requires_grad: True\n","Step 7154: Gradients computed successfully\n","Step 7155: Loss: 0.1626, requires_grad: True\n","Step 7155: Gradients computed successfully\n","Step 7156: Loss: 0.2648, requires_grad: True\n","Step 7156: Gradients computed successfully\n","Step 7157: Loss: 0.1517, requires_grad: True\n","Step 7157: Gradients computed successfully\n","Step 7158: Loss: 0.1725, requires_grad: True\n","Step 7158: Gradients computed successfully\n","Step 7159: Loss: 0.3085, requires_grad: True\n","Step 7159: Gradients computed successfully\n","Step 7160: Loss: 0.1662, requires_grad: True\n","Step 7160: Gradients computed successfully\n","Epoch 0, Step 7160, Loss 0.1662, Avg Loss: 0.2558\n","Step 7161: Loss: 0.0951, requires_grad: True\n","Step 7161: Gradients computed successfully\n","Step 7162: Loss: 0.3436, requires_grad: True\n","Step 7162: Gradients computed successfully\n","Step 7163: Loss: 0.1676, requires_grad: True\n","Step 7163: Gradients computed successfully\n","Step 7164: Loss: 0.2373, requires_grad: True\n","Step 7164: Gradients computed successfully\n","Step 7165: Loss: 0.0941, requires_grad: True\n","Step 7165: Gradients computed successfully\n","Step 7166: Loss: 0.2447, requires_grad: True\n","Step 7166: Gradients computed successfully\n","Step 7167: Loss: 0.2007, requires_grad: True\n","Step 7167: Gradients computed successfully\n","Step 7168: Loss: 0.4152, requires_grad: True\n","Step 7168: Gradients computed successfully\n","Step 7169: Loss: 0.3409, requires_grad: True\n","Step 7169: Gradients computed successfully\n","Step 7170: Loss: 0.1360, requires_grad: True\n","Step 7170: Gradients computed successfully\n","Epoch 0, Step 7170, Loss 0.1360, Avg Loss: 0.2558\n","Step 7171: Loss: 0.3022, requires_grad: True\n","Step 7171: Gradients computed successfully\n","Step 7172: Loss: 0.2591, requires_grad: True\n","Step 7172: Gradients computed successfully\n","Step 7173: Loss: 0.1260, requires_grad: True\n","Step 7173: Gradients computed successfully\n","Step 7174: Loss: 0.1066, requires_grad: True\n","Step 7174: Gradients computed successfully\n","Step 7175: Loss: 0.1925, requires_grad: True\n","Step 7175: Gradients computed successfully\n","Step 7176: Loss: 0.1542, requires_grad: True\n","Step 7176: Gradients computed successfully\n","Step 7177: Loss: 0.2116, requires_grad: True\n","Step 7177: Gradients computed successfully\n","Step 7178: Loss: 0.1960, requires_grad: True\n","Step 7178: Gradients computed successfully\n","Step 7179: Loss: 0.2624, requires_grad: True\n","Step 7179: Gradients computed successfully\n","Step 7180: Loss: 0.5902, requires_grad: True\n","Step 7180: Gradients computed successfully\n","Epoch 0, Step 7180, Loss 0.5902, Avg Loss: 0.2558\n","Step 7181: Loss: 0.1694, requires_grad: True\n","Step 7181: Gradients computed successfully\n","Step 7182: Loss: 0.4331, requires_grad: True\n","Step 7182: Gradients computed successfully\n","Step 7183: Loss: 0.1902, requires_grad: True\n","Step 7183: Gradients computed successfully\n","Step 7184: Loss: 0.3776, requires_grad: True\n","Step 7184: Gradients computed successfully\n","Step 7185: Loss: 0.2569, requires_grad: True\n","Step 7185: Gradients computed successfully\n","Step 7186: Loss: 0.1584, requires_grad: True\n","Step 7186: Gradients computed successfully\n","Step 7187: Loss: 0.2706, requires_grad: True\n","Step 7187: Gradients computed successfully\n","Step 7188: Loss: 0.1334, requires_grad: True\n","Step 7188: Gradients computed successfully\n","Step 7189: Loss: 0.1964, requires_grad: True\n","Step 7189: Gradients computed successfully\n","Step 7190: Loss: 0.2306, requires_grad: True\n","Step 7190: Gradients computed successfully\n","Epoch 0, Step 7190, Loss 0.2306, Avg Loss: 0.2557\n","Step 7191: Loss: 0.2962, requires_grad: True\n","Step 7191: Gradients computed successfully\n","Step 7192: Loss: 0.3455, requires_grad: True\n","Step 7192: Gradients computed successfully\n","Step 7193: Loss: 0.2663, requires_grad: True\n","Step 7193: Gradients computed successfully\n","Step 7194: Loss: 0.1745, requires_grad: True\n","Step 7194: Gradients computed successfully\n","Step 7195: Loss: 0.1054, requires_grad: True\n","Step 7195: Gradients computed successfully\n","Step 7196: Loss: 0.2424, requires_grad: True\n","Step 7196: Gradients computed successfully\n","Step 7197: Loss: 0.2669, requires_grad: True\n","Step 7197: Gradients computed successfully\n","Step 7198: Loss: 0.1763, requires_grad: True\n","Step 7198: Gradients computed successfully\n","Step 7199: Loss: 0.4145, requires_grad: True\n","Step 7199: Gradients computed successfully\n","Step 7200: Loss: 0.4247, requires_grad: True\n","Step 7200: Gradients computed successfully\n","Epoch 0, Step 7200, Loss 0.4247, Avg Loss: 0.2558\n","Step 7201: Loss: 0.2189, requires_grad: True\n","Step 7201: Gradients computed successfully\n","Step 7202: Loss: 0.3482, requires_grad: True\n","Step 7202: Gradients computed successfully\n","Step 7203: Loss: 0.1574, requires_grad: True\n","Step 7203: Gradients computed successfully\n","Step 7204: Loss: 0.1750, requires_grad: True\n","Step 7204: Gradients computed successfully\n","Step 7205: Loss: 0.6780, requires_grad: True\n","Step 7205: Gradients computed successfully\n","Step 7206: Loss: 0.3251, requires_grad: True\n","Step 7206: Gradients computed successfully\n","Step 7207: Loss: 0.0887, requires_grad: True\n","Step 7207: Gradients computed successfully\n","Step 7208: Loss: 0.5423, requires_grad: True\n","Step 7208: Gradients computed successfully\n","Step 7209: Loss: 0.2752, requires_grad: True\n","Step 7209: Gradients computed successfully\n","Step 7210: Loss: 0.1658, requires_grad: True\n","Step 7210: Gradients computed successfully\n","Epoch 0, Step 7210, Loss 0.1658, Avg Loss: 0.2558\n","Step 7211: Loss: 0.1449, requires_grad: True\n","Step 7211: Gradients computed successfully\n","Step 7212: Loss: 0.2591, requires_grad: True\n","Step 7212: Gradients computed successfully\n","Step 7213: Loss: 0.2622, requires_grad: True\n","Step 7213: Gradients computed successfully\n","Step 7214: Loss: 0.2850, requires_grad: True\n","Step 7214: Gradients computed successfully\n","Step 7215: Loss: 0.1025, requires_grad: True\n","Step 7215: Gradients computed successfully\n","Step 7216: Loss: 0.2293, requires_grad: True\n","Step 7216: Gradients computed successfully\n","Step 7217: Loss: 0.1779, requires_grad: True\n","Step 7217: Gradients computed successfully\n","Step 7218: Loss: 0.1153, requires_grad: True\n","Step 7218: Gradients computed successfully\n","Step 7219: Loss: 0.3082, requires_grad: True\n","Step 7219: Gradients computed successfully\n","Step 7220: Loss: 0.1442, requires_grad: True\n","Step 7220: Gradients computed successfully\n","Epoch 0, Step 7220, Loss 0.1442, Avg Loss: 0.2558\n","Step 7221: Loss: 0.2449, requires_grad: True\n","Step 7221: Gradients computed successfully\n","Step 7222: Loss: 0.2311, requires_grad: True\n","Step 7222: Gradients computed successfully\n","Step 7223: Loss: 0.3482, requires_grad: True\n","Step 7223: Gradients computed successfully\n","Step 7224: Loss: 0.2302, requires_grad: True\n","Step 7224: Gradients computed successfully\n","Step 7225: Loss: 0.0800, requires_grad: True\n","Step 7225: Gradients computed successfully\n","Step 7226: Loss: 0.2567, requires_grad: True\n","Step 7226: Gradients computed successfully\n","Step 7227: Loss: 0.2146, requires_grad: True\n","Step 7227: Gradients computed successfully\n","Step 7228: Loss: 0.0554, requires_grad: True\n","Step 7228: Gradients computed successfully\n","Step 7229: Loss: 0.1621, requires_grad: True\n","Step 7229: Gradients computed successfully\n","Step 7230: Loss: 0.1433, requires_grad: True\n","Step 7230: Gradients computed successfully\n","Epoch 0, Step 7230, Loss 0.1433, Avg Loss: 0.2557\n","Step 7231: Loss: 0.5570, requires_grad: True\n","Step 7231: Gradients computed successfully\n","Step 7232: Loss: 0.2712, requires_grad: True\n","Step 7232: Gradients computed successfully\n","Step 7233: Loss: 0.2137, requires_grad: True\n","Step 7233: Gradients computed successfully\n","Step 7234: Loss: 0.0673, requires_grad: True\n","Step 7234: Gradients computed successfully\n","Step 7235: Loss: 0.2999, requires_grad: True\n","Step 7235: Gradients computed successfully\n","Step 7236: Loss: 0.2811, requires_grad: True\n","Step 7236: Gradients computed successfully\n","Step 7237: Loss: 0.3492, requires_grad: True\n","Step 7237: Gradients computed successfully\n","Step 7238: Loss: 0.0888, requires_grad: True\n","Step 7238: Gradients computed successfully\n","Step 7239: Loss: 0.2118, requires_grad: True\n","Step 7239: Gradients computed successfully\n","Step 7240: Loss: 0.1075, requires_grad: True\n","Step 7240: Gradients computed successfully\n","Epoch 0, Step 7240, Loss 0.1075, Avg Loss: 0.2557\n","Step 7241: Loss: 0.1335, requires_grad: True\n","Step 7241: Gradients computed successfully\n","Step 7242: Loss: 0.1467, requires_grad: True\n","Step 7242: Gradients computed successfully\n","Step 7243: Loss: 0.1873, requires_grad: True\n","Step 7243: Gradients computed successfully\n","Step 7244: Loss: 0.4547, requires_grad: True\n","Step 7244: Gradients computed successfully\n","Step 7245: Loss: 0.1130, requires_grad: True\n","Step 7245: Gradients computed successfully\n","Step 7246: Loss: 0.1671, requires_grad: True\n","Step 7246: Gradients computed successfully\n","Step 7247: Loss: 0.1362, requires_grad: True\n","Step 7247: Gradients computed successfully\n","Step 7248: Loss: 0.1276, requires_grad: True\n","Step 7248: Gradients computed successfully\n","Step 7249: Loss: 0.2645, requires_grad: True\n","Step 7249: Gradients computed successfully\n","Step 7250: Loss: 0.1989, requires_grad: True\n","Step 7250: Gradients computed successfully\n","Epoch 0, Step 7250, Loss 0.1989, Avg Loss: 0.2556\n","Step 7251: Loss: 0.3075, requires_grad: True\n","Step 7251: Gradients computed successfully\n","Step 7252: Loss: 0.1811, requires_grad: True\n","Step 7252: Gradients computed successfully\n","Step 7253: Loss: 0.1310, requires_grad: True\n","Step 7253: Gradients computed successfully\n","Step 7254: Loss: 0.2120, requires_grad: True\n","Step 7254: Gradients computed successfully\n","Step 7255: Loss: 0.1353, requires_grad: True\n","Step 7255: Gradients computed successfully\n","Step 7256: Loss: 0.1084, requires_grad: True\n","Step 7256: Gradients computed successfully\n","Step 7257: Loss: 0.2323, requires_grad: True\n","Step 7257: Gradients computed successfully\n","Step 7258: Loss: 0.4520, requires_grad: True\n","Step 7258: Gradients computed successfully\n","Step 7259: Loss: 0.3609, requires_grad: True\n","Step 7259: Gradients computed successfully\n","Step 7260: Loss: 0.0856, requires_grad: True\n","Step 7260: Gradients computed successfully\n","Epoch 0, Step 7260, Loss 0.0856, Avg Loss: 0.2555\n","Step 7261: Loss: 0.2837, requires_grad: True\n","Step 7261: Gradients computed successfully\n","Step 7262: Loss: 0.1052, requires_grad: True\n","Step 7262: Gradients computed successfully\n","Step 7263: Loss: 0.3145, requires_grad: True\n","Step 7263: Gradients computed successfully\n","Step 7264: Loss: 0.2445, requires_grad: True\n","Step 7264: Gradients computed successfully\n","Step 7265: Loss: 0.2819, requires_grad: True\n","Step 7265: Gradients computed successfully\n","Step 7266: Loss: 0.2743, requires_grad: True\n","Step 7266: Gradients computed successfully\n","Step 7267: Loss: 0.4309, requires_grad: True\n","Step 7267: Gradients computed successfully\n","Step 7268: Loss: 0.2797, requires_grad: True\n","Step 7268: Gradients computed successfully\n","Step 7269: Loss: 0.1590, requires_grad: True\n","Step 7269: Gradients computed successfully\n","Step 7270: Loss: 0.2105, requires_grad: True\n","Step 7270: Gradients computed successfully\n","Epoch 0, Step 7270, Loss 0.2105, Avg Loss: 0.2555\n","Step 7271: Loss: 0.3113, requires_grad: True\n","Step 7271: Gradients computed successfully\n","Step 7272: Loss: 0.2775, requires_grad: True\n","Step 7272: Gradients computed successfully\n","Step 7273: Loss: 0.2752, requires_grad: True\n","Step 7273: Gradients computed successfully\n","Step 7274: Loss: 0.2162, requires_grad: True\n","Step 7274: Gradients computed successfully\n","Step 7275: Loss: 0.5630, requires_grad: True\n","Step 7275: Gradients computed successfully\n","Step 7276: Loss: 0.1681, requires_grad: True\n","Step 7276: Gradients computed successfully\n","Step 7277: Loss: 0.1754, requires_grad: True\n","Step 7277: Gradients computed successfully\n","Step 7278: Loss: 0.2613, requires_grad: True\n","Step 7278: Gradients computed successfully\n","Step 7279: Loss: 0.2640, requires_grad: True\n","Step 7279: Gradients computed successfully\n","Step 7280: Loss: 0.1717, requires_grad: True\n","Step 7280: Gradients computed successfully\n","Epoch 0, Step 7280, Loss 0.1717, Avg Loss: 0.2555\n","Step 7281: Loss: 0.4731, requires_grad: True\n","Step 7281: Gradients computed successfully\n","Step 7282: Loss: 0.2081, requires_grad: True\n","Step 7282: Gradients computed successfully\n","Step 7283: Loss: 0.2105, requires_grad: True\n","Step 7283: Gradients computed successfully\n","Step 7284: Loss: 0.2425, requires_grad: True\n","Step 7284: Gradients computed successfully\n","Step 7285: Loss: 0.2276, requires_grad: True\n","Step 7285: Gradients computed successfully\n","Step 7286: Loss: 0.4013, requires_grad: True\n","Step 7286: Gradients computed successfully\n","Step 7287: Loss: 0.2421, requires_grad: True\n","Step 7287: Gradients computed successfully\n","Step 7288: Loss: 0.2509, requires_grad: True\n","Step 7288: Gradients computed successfully\n","Step 7289: Loss: 0.2715, requires_grad: True\n","Step 7289: Gradients computed successfully\n","Step 7290: Loss: 0.1581, requires_grad: True\n","Step 7290: Gradients computed successfully\n","Epoch 0, Step 7290, Loss 0.1581, Avg Loss: 0.2556\n","Step 7291: Loss: 0.1777, requires_grad: True\n","Step 7291: Gradients computed successfully\n","Step 7292: Loss: 0.3670, requires_grad: True\n","Step 7292: Gradients computed successfully\n","Step 7293: Loss: 0.0783, requires_grad: True\n","Step 7293: Gradients computed successfully\n","Step 7294: Loss: 0.1423, requires_grad: True\n","Step 7294: Gradients computed successfully\n","Step 7295: Loss: 0.3972, requires_grad: True\n","Step 7295: Gradients computed successfully\n","Step 7296: Loss: 0.2630, requires_grad: True\n","Step 7296: Gradients computed successfully\n","Step 7297: Loss: 0.1303, requires_grad: True\n","Step 7297: Gradients computed successfully\n","Step 7298: Loss: 0.3556, requires_grad: True\n","Step 7298: Gradients computed successfully\n","Step 7299: Loss: 0.2973, requires_grad: True\n","Step 7299: Gradients computed successfully\n","Step 7300: Loss: 0.5238, requires_grad: True\n","Step 7300: Gradients computed successfully\n","Epoch 0, Step 7300, Loss 0.5238, Avg Loss: 0.2556\n","Step 7301: Loss: 0.2366, requires_grad: True\n","Step 7301: Gradients computed successfully\n","Step 7302: Loss: 0.1751, requires_grad: True\n","Step 7302: Gradients computed successfully\n","Step 7303: Loss: 0.1779, requires_grad: True\n","Step 7303: Gradients computed successfully\n","Step 7304: Loss: 0.3124, requires_grad: True\n","Step 7304: Gradients computed successfully\n","Step 7305: Loss: 0.3956, requires_grad: True\n","Step 7305: Gradients computed successfully\n","Step 7306: Loss: 0.3650, requires_grad: True\n","Step 7306: Gradients computed successfully\n","Step 7307: Loss: 0.2304, requires_grad: True\n","Step 7307: Gradients computed successfully\n","Step 7308: Loss: 0.3040, requires_grad: True\n","Step 7308: Gradients computed successfully\n","Step 7309: Loss: 0.1220, requires_grad: True\n","Step 7309: Gradients computed successfully\n","Step 7310: Loss: 0.2936, requires_grad: True\n","Step 7310: Gradients computed successfully\n","Epoch 0, Step 7310, Loss 0.2936, Avg Loss: 0.2556\n","Step 7311: Loss: 0.1826, requires_grad: True\n","Step 7311: Gradients computed successfully\n","Step 7312: Loss: 0.2429, requires_grad: True\n","Step 7312: Gradients computed successfully\n","Step 7313: Loss: 0.1091, requires_grad: True\n","Step 7313: Gradients computed successfully\n","Step 7314: Loss: 0.3578, requires_grad: True\n","Step 7314: Gradients computed successfully\n","Step 7315: Loss: 0.1370, requires_grad: True\n","Step 7315: Gradients computed successfully\n","Step 7316: Loss: 0.3999, requires_grad: True\n","Step 7316: Gradients computed successfully\n","Step 7317: Loss: 0.0974, requires_grad: True\n","Step 7317: Gradients computed successfully\n","Step 7318: Loss: 0.2375, requires_grad: True\n","Step 7318: Gradients computed successfully\n","Step 7319: Loss: 0.1650, requires_grad: True\n","Step 7319: Gradients computed successfully\n","Step 7320: Loss: 0.1409, requires_grad: True\n","Step 7320: Gradients computed successfully\n","Epoch 0, Step 7320, Loss 0.1409, Avg Loss: 0.2555\n","Step 7321: Loss: 0.1458, requires_grad: True\n","Step 7321: Gradients computed successfully\n","Step 7322: Loss: 0.2865, requires_grad: True\n","Step 7322: Gradients computed successfully\n","Step 7323: Loss: 0.2094, requires_grad: True\n","Step 7323: Gradients computed successfully\n","Step 7324: Loss: 0.1319, requires_grad: True\n","Step 7324: Gradients computed successfully\n","Step 7325: Loss: 0.2168, requires_grad: True\n","Step 7325: Gradients computed successfully\n","Step 7326: Loss: 0.2622, requires_grad: True\n","Step 7326: Gradients computed successfully\n","Step 7327: Loss: 0.1724, requires_grad: True\n","Step 7327: Gradients computed successfully\n","Step 7328: Loss: 0.4969, requires_grad: True\n","Step 7328: Gradients computed successfully\n","Step 7329: Loss: 0.7282, requires_grad: True\n","Step 7329: Gradients computed successfully\n","Step 7330: Loss: 0.1565, requires_grad: True\n","Step 7330: Gradients computed successfully\n","Epoch 0, Step 7330, Loss 0.1565, Avg Loss: 0.2556\n","Step 7331: Loss: 0.1702, requires_grad: True\n","Step 7331: Gradients computed successfully\n","Step 7332: Loss: 0.2522, requires_grad: True\n","Step 7332: Gradients computed successfully\n","Step 7333: Loss: 0.1857, requires_grad: True\n","Step 7333: Gradients computed successfully\n","Step 7334: Loss: 0.1859, requires_grad: True\n","Step 7334: Gradients computed successfully\n","Step 7335: Loss: 0.1852, requires_grad: True\n","Step 7335: Gradients computed successfully\n","Step 7336: Loss: 0.6419, requires_grad: True\n","Step 7336: Gradients computed successfully\n","Step 7337: Loss: 0.1526, requires_grad: True\n","Step 7337: Gradients computed successfully\n","Step 7338: Loss: 0.1694, requires_grad: True\n","Step 7338: Gradients computed successfully\n","Step 7339: Loss: 0.1922, requires_grad: True\n","Step 7339: Gradients computed successfully\n","Step 7340: Loss: 0.2096, requires_grad: True\n","Step 7340: Gradients computed successfully\n","Epoch 0, Step 7340, Loss 0.2096, Avg Loss: 0.2555\n","Step 7341: Loss: 0.2731, requires_grad: True\n","Step 7341: Gradients computed successfully\n","Step 7342: Loss: 0.2469, requires_grad: True\n","Step 7342: Gradients computed successfully\n","Step 7343: Loss: 0.1727, requires_grad: True\n","Step 7343: Gradients computed successfully\n","Step 7344: Loss: 0.0841, requires_grad: True\n","Step 7344: Gradients computed successfully\n","Step 7345: Loss: 0.1349, requires_grad: True\n","Step 7345: Gradients computed successfully\n","Step 7346: Loss: 0.1654, requires_grad: True\n","Step 7346: Gradients computed successfully\n","Step 7347: Loss: 0.2922, requires_grad: True\n","Step 7347: Gradients computed successfully\n","Step 7348: Loss: 0.3905, requires_grad: True\n","Step 7348: Gradients computed successfully\n","Step 7349: Loss: 0.2503, requires_grad: True\n","Step 7349: Gradients computed successfully\n","Step 7350: Loss: 0.1639, requires_grad: True\n","Step 7350: Gradients computed successfully\n","Epoch 0, Step 7350, Loss 0.1639, Avg Loss: 0.2555\n","Step 7351: Loss: 0.0917, requires_grad: True\n","Step 7351: Gradients computed successfully\n","Step 7352: Loss: 0.8210, requires_grad: True\n","Step 7352: Gradients computed successfully\n","Step 7353: Loss: 0.3974, requires_grad: True\n","Step 7353: Gradients computed successfully\n","Step 7354: Loss: 0.4060, requires_grad: True\n","Step 7354: Gradients computed successfully\n","Step 7355: Loss: 0.2619, requires_grad: True\n","Step 7355: Gradients computed successfully\n","Step 7356: Loss: 0.1900, requires_grad: True\n","Step 7356: Gradients computed successfully\n","Step 7357: Loss: 0.2423, requires_grad: True\n","Step 7357: Gradients computed successfully\n","Step 7358: Loss: 0.1323, requires_grad: True\n","Step 7358: Gradients computed successfully\n","Step 7359: Loss: 0.3062, requires_grad: True\n","Step 7359: Gradients computed successfully\n","Step 7360: Loss: 0.1848, requires_grad: True\n","Step 7360: Gradients computed successfully\n","Epoch 0, Step 7360, Loss 0.1848, Avg Loss: 0.2555\n","Step 7361: Loss: 0.1263, requires_grad: True\n","Step 7361: Gradients computed successfully\n","Step 7362: Loss: 0.1770, requires_grad: True\n","Step 7362: Gradients computed successfully\n","Step 7363: Loss: 0.3419, requires_grad: True\n","Step 7363: Gradients computed successfully\n","Step 7364: Loss: 0.1011, requires_grad: True\n","Step 7364: Gradients computed successfully\n","Step 7365: Loss: 0.1263, requires_grad: True\n","Step 7365: Gradients computed successfully\n","Step 7366: Loss: 0.3094, requires_grad: True\n","Step 7366: Gradients computed successfully\n","Step 7367: Loss: 0.3263, requires_grad: True\n","Step 7367: Gradients computed successfully\n","Step 7368: Loss: 0.3082, requires_grad: True\n","Step 7368: Gradients computed successfully\n","Step 7369: Loss: 0.3659, requires_grad: True\n","Step 7369: Gradients computed successfully\n","Step 7370: Loss: 0.5884, requires_grad: True\n","Step 7370: Gradients computed successfully\n","Epoch 0, Step 7370, Loss 0.5884, Avg Loss: 0.2556\n","Step 7371: Loss: 0.2310, requires_grad: True\n","Step 7371: Gradients computed successfully\n","Step 7372: Loss: 0.2677, requires_grad: True\n","Step 7372: Gradients computed successfully\n","Step 7373: Loss: 0.4016, requires_grad: True\n","Step 7373: Gradients computed successfully\n","Step 7374: Loss: 0.1237, requires_grad: True\n","Step 7374: Gradients computed successfully\n","Step 7375: Loss: 0.1559, requires_grad: True\n","Step 7375: Gradients computed successfully\n","Step 7376: Loss: 0.1684, requires_grad: True\n","Step 7376: Gradients computed successfully\n","Step 7377: Loss: 0.3551, requires_grad: True\n","Step 7377: Gradients computed successfully\n","Step 7378: Loss: 0.1486, requires_grad: True\n","Step 7378: Gradients computed successfully\n","Step 7379: Loss: 0.1581, requires_grad: True\n","Step 7379: Gradients computed successfully\n","Step 7380: Loss: 0.3688, requires_grad: True\n","Step 7380: Gradients computed successfully\n","Epoch 0, Step 7380, Loss 0.3688, Avg Loss: 0.2555\n","Step 7381: Loss: 0.1933, requires_grad: True\n","Step 7381: Gradients computed successfully\n","Step 7382: Loss: 0.2236, requires_grad: True\n","Step 7382: Gradients computed successfully\n","Step 7383: Loss: 0.5709, requires_grad: True\n","Step 7383: Gradients computed successfully\n","Step 7384: Loss: 0.4295, requires_grad: True\n","Step 7384: Gradients computed successfully\n","Step 7385: Loss: 0.0853, requires_grad: True\n","Step 7385: Gradients computed successfully\n","Step 7386: Loss: 0.4155, requires_grad: True\n","Step 7386: Gradients computed successfully\n","Step 7387: Loss: 0.2399, requires_grad: True\n","Step 7387: Gradients computed successfully\n","Step 7388: Loss: 0.2698, requires_grad: True\n","Step 7388: Gradients computed successfully\n","Step 7389: Loss: 0.1888, requires_grad: True\n","Step 7389: Gradients computed successfully\n","Step 7390: Loss: 0.2789, requires_grad: True\n","Step 7390: Gradients computed successfully\n","Epoch 0, Step 7390, Loss 0.2789, Avg Loss: 0.2556\n","Step 7391: Loss: 0.3054, requires_grad: True\n","Step 7391: Gradients computed successfully\n","Step 7392: Loss: 0.1915, requires_grad: True\n","Step 7392: Gradients computed successfully\n","Step 7393: Loss: 0.2084, requires_grad: True\n","Step 7393: Gradients computed successfully\n","Step 7394: Loss: 0.2433, requires_grad: True\n","Step 7394: Gradients computed successfully\n","Step 7395: Loss: 0.1417, requires_grad: True\n","Step 7395: Gradients computed successfully\n","Step 7396: Loss: 0.4273, requires_grad: True\n","Step 7396: Gradients computed successfully\n","Step 7397: Loss: 0.4620, requires_grad: True\n","Step 7397: Gradients computed successfully\n","Step 7398: Loss: 0.1231, requires_grad: True\n","Step 7398: Gradients computed successfully\n","Step 7399: Loss: 0.1829, requires_grad: True\n","Step 7399: Gradients computed successfully\n","Step 7400: Loss: 0.1702, requires_grad: True\n","Step 7400: Gradients computed successfully\n","Epoch 0, Step 7400, Loss 0.1702, Avg Loss: 0.2556\n","Step 7401: Loss: 0.2151, requires_grad: True\n","Step 7401: Gradients computed successfully\n","Step 7402: Loss: 0.2664, requires_grad: True\n","Step 7402: Gradients computed successfully\n","Step 7403: Loss: 0.2684, requires_grad: True\n","Step 7403: Gradients computed successfully\n","Step 7404: Loss: 0.3162, requires_grad: True\n","Step 7404: Gradients computed successfully\n","Step 7405: Loss: 0.1344, requires_grad: True\n","Step 7405: Gradients computed successfully\n","Step 7406: Loss: 0.1795, requires_grad: True\n","Step 7406: Gradients computed successfully\n","Step 7407: Loss: 0.1170, requires_grad: True\n","Step 7407: Gradients computed successfully\n","Step 7408: Loss: 0.2332, requires_grad: True\n","Step 7408: Gradients computed successfully\n","Step 7409: Loss: 0.1435, requires_grad: True\n","Step 7409: Gradients computed successfully\n","Step 7410: Loss: 0.2165, requires_grad: True\n","Step 7410: Gradients computed successfully\n","Epoch 0, Step 7410, Loss 0.2165, Avg Loss: 0.2555\n","Step 7411: Loss: 0.1946, requires_grad: True\n","Step 7411: Gradients computed successfully\n","Step 7412: Loss: 0.1249, requires_grad: True\n","Step 7412: Gradients computed successfully\n","Step 7413: Loss: 0.5830, requires_grad: True\n","Step 7413: Gradients computed successfully\n","Step 7414: Loss: 0.2326, requires_grad: True\n","Step 7414: Gradients computed successfully\n","Step 7415: Loss: 0.1336, requires_grad: True\n","Step 7415: Gradients computed successfully\n","Step 7416: Loss: 0.6367, requires_grad: True\n","Step 7416: Gradients computed successfully\n","Step 7417: Loss: 0.5294, requires_grad: True\n","Step 7417: Gradients computed successfully\n","Step 7418: Loss: 0.0870, requires_grad: True\n","Step 7418: Gradients computed successfully\n","Step 7419: Loss: 0.1299, requires_grad: True\n","Step 7419: Gradients computed successfully\n","Step 7420: Loss: 0.1518, requires_grad: True\n","Step 7420: Gradients computed successfully\n","Epoch 0, Step 7420, Loss 0.1518, Avg Loss: 0.2556\n","Step 7421: Loss: 0.1562, requires_grad: True\n","Step 7421: Gradients computed successfully\n","Step 7422: Loss: 0.3699, requires_grad: True\n","Step 7422: Gradients computed successfully\n","Step 7423: Loss: 0.3002, requires_grad: True\n","Step 7423: Gradients computed successfully\n","Step 7424: Loss: 0.1575, requires_grad: True\n","Step 7424: Gradients computed successfully\n","Step 7425: Loss: 0.1937, requires_grad: True\n","Step 7425: Gradients computed successfully\n","Step 7426: Loss: 0.2019, requires_grad: True\n","Step 7426: Gradients computed successfully\n","Step 7427: Loss: 0.1769, requires_grad: True\n","Step 7427: Gradients computed successfully\n","Step 7428: Loss: 0.4663, requires_grad: True\n","Step 7428: Gradients computed successfully\n","Step 7429: Loss: 0.3654, requires_grad: True\n","Step 7429: Gradients computed successfully\n","Step 7430: Loss: 0.2281, requires_grad: True\n","Step 7430: Gradients computed successfully\n","Epoch 0, Step 7430, Loss 0.2281, Avg Loss: 0.2556\n","Step 7431: Loss: 0.2007, requires_grad: True\n","Step 7431: Gradients computed successfully\n","Step 7432: Loss: 0.1940, requires_grad: True\n","Step 7432: Gradients computed successfully\n","Step 7433: Loss: 0.1674, requires_grad: True\n","Step 7433: Gradients computed successfully\n","Step 7434: Loss: 0.2400, requires_grad: True\n","Step 7434: Gradients computed successfully\n","Step 7435: Loss: 0.1221, requires_grad: True\n","Step 7435: Gradients computed successfully\n","Step 7436: Loss: 0.2622, requires_grad: True\n","Step 7436: Gradients computed successfully\n","Step 7437: Loss: 0.2360, requires_grad: True\n","Step 7437: Gradients computed successfully\n","Step 7438: Loss: 0.2780, requires_grad: True\n","Step 7438: Gradients computed successfully\n","Step 7439: Loss: 0.2346, requires_grad: True\n","Step 7439: Gradients computed successfully\n","Step 7440: Loss: 0.1388, requires_grad: True\n","Step 7440: Gradients computed successfully\n","Epoch 0, Step 7440, Loss 0.1388, Avg Loss: 0.2555\n","Step 7441: Loss: 0.2626, requires_grad: True\n","Step 7441: Gradients computed successfully\n","Step 7442: Loss: 0.3763, requires_grad: True\n","Step 7442: Gradients computed successfully\n","Step 7443: Loss: 0.2423, requires_grad: True\n","Step 7443: Gradients computed successfully\n","Step 7444: Loss: 0.1981, requires_grad: True\n","Step 7444: Gradients computed successfully\n","Step 7445: Loss: 0.3205, requires_grad: True\n","Step 7445: Gradients computed successfully\n","Step 7446: Loss: 0.9235, requires_grad: True\n","Step 7446: Gradients computed successfully\n","Step 7447: Loss: 0.0968, requires_grad: True\n","Step 7447: Gradients computed successfully\n","Step 7448: Loss: 0.3904, requires_grad: True\n","Step 7448: Gradients computed successfully\n","Step 7449: Loss: 0.3560, requires_grad: True\n","Step 7449: Gradients computed successfully\n","Step 7450: Loss: 0.0999, requires_grad: True\n","Step 7450: Gradients computed successfully\n","Epoch 0, Step 7450, Loss 0.0999, Avg Loss: 0.2556\n","Step 7451: Loss: 0.1569, requires_grad: True\n","Step 7451: Gradients computed successfully\n","Step 7452: Loss: 0.3382, requires_grad: True\n","Step 7452: Gradients computed successfully\n","Step 7453: Loss: 0.2407, requires_grad: True\n","Step 7453: Gradients computed successfully\n","Step 7454: Loss: 0.2757, requires_grad: True\n","Step 7454: Gradients computed successfully\n","Step 7455: Loss: 0.0864, requires_grad: True\n","Step 7455: Gradients computed successfully\n","Step 7456: Loss: 0.5146, requires_grad: True\n","Step 7456: Gradients computed successfully\n","Step 7457: Loss: 0.1190, requires_grad: True\n","Step 7457: Gradients computed successfully\n","Step 7458: Loss: 0.4516, requires_grad: True\n","Step 7458: Gradients computed successfully\n","Step 7459: Loss: 0.2336, requires_grad: True\n","Step 7459: Gradients computed successfully\n","Step 7460: Loss: 0.2037, requires_grad: True\n","Step 7460: Gradients computed successfully\n","Epoch 0, Step 7460, Loss 0.2037, Avg Loss: 0.2556\n","Step 7461: Loss: 0.1206, requires_grad: True\n","Step 7461: Gradients computed successfully\n","Step 7462: Loss: 0.2136, requires_grad: True\n","Step 7462: Gradients computed successfully\n","Step 7463: Loss: 0.2274, requires_grad: True\n","Step 7463: Gradients computed successfully\n","Step 7464: Loss: 0.1781, requires_grad: True\n","Step 7464: Gradients computed successfully\n","Step 7465: Loss: 0.1525, requires_grad: True\n","Step 7465: Gradients computed successfully\n","Step 7466: Loss: 0.1551, requires_grad: True\n","Step 7466: Gradients computed successfully\n","Step 7467: Loss: 0.1684, requires_grad: True\n","Step 7467: Gradients computed successfully\n","Step 7468: Loss: 0.1452, requires_grad: True\n","Step 7468: Gradients computed successfully\n","Step 7469: Loss: 0.1046, requires_grad: True\n","Step 7469: Gradients computed successfully\n","Step 7470: Loss: 0.0843, requires_grad: True\n","Step 7470: Gradients computed successfully\n","Epoch 0, Step 7470, Loss 0.0843, Avg Loss: 0.2555\n","Step 7471: Loss: 0.1610, requires_grad: True\n","Step 7471: Gradients computed successfully\n","Step 7472: Loss: 0.2133, requires_grad: True\n","Step 7472: Gradients computed successfully\n","Step 7473: Loss: 0.2347, requires_grad: True\n","Step 7473: Gradients computed successfully\n","Step 7474: Loss: 0.1374, requires_grad: True\n","Step 7474: Gradients computed successfully\n","Step 7475: Loss: 0.1714, requires_grad: True\n","Step 7475: Gradients computed successfully\n","Step 7476: Loss: 0.1015, requires_grad: True\n","Step 7476: Gradients computed successfully\n","Step 7477: Loss: 0.2051, requires_grad: True\n","Step 7477: Gradients computed successfully\n","Step 7478: Loss: 0.1344, requires_grad: True\n","Step 7478: Gradients computed successfully\n","Step 7479: Loss: 0.2223, requires_grad: True\n","Step 7479: Gradients computed successfully\n","Step 7480: Loss: 0.2158, requires_grad: True\n","Step 7480: Gradients computed successfully\n","Epoch 0, Step 7480, Loss 0.2158, Avg Loss: 0.2554\n","Step 7481: Loss: 0.2262, requires_grad: True\n","Step 7481: Gradients computed successfully\n","Step 7482: Loss: 0.2526, requires_grad: True\n","Step 7482: Gradients computed successfully\n","Step 7483: Loss: 0.1531, requires_grad: True\n","Step 7483: Gradients computed successfully\n","Step 7484: Loss: 0.1766, requires_grad: True\n","Step 7484: Gradients computed successfully\n","Step 7485: Loss: 0.2533, requires_grad: True\n","Step 7485: Gradients computed successfully\n","Step 7486: Loss: 0.1284, requires_grad: True\n","Step 7486: Gradients computed successfully\n","Step 7487: Loss: 0.2788, requires_grad: True\n","Step 7487: Gradients computed successfully\n","Step 7488: Loss: 0.4414, requires_grad: True\n","Step 7488: Gradients computed successfully\n","Step 7489: Loss: 0.2635, requires_grad: True\n","Step 7489: Gradients computed successfully\n","Step 7490: Loss: 0.2274, requires_grad: True\n","Step 7490: Gradients computed successfully\n","Epoch 0, Step 7490, Loss 0.2274, Avg Loss: 0.2553\n","Step 7491: Loss: 0.2817, requires_grad: True\n","Step 7491: Gradients computed successfully\n","Step 7492: Loss: 0.2032, requires_grad: True\n","Step 7492: Gradients computed successfully\n","Step 7493: Loss: 0.1812, requires_grad: True\n","Step 7493: Gradients computed successfully\n","Step 7494: Loss: 0.3404, requires_grad: True\n","Step 7494: Gradients computed successfully\n","Step 7495: Loss: 0.2549, requires_grad: True\n","Step 7495: Gradients computed successfully\n","Step 7496: Loss: 0.1420, requires_grad: True\n","Step 7496: Gradients computed successfully\n","Step 7497: Loss: 0.4322, requires_grad: True\n","Step 7497: Gradients computed successfully\n","Step 7498: Loss: 0.1603, requires_grad: True\n","Step 7498: Gradients computed successfully\n","Step 7499: Loss: 0.1809, requires_grad: True\n","Step 7499: Gradients computed successfully\n","Step 7500: Loss: 0.2019, requires_grad: True\n","Step 7500: Gradients computed successfully\n","Epoch 0, Step 7500, Loss 0.2019, Avg Loss: 0.2553\n","Saved checkpoint to /content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_complex_sql_continued/manual-epoch-0-step-7500\n","Step 7501: Loss: 0.2837, requires_grad: True\n","Step 7501: Gradients computed successfully\n","Step 7502: Loss: 0.2351, requires_grad: True\n","Step 7502: Gradients computed successfully\n","Step 7503: Loss: 0.1774, requires_grad: True\n","Step 7503: Gradients computed successfully\n","Step 7504: Loss: 0.2245, requires_grad: True\n","Step 7504: Gradients computed successfully\n","Step 7505: Loss: 0.3198, requires_grad: True\n","Step 7505: Gradients computed successfully\n","Step 7506: Loss: 0.1083, requires_grad: True\n","Step 7506: Gradients computed successfully\n","Step 7507: Loss: 0.3590, requires_grad: True\n","Step 7507: Gradients computed successfully\n","Step 7508: Loss: 0.3091, requires_grad: True\n","Step 7508: Gradients computed successfully\n","Step 7509: Loss: 0.1394, requires_grad: True\n","Step 7509: Gradients computed successfully\n","Step 7510: Loss: 0.2936, requires_grad: True\n","Step 7510: Gradients computed successfully\n","Epoch 0, Step 7510, Loss 0.2936, Avg Loss: 0.2553\n","Step 7511: Loss: 0.3430, requires_grad: True\n","Step 7511: Gradients computed successfully\n","Step 7512: Loss: 0.4640, requires_grad: True\n","Step 7512: Gradients computed successfully\n","Step 7513: Loss: 0.1007, requires_grad: True\n","Step 7513: Gradients computed successfully\n","Step 7514: Loss: 0.3600, requires_grad: True\n","Step 7514: Gradients computed successfully\n","Step 7515: Loss: 0.3984, requires_grad: True\n","Step 7515: Gradients computed successfully\n","Step 7516: Loss: 0.2394, requires_grad: True\n","Step 7516: Gradients computed successfully\n","Step 7517: Loss: 0.5503, requires_grad: True\n","Step 7517: Gradients computed successfully\n","Step 7518: Loss: 0.2358, requires_grad: True\n","Step 7518: Gradients computed successfully\n","Step 7519: Loss: 0.1409, requires_grad: True\n","Step 7519: Gradients computed successfully\n","Step 7520: Loss: 0.2953, requires_grad: True\n","Step 7520: Gradients computed successfully\n","Epoch 0, Step 7520, Loss 0.2953, Avg Loss: 0.2554\n","Step 7521: Loss: 0.1507, requires_grad: True\n","Step 7521: Gradients computed successfully\n","Step 7522: Loss: 0.1209, requires_grad: True\n","Step 7522: Gradients computed successfully\n","Step 7523: Loss: 0.1848, requires_grad: True\n","Step 7523: Gradients computed successfully\n","Step 7524: Loss: 0.2380, requires_grad: True\n","Step 7524: Gradients computed successfully\n","Step 7525: Loss: 0.2577, requires_grad: True\n","Step 7525: Gradients computed successfully\n","Step 7526: Loss: 0.1782, requires_grad: True\n","Step 7526: Gradients computed successfully\n","Step 7527: Loss: 0.1968, requires_grad: True\n","Step 7527: Gradients computed successfully\n","Step 7528: Loss: 0.2518, requires_grad: True\n","Step 7528: Gradients computed successfully\n","Step 7529: Loss: 0.1943, requires_grad: True\n","Step 7529: Gradients computed successfully\n","Step 7530: Loss: 0.1894, requires_grad: True\n","Step 7530: Gradients computed successfully\n","Epoch 0, Step 7530, Loss 0.1894, Avg Loss: 0.2553\n","Step 7531: Loss: 0.3119, requires_grad: True\n","Step 7531: Gradients computed successfully\n","Step 7532: Loss: 0.5300, requires_grad: True\n","Step 7532: Gradients computed successfully\n","Step 7533: Loss: 0.1828, requires_grad: True\n","Step 7533: Gradients computed successfully\n","Step 7534: Loss: 0.0917, requires_grad: True\n","Step 7534: Gradients computed successfully\n","Step 7535: Loss: 0.2405, requires_grad: True\n","Step 7535: Gradients computed successfully\n","Step 7536: Loss: 0.1673, requires_grad: True\n","Step 7536: Gradients computed successfully\n","Step 7537: Loss: 0.3351, requires_grad: True\n","Step 7537: Gradients computed successfully\n","Step 7538: Loss: 0.3871, requires_grad: True\n","Step 7538: Gradients computed successfully\n","Step 7539: Loss: 0.2110, requires_grad: True\n","Step 7539: Gradients computed successfully\n","Step 7540: Loss: 0.1347, requires_grad: True\n","Step 7540: Gradients computed successfully\n","Epoch 0, Step 7540, Loss 0.1347, Avg Loss: 0.2553\n","Step 7541: Loss: 0.2774, requires_grad: True\n","Step 7541: Gradients computed successfully\n","Step 7542: Loss: 0.2776, requires_grad: True\n","Step 7542: Gradients computed successfully\n","Step 7543: Loss: 0.1030, requires_grad: True\n","Step 7543: Gradients computed successfully\n","Step 7544: Loss: 0.2862, requires_grad: True\n","Step 7544: Gradients computed successfully\n","Step 7545: Loss: 0.2397, requires_grad: True\n","Step 7545: Gradients computed successfully\n","Step 7546: Loss: 0.1912, requires_grad: True\n","Step 7546: Gradients computed successfully\n","Step 7547: Loss: 0.2862, requires_grad: True\n","Step 7547: Gradients computed successfully\n","Step 7548: Loss: 0.2301, requires_grad: True\n","Step 7548: Gradients computed successfully\n","Step 7549: Loss: 0.1550, requires_grad: True\n","Step 7549: Gradients computed successfully\n","Step 7550: Loss: 0.1991, requires_grad: True\n","Step 7550: Gradients computed successfully\n","Epoch 0, Step 7550, Loss 0.1991, Avg Loss: 0.2553\n","Step 7551: Loss: 0.2600, requires_grad: True\n","Step 7551: Gradients computed successfully\n","Step 7552: Loss: 0.3123, requires_grad: True\n","Step 7552: Gradients computed successfully\n","Step 7553: Loss: 0.1505, requires_grad: True\n","Step 7553: Gradients computed successfully\n","Step 7554: Loss: 0.4483, requires_grad: True\n","Step 7554: Gradients computed successfully\n","Step 7555: Loss: 0.3078, requires_grad: True\n","Step 7555: Gradients computed successfully\n","Step 7556: Loss: 0.3241, requires_grad: True\n","Step 7556: Gradients computed successfully\n","Step 7557: Loss: 0.3104, requires_grad: True\n","Step 7557: Gradients computed successfully\n","Step 7558: Loss: 0.2941, requires_grad: True\n","Step 7558: Gradients computed successfully\n","Step 7559: Loss: 0.3616, requires_grad: True\n","Step 7559: Gradients computed successfully\n","Step 7560: Loss: 0.2103, requires_grad: True\n","Step 7560: Gradients computed successfully\n","Epoch 0, Step 7560, Loss 0.2103, Avg Loss: 0.2553\n","Step 7561: Loss: 0.2860, requires_grad: True\n","Step 7561: Gradients computed successfully\n","Step 7562: Loss: 0.3500, requires_grad: True\n","Step 7562: Gradients computed successfully\n","Step 7563: Loss: 0.0950, requires_grad: True\n","Step 7563: Gradients computed successfully\n","Step 7564: Loss: 0.2045, requires_grad: True\n","Step 7564: Gradients computed successfully\n","Step 7565: Loss: 0.2895, requires_grad: True\n","Step 7565: Gradients computed successfully\n","Step 7566: Loss: 0.0936, requires_grad: True\n","Step 7566: Gradients computed successfully\n","Step 7567: Loss: 0.3383, requires_grad: True\n","Step 7567: Gradients computed successfully\n","Step 7568: Loss: 0.2308, requires_grad: True\n","Step 7568: Gradients computed successfully\n","Step 7569: Loss: 0.4189, requires_grad: True\n","Step 7569: Gradients computed successfully\n","Step 7570: Loss: 0.2543, requires_grad: True\n","Step 7570: Gradients computed successfully\n","Epoch 0, Step 7570, Loss 0.2543, Avg Loss: 0.2553\n","Step 7571: Loss: 0.2340, requires_grad: True\n","Step 7571: Gradients computed successfully\n","Step 7572: Loss: 0.1631, requires_grad: True\n","Step 7572: Gradients computed successfully\n","Step 7573: Loss: 0.2429, requires_grad: True\n","Step 7573: Gradients computed successfully\n","Step 7574: Loss: 0.3113, requires_grad: True\n","Step 7574: Gradients computed successfully\n","Step 7575: Loss: 0.3440, requires_grad: True\n","Step 7575: Gradients computed successfully\n","Step 7576: Loss: 0.2652, requires_grad: True\n","Step 7576: Gradients computed successfully\n","Step 7577: Loss: 0.1365, requires_grad: True\n","Step 7577: Gradients computed successfully\n","Step 7578: Loss: 0.2028, requires_grad: True\n","Step 7578: Gradients computed successfully\n","Step 7579: Loss: 0.2593, requires_grad: True\n","Step 7579: Gradients computed successfully\n","Step 7580: Loss: 0.2280, requires_grad: True\n","Step 7580: Gradients computed successfully\n","Epoch 0, Step 7580, Loss 0.2280, Avg Loss: 0.2553\n","Step 7581: Loss: 0.2719, requires_grad: True\n","Step 7581: Gradients computed successfully\n","Step 7582: Loss: 0.2382, requires_grad: True\n","Step 7582: Gradients computed successfully\n","Step 7583: Loss: 0.3061, requires_grad: True\n","Step 7583: Gradients computed successfully\n","Step 7584: Loss: 0.2470, requires_grad: True\n","Step 7584: Gradients computed successfully\n","Step 7585: Loss: 0.1770, requires_grad: True\n","Step 7585: Gradients computed successfully\n","Step 7586: Loss: 0.2014, requires_grad: True\n","Step 7586: Gradients computed successfully\n","Step 7587: Loss: 0.2638, requires_grad: True\n","Step 7587: Gradients computed successfully\n","Step 7588: Loss: 0.2256, requires_grad: True\n","Step 7588: Gradients computed successfully\n","Step 7589: Loss: 0.2342, requires_grad: True\n","Step 7589: Gradients computed successfully\n","Step 7590: Loss: 0.1804, requires_grad: True\n","Step 7590: Gradients computed successfully\n","Epoch 0, Step 7590, Loss 0.1804, Avg Loss: 0.2553\n","Step 7591: Loss: 0.1915, requires_grad: True\n","Step 7591: Gradients computed successfully\n","Step 7592: Loss: 0.0745, requires_grad: True\n","Step 7592: Gradients computed successfully\n","Step 7593: Loss: 0.2817, requires_grad: True\n","Step 7593: Gradients computed successfully\n","Step 7594: Loss: 0.1671, requires_grad: True\n","Step 7594: Gradients computed successfully\n","Step 7595: Loss: 0.1906, requires_grad: True\n","Step 7595: Gradients computed successfully\n","Step 7596: Loss: 0.2167, requires_grad: True\n","Step 7596: Gradients computed successfully\n","Step 7597: Loss: 0.1563, requires_grad: True\n","Step 7597: Gradients computed successfully\n","Step 7598: Loss: 0.2692, requires_grad: True\n","Step 7598: Gradients computed successfully\n","Step 7599: Loss: 0.1734, requires_grad: True\n","Step 7599: Gradients computed successfully\n","Step 7600: Loss: 0.2080, requires_grad: True\n","Step 7600: Gradients computed successfully\n","Epoch 0, Step 7600, Loss 0.2080, Avg Loss: 0.2552\n","Step 7601: Loss: 0.2021, requires_grad: True\n","Step 7601: Gradients computed successfully\n","Step 7602: Loss: 0.1940, requires_grad: True\n","Step 7602: Gradients computed successfully\n","Step 7603: Loss: 0.3445, requires_grad: True\n","Step 7603: Gradients computed successfully\n","Step 7604: Loss: 0.1214, requires_grad: True\n","Step 7604: Gradients computed successfully\n","Step 7605: Loss: 0.1692, requires_grad: True\n","Step 7605: Gradients computed successfully\n","Step 7606: Loss: 0.3917, requires_grad: True\n","Step 7606: Gradients computed successfully\n","Step 7607: Loss: 0.3157, requires_grad: True\n","Step 7607: Gradients computed successfully\n","Step 7608: Loss: 0.1093, requires_grad: True\n","Step 7608: Gradients computed successfully\n","Step 7609: Loss: 0.5244, requires_grad: True\n","Step 7609: Gradients computed successfully\n","Step 7610: Loss: 0.1601, requires_grad: True\n","Step 7610: Gradients computed successfully\n","Epoch 0, Step 7610, Loss 0.1601, Avg Loss: 0.2552\n","Step 7611: Loss: 0.3960, requires_grad: True\n","Step 7611: Gradients computed successfully\n","Step 7612: Loss: 0.2538, requires_grad: True\n","Step 7612: Gradients computed successfully\n","Step 7613: Loss: 0.2628, requires_grad: True\n","Step 7613: Gradients computed successfully\n","Step 7614: Loss: 0.2739, requires_grad: True\n","Step 7614: Gradients computed successfully\n","Step 7615: Loss: 0.1311, requires_grad: True\n","Step 7615: Gradients computed successfully\n","Step 7616: Loss: 0.2576, requires_grad: True\n","Step 7616: Gradients computed successfully\n","Step 7617: Loss: 0.2822, requires_grad: True\n","Step 7617: Gradients computed successfully\n","Step 7618: Loss: 0.2382, requires_grad: True\n","Step 7618: Gradients computed successfully\n","Step 7619: Loss: 0.2384, requires_grad: True\n","Step 7619: Gradients computed successfully\n","Step 7620: Loss: 0.2183, requires_grad: True\n","Step 7620: Gradients computed successfully\n","Epoch 0, Step 7620, Loss 0.2183, Avg Loss: 0.2552\n","Step 7621: Loss: 0.1755, requires_grad: True\n","Step 7621: Gradients computed successfully\n","Step 7622: Loss: 0.0799, requires_grad: True\n","Step 7622: Gradients computed successfully\n","Step 7623: Loss: 0.1807, requires_grad: True\n","Step 7623: Gradients computed successfully\n","Step 7624: Loss: 0.5021, requires_grad: True\n","Step 7624: Gradients computed successfully\n","Step 7625: Loss: 0.2170, requires_grad: True\n","Step 7625: Gradients computed successfully\n","Step 7626: Loss: 0.1378, requires_grad: True\n","Step 7626: Gradients computed successfully\n","Step 7627: Loss: 0.2933, requires_grad: True\n","Step 7627: Gradients computed successfully\n","Step 7628: Loss: 0.1751, requires_grad: True\n","Step 7628: Gradients computed successfully\n","Step 7629: Loss: 0.1780, requires_grad: True\n","Step 7629: Gradients computed successfully\n","Step 7630: Loss: 0.1318, requires_grad: True\n","Step 7630: Gradients computed successfully\n","Epoch 0, Step 7630, Loss 0.1318, Avg Loss: 0.2551\n","Step 7631: Loss: 0.2600, requires_grad: True\n","Step 7631: Gradients computed successfully\n","Step 7632: Loss: 0.4975, requires_grad: True\n","Step 7632: Gradients computed successfully\n","Step 7633: Loss: 0.2148, requires_grad: True\n","Step 7633: Gradients computed successfully\n","Step 7634: Loss: 0.2028, requires_grad: True\n","Step 7634: Gradients computed successfully\n","Step 7635: Loss: 0.5525, requires_grad: True\n","Step 7635: Gradients computed successfully\n","Step 7636: Loss: 0.1255, requires_grad: True\n","Step 7636: Gradients computed successfully\n","Step 7637: Loss: 0.2808, requires_grad: True\n","Step 7637: Gradients computed successfully\n","Step 7638: Loss: 0.3693, requires_grad: True\n","Step 7638: Gradients computed successfully\n","Step 7639: Loss: 0.1418, requires_grad: True\n","Step 7639: Gradients computed successfully\n","Step 7640: Loss: 0.1534, requires_grad: True\n","Step 7640: Gradients computed successfully\n","Epoch 0, Step 7640, Loss 0.1534, Avg Loss: 0.2552\n","Step 7641: Loss: 0.2532, requires_grad: True\n","Step 7641: Gradients computed successfully\n","Step 7642: Loss: 0.2901, requires_grad: True\n","Step 7642: Gradients computed successfully\n","Step 7643: Loss: 0.1959, requires_grad: True\n","Step 7643: Gradients computed successfully\n","Step 7644: Loss: 0.3512, requires_grad: True\n","Step 7644: Gradients computed successfully\n","Step 7645: Loss: 0.1727, requires_grad: True\n","Step 7645: Gradients computed successfully\n","Step 7646: Loss: 0.1597, requires_grad: True\n","Step 7646: Gradients computed successfully\n","Step 7647: Loss: 0.1160, requires_grad: True\n","Step 7647: Gradients computed successfully\n","Step 7648: Loss: 0.0664, requires_grad: True\n","Step 7648: Gradients computed successfully\n","Step 7649: Loss: 0.3740, requires_grad: True\n","Step 7649: Gradients computed successfully\n","Step 7650: Loss: 0.1891, requires_grad: True\n","Step 7650: Gradients computed successfully\n","Epoch 0, Step 7650, Loss 0.1891, Avg Loss: 0.2551\n","Step 7651: Loss: 0.3916, requires_grad: True\n","Step 7651: Gradients computed successfully\n","Step 7652: Loss: 0.1758, requires_grad: True\n","Step 7652: Gradients computed successfully\n","Step 7653: Loss: 0.0921, requires_grad: True\n","Step 7653: Gradients computed successfully\n","Step 7654: Loss: 0.2060, requires_grad: True\n","Step 7654: Gradients computed successfully\n","Step 7655: Loss: 0.2219, requires_grad: True\n","Step 7655: Gradients computed successfully\n","Step 7656: Loss: 0.1593, requires_grad: True\n","Step 7656: Gradients computed successfully\n","Step 7657: Loss: 0.5018, requires_grad: True\n","Step 7657: Gradients computed successfully\n","Step 7658: Loss: 0.5429, requires_grad: True\n","Step 7658: Gradients computed successfully\n","Step 7659: Loss: 0.2419, requires_grad: True\n","Step 7659: Gradients computed successfully\n","Step 7660: Loss: 0.2563, requires_grad: True\n","Step 7660: Gradients computed successfully\n","Epoch 0, Step 7660, Loss 0.2563, Avg Loss: 0.2551\n","Step 7661: Loss: 0.4337, requires_grad: True\n","Step 7661: Gradients computed successfully\n","Step 7662: Loss: 0.1202, requires_grad: True\n","Step 7662: Gradients computed successfully\n","Step 7663: Loss: 0.2312, requires_grad: True\n","Step 7663: Gradients computed successfully\n","Step 7664: Loss: 0.2076, requires_grad: True\n","Step 7664: Gradients computed successfully\n","Step 7665: Loss: 0.1913, requires_grad: True\n","Step 7665: Gradients computed successfully\n","Step 7666: Loss: 0.4533, requires_grad: True\n","Step 7666: Gradients computed successfully\n","Step 7667: Loss: 0.4257, requires_grad: True\n","Step 7667: Gradients computed successfully\n","Step 7668: Loss: 0.1122, requires_grad: True\n","Step 7668: Gradients computed successfully\n","Step 7669: Loss: 0.2260, requires_grad: True\n","Step 7669: Gradients computed successfully\n","Step 7670: Loss: 0.1832, requires_grad: True\n","Step 7670: Gradients computed successfully\n","Epoch 0, Step 7670, Loss 0.1832, Avg Loss: 0.2551\n","Step 7671: Loss: 0.1450, requires_grad: True\n","Step 7671: Gradients computed successfully\n","Step 7672: Loss: 0.2402, requires_grad: True\n","Step 7672: Gradients computed successfully\n","Step 7673: Loss: 0.0739, requires_grad: True\n","Step 7673: Gradients computed successfully\n","Step 7674: Loss: 0.0978, requires_grad: True\n","Step 7674: Gradients computed successfully\n","Step 7675: Loss: 0.1361, requires_grad: True\n","Step 7675: Gradients computed successfully\n","Step 7676: Loss: 0.1712, requires_grad: True\n","Step 7676: Gradients computed successfully\n","Step 7677: Loss: 0.1846, requires_grad: True\n","Step 7677: Gradients computed successfully\n","Step 7678: Loss: 0.2029, requires_grad: True\n","Step 7678: Gradients computed successfully\n","Step 7679: Loss: 0.4055, requires_grad: True\n","Step 7679: Gradients computed successfully\n","Step 7680: Loss: 0.7202, requires_grad: True\n","Step 7680: Gradients computed successfully\n","Epoch 0, Step 7680, Loss 0.7202, Avg Loss: 0.2551\n","Step 7681: Loss: 0.2161, requires_grad: True\n","Step 7681: Gradients computed successfully\n","Step 7682: Loss: 0.1609, requires_grad: True\n","Step 7682: Gradients computed successfully\n","Step 7683: Loss: 0.1040, requires_grad: True\n","Step 7683: Gradients computed successfully\n","Step 7684: Loss: 0.1249, requires_grad: True\n","Step 7684: Gradients computed successfully\n","Step 7685: Loss: 0.2020, requires_grad: True\n","Step 7685: Gradients computed successfully\n","Step 7686: Loss: 0.3107, requires_grad: True\n","Step 7686: Gradients computed successfully\n","Step 7687: Loss: 0.1831, requires_grad: True\n","Step 7687: Gradients computed successfully\n","Step 7688: Loss: 0.3515, requires_grad: True\n","Step 7688: Gradients computed successfully\n","Step 7689: Loss: 0.2699, requires_grad: True\n","Step 7689: Gradients computed successfully\n","Step 7690: Loss: 0.1762, requires_grad: True\n","Step 7690: Gradients computed successfully\n","Epoch 0, Step 7690, Loss 0.1762, Avg Loss: 0.2551\n","Step 7691: Loss: 0.4086, requires_grad: True\n","Step 7691: Gradients computed successfully\n","Step 7692: Loss: 0.4231, requires_grad: True\n","Step 7692: Gradients computed successfully\n","Step 7693: Loss: 0.1280, requires_grad: True\n","Step 7693: Gradients computed successfully\n","Step 7694: Loss: 0.1431, requires_grad: True\n","Step 7694: Gradients computed successfully\n","Step 7695: Loss: 0.2263, requires_grad: True\n","Step 7695: Gradients computed successfully\n","Step 7696: Loss: 0.1382, requires_grad: True\n","Step 7696: Gradients computed successfully\n","Step 7697: Loss: 0.4315, requires_grad: True\n","Step 7697: Gradients computed successfully\n","Step 7698: Loss: 0.2624, requires_grad: True\n","Step 7698: Gradients computed successfully\n","Step 7699: Loss: 0.2796, requires_grad: True\n","Step 7699: Gradients computed successfully\n","Step 7700: Loss: 0.6278, requires_grad: True\n","Step 7700: Gradients computed successfully\n","Epoch 0, Step 7700, Loss 0.6278, Avg Loss: 0.2551\n","Step 7701: Loss: 0.1725, requires_grad: True\n","Step 7701: Gradients computed successfully\n","Step 7702: Loss: 0.3139, requires_grad: True\n","Step 7702: Gradients computed successfully\n","Step 7703: Loss: 0.1388, requires_grad: True\n","Step 7703: Gradients computed successfully\n","Step 7704: Loss: 0.1513, requires_grad: True\n","Step 7704: Gradients computed successfully\n","Step 7705: Loss: 0.3626, requires_grad: True\n","Step 7705: Gradients computed successfully\n","Step 7706: Loss: 0.1124, requires_grad: True\n","Step 7706: Gradients computed successfully\n","Step 7707: Loss: 0.1651, requires_grad: True\n","Step 7707: Gradients computed successfully\n","Step 7708: Loss: 0.1736, requires_grad: True\n","Step 7708: Gradients computed successfully\n","Step 7709: Loss: 0.2862, requires_grad: True\n","Step 7709: Gradients computed successfully\n","Step 7710: Loss: 0.1035, requires_grad: True\n","Step 7710: Gradients computed successfully\n","Epoch 0, Step 7710, Loss 0.1035, Avg Loss: 0.2551\n","Step 7711: Loss: 0.0940, requires_grad: True\n","Step 7711: Gradients computed successfully\n","Step 7712: Loss: 0.4231, requires_grad: True\n","Step 7712: Gradients computed successfully\n","Step 7713: Loss: 0.0915, requires_grad: True\n","Step 7713: Gradients computed successfully\n","Step 7714: Loss: 0.1008, requires_grad: True\n","Step 7714: Gradients computed successfully\n","Step 7715: Loss: 0.1812, requires_grad: True\n","Step 7715: Gradients computed successfully\n","Step 7716: Loss: 0.3799, requires_grad: True\n","Step 7716: Gradients computed successfully\n","Step 7717: Loss: 0.0966, requires_grad: True\n","Step 7717: Gradients computed successfully\n","Step 7718: Loss: 0.2450, requires_grad: True\n","Step 7718: Gradients computed successfully\n","Step 7719: Loss: 0.0901, requires_grad: True\n","Step 7719: Gradients computed successfully\n","Step 7720: Loss: 0.1037, requires_grad: True\n","Step 7720: Gradients computed successfully\n","Epoch 0, Step 7720, Loss 0.1037, Avg Loss: 0.2550\n","Step 7721: Loss: 0.2823, requires_grad: True\n","Step 7721: Gradients computed successfully\n","Step 7722: Loss: 0.1924, requires_grad: True\n","Step 7722: Gradients computed successfully\n","Step 7723: Loss: 0.1496, requires_grad: True\n","Step 7723: Gradients computed successfully\n","Step 7724: Loss: 0.3210, requires_grad: True\n","Step 7724: Gradients computed successfully\n","Step 7725: Loss: 0.1563, requires_grad: True\n","Step 7725: Gradients computed successfully\n","Step 7726: Loss: 0.2315, requires_grad: True\n","Step 7726: Gradients computed successfully\n","Step 7727: Loss: 0.1107, requires_grad: True\n","Step 7727: Gradients computed successfully\n","Step 7728: Loss: 0.1353, requires_grad: True\n","Step 7728: Gradients computed successfully\n","Step 7729: Loss: 0.4484, requires_grad: True\n","Step 7729: Gradients computed successfully\n","Step 7730: Loss: 0.1076, requires_grad: True\n","Step 7730: Gradients computed successfully\n","Epoch 0, Step 7730, Loss 0.1076, Avg Loss: 0.2549\n","Step 7731: Loss: 0.5881, requires_grad: True\n","Step 7731: Gradients computed successfully\n","Step 7732: Loss: 0.2852, requires_grad: True\n","Step 7732: Gradients computed successfully\n","Step 7733: Loss: 0.3051, requires_grad: True\n","Step 7733: Gradients computed successfully\n","Step 7734: Loss: 0.1543, requires_grad: True\n","Step 7734: Gradients computed successfully\n","Step 7735: Loss: 0.3575, requires_grad: True\n","Step 7735: Gradients computed successfully\n","Step 7736: Loss: 0.1409, requires_grad: True\n","Step 7736: Gradients computed successfully\n","Step 7737: Loss: 0.2565, requires_grad: True\n","Step 7737: Gradients computed successfully\n","Step 7738: Loss: 0.2665, requires_grad: True\n","Step 7738: Gradients computed successfully\n","Step 7739: Loss: 0.1067, requires_grad: True\n","Step 7739: Gradients computed successfully\n","Step 7740: Loss: 0.1159, requires_grad: True\n","Step 7740: Gradients computed successfully\n","Epoch 0, Step 7740, Loss 0.1159, Avg Loss: 0.2549\n","Step 7741: Loss: 0.4134, requires_grad: True\n","Step 7741: Gradients computed successfully\n","Step 7742: Loss: 0.1768, requires_grad: True\n","Step 7742: Gradients computed successfully\n","Step 7743: Loss: 0.0913, requires_grad: True\n","Step 7743: Gradients computed successfully\n","Step 7744: Loss: 0.2171, requires_grad: True\n","Step 7744: Gradients computed successfully\n","Step 7745: Loss: 0.2950, requires_grad: True\n","Step 7745: Gradients computed successfully\n","Step 7746: Loss: 0.5087, requires_grad: True\n","Step 7746: Gradients computed successfully\n","Step 7747: Loss: 0.1513, requires_grad: True\n","Step 7747: Gradients computed successfully\n","Step 7748: Loss: 0.1437, requires_grad: True\n","Step 7748: Gradients computed successfully\n","Step 7749: Loss: 0.2738, requires_grad: True\n","Step 7749: Gradients computed successfully\n","Step 7750: Loss: 0.1733, requires_grad: True\n","Step 7750: Gradients computed successfully\n","Epoch 0, Step 7750, Loss 0.1733, Avg Loss: 0.2549\n","Step 7751: Loss: 0.1922, requires_grad: True\n","Step 7751: Gradients computed successfully\n","Step 7752: Loss: 0.1685, requires_grad: True\n","Step 7752: Gradients computed successfully\n","Step 7753: Loss: 0.1462, requires_grad: True\n","Step 7753: Gradients computed successfully\n","Step 7754: Loss: 0.5577, requires_grad: True\n","Step 7754: Gradients computed successfully\n","Step 7755: Loss: 0.1734, requires_grad: True\n","Step 7755: Gradients computed successfully\n","Step 7756: Loss: 0.2002, requires_grad: True\n","Step 7756: Gradients computed successfully\n","Step 7757: Loss: 0.2660, requires_grad: True\n","Step 7757: Gradients computed successfully\n","Step 7758: Loss: 0.3315, requires_grad: True\n","Step 7758: Gradients computed successfully\n","Step 7759: Loss: 0.2922, requires_grad: True\n","Step 7759: Gradients computed successfully\n","Step 7760: Loss: 0.3712, requires_grad: True\n","Step 7760: Gradients computed successfully\n","Epoch 0, Step 7760, Loss 0.3712, Avg Loss: 0.2549\n","Step 7761: Loss: 0.1545, requires_grad: True\n","Step 7761: Gradients computed successfully\n","Step 7762: Loss: 0.3722, requires_grad: True\n","Step 7762: Gradients computed successfully\n","Step 7763: Loss: 0.2435, requires_grad: True\n","Step 7763: Gradients computed successfully\n","Step 7764: Loss: 0.2020, requires_grad: True\n","Step 7764: Gradients computed successfully\n","Step 7765: Loss: 0.1694, requires_grad: True\n","Step 7765: Gradients computed successfully\n","Step 7766: Loss: 0.1063, requires_grad: True\n","Step 7766: Gradients computed successfully\n","Step 7767: Loss: 0.1674, requires_grad: True\n","Step 7767: Gradients computed successfully\n","Step 7768: Loss: 0.1649, requires_grad: True\n","Step 7768: Gradients computed successfully\n","Step 7769: Loss: 0.1336, requires_grad: True\n","Step 7769: Gradients computed successfully\n","Step 7770: Loss: 0.4716, requires_grad: True\n","Step 7770: Gradients computed successfully\n","Epoch 0, Step 7770, Loss 0.4716, Avg Loss: 0.2549\n","Step 7771: Loss: 0.2249, requires_grad: True\n","Step 7771: Gradients computed successfully\n","Step 7772: Loss: 0.1101, requires_grad: True\n","Step 7772: Gradients computed successfully\n","Step 7773: Loss: 0.3476, requires_grad: True\n","Step 7773: Gradients computed successfully\n","Step 7774: Loss: 0.1685, requires_grad: True\n","Step 7774: Gradients computed successfully\n","Step 7775: Loss: 0.2270, requires_grad: True\n","Step 7775: Gradients computed successfully\n","Step 7776: Loss: 0.3099, requires_grad: True\n","Step 7776: Gradients computed successfully\n","Step 7777: Loss: 0.2478, requires_grad: True\n","Step 7777: Gradients computed successfully\n","Step 7778: Loss: 0.4241, requires_grad: True\n","Step 7778: Gradients computed successfully\n","Step 7779: Loss: 0.2410, requires_grad: True\n","Step 7779: Gradients computed successfully\n","Step 7780: Loss: 0.1621, requires_grad: True\n","Step 7780: Gradients computed successfully\n","Epoch 0, Step 7780, Loss 0.1621, Avg Loss: 0.2549\n","Step 7781: Loss: 0.2433, requires_grad: True\n","Step 7781: Gradients computed successfully\n","Step 7782: Loss: 0.2281, requires_grad: True\n","Step 7782: Gradients computed successfully\n","Step 7783: Loss: 0.1629, requires_grad: True\n","Step 7783: Gradients computed successfully\n","Step 7784: Loss: 0.2806, requires_grad: True\n","Step 7784: Gradients computed successfully\n","Step 7785: Loss: 0.2965, requires_grad: True\n","Step 7785: Gradients computed successfully\n","Step 7786: Loss: 0.1847, requires_grad: True\n","Step 7786: Gradients computed successfully\n","Step 7787: Loss: 0.1256, requires_grad: True\n","Step 7787: Gradients computed successfully\n","Step 7788: Loss: 0.4314, requires_grad: True\n","Step 7788: Gradients computed successfully\n","Step 7789: Loss: 0.0985, requires_grad: True\n","Step 7789: Gradients computed successfully\n","Step 7790: Loss: 0.2342, requires_grad: True\n","Step 7790: Gradients computed successfully\n","Epoch 0, Step 7790, Loss 0.2342, Avg Loss: 0.2548\n","Step 7791: Loss: 0.2724, requires_grad: True\n","Step 7791: Gradients computed successfully\n","Step 7792: Loss: 0.1960, requires_grad: True\n","Step 7792: Gradients computed successfully\n","Step 7793: Loss: 0.3479, requires_grad: True\n","Step 7793: Gradients computed successfully\n","Step 7794: Loss: 0.1111, requires_grad: True\n","Step 7794: Gradients computed successfully\n","Step 7795: Loss: 0.3245, requires_grad: True\n","Step 7795: Gradients computed successfully\n","Step 7796: Loss: 0.1397, requires_grad: True\n","Step 7796: Gradients computed successfully\n","Step 7797: Loss: 0.1368, requires_grad: True\n","Step 7797: Gradients computed successfully\n","Step 7798: Loss: 0.2527, requires_grad: True\n","Step 7798: Gradients computed successfully\n","Step 7799: Loss: 0.1128, requires_grad: True\n","Step 7799: Gradients computed successfully\n","Step 7800: Loss: 0.1057, requires_grad: True\n","Step 7800: Gradients computed successfully\n","Epoch 0, Step 7800, Loss 0.1057, Avg Loss: 0.2548\n","Step 7801: Loss: 0.3019, requires_grad: True\n","Step 7801: Gradients computed successfully\n","Step 7802: Loss: 0.1181, requires_grad: True\n","Step 7802: Gradients computed successfully\n","Step 7803: Loss: 0.4483, requires_grad: True\n","Step 7803: Gradients computed successfully\n","Step 7804: Loss: 0.1805, requires_grad: True\n","Step 7804: Gradients computed successfully\n","Step 7805: Loss: 0.1097, requires_grad: True\n","Step 7805: Gradients computed successfully\n","Step 7806: Loss: 0.1130, requires_grad: True\n","Step 7806: Gradients computed successfully\n","Step 7807: Loss: 0.2356, requires_grad: True\n","Step 7807: Gradients computed successfully\n","Step 7808: Loss: 0.1656, requires_grad: True\n","Step 7808: Gradients computed successfully\n","Step 7809: Loss: 0.2103, requires_grad: True\n","Step 7809: Gradients computed successfully\n","Step 7810: Loss: 0.2347, requires_grad: True\n","Step 7810: Gradients computed successfully\n","Epoch 0, Step 7810, Loss 0.2347, Avg Loss: 0.2547\n","Step 7811: Loss: 0.1870, requires_grad: True\n","Step 7811: Gradients computed successfully\n","Step 7812: Loss: 0.0838, requires_grad: True\n","Step 7812: Gradients computed successfully\n","Step 7813: Loss: 0.2327, requires_grad: True\n","Step 7813: Gradients computed successfully\n","Step 7814: Loss: 0.1209, requires_grad: True\n","Step 7814: Gradients computed successfully\n","Step 7815: Loss: 0.2386, requires_grad: True\n","Step 7815: Gradients computed successfully\n","Step 7816: Loss: 0.1222, requires_grad: True\n","Step 7816: Gradients computed successfully\n","Step 7817: Loss: 0.1852, requires_grad: True\n","Step 7817: Gradients computed successfully\n","Step 7818: Loss: 0.2091, requires_grad: True\n","Step 7818: Gradients computed successfully\n","Step 7819: Loss: 0.2781, requires_grad: True\n","Step 7819: Gradients computed successfully\n","Step 7820: Loss: 0.2174, requires_grad: True\n","Step 7820: Gradients computed successfully\n","Epoch 0, Step 7820, Loss 0.2174, Avg Loss: 0.2546\n","Step 7821: Loss: 0.5818, requires_grad: True\n","Step 7821: Gradients computed successfully\n","Step 7822: Loss: 0.2635, requires_grad: True\n","Step 7822: Gradients computed successfully\n","Step 7823: Loss: 0.2000, requires_grad: True\n","Step 7823: Gradients computed successfully\n","Step 7824: Loss: 0.2138, requires_grad: True\n","Step 7824: Gradients computed successfully\n","Step 7825: Loss: 0.1192, requires_grad: True\n","Step 7825: Gradients computed successfully\n","Step 7826: Loss: 0.4107, requires_grad: True\n","Step 7826: Gradients computed successfully\n","Step 7827: Loss: 0.1696, requires_grad: True\n","Step 7827: Gradients computed successfully\n","Step 7828: Loss: 0.3178, requires_grad: True\n","Step 7828: Gradients computed successfully\n","Step 7829: Loss: 0.1673, requires_grad: True\n","Step 7829: Gradients computed successfully\n","Step 7830: Loss: 0.1621, requires_grad: True\n","Step 7830: Gradients computed successfully\n","Epoch 0, Step 7830, Loss 0.1621, Avg Loss: 0.2546\n","Step 7831: Loss: 0.1547, requires_grad: True\n","Step 7831: Gradients computed successfully\n","Step 7832: Loss: 0.3199, requires_grad: True\n","Step 7832: Gradients computed successfully\n","Step 7833: Loss: 0.2570, requires_grad: True\n","Step 7833: Gradients computed successfully\n","Step 7834: Loss: 0.3488, requires_grad: True\n","Step 7834: Gradients computed successfully\n","Step 7835: Loss: 0.2171, requires_grad: True\n","Step 7835: Gradients computed successfully\n","Step 7836: Loss: 0.1683, requires_grad: True\n","Step 7836: Gradients computed successfully\n","Step 7837: Loss: 0.3316, requires_grad: True\n","Step 7837: Gradients computed successfully\n","Step 7838: Loss: 0.3106, requires_grad: True\n","Step 7838: Gradients computed successfully\n","Step 7839: Loss: 0.1486, requires_grad: True\n","Step 7839: Gradients computed successfully\n","Step 7840: Loss: 0.1465, requires_grad: True\n","Step 7840: Gradients computed successfully\n","Epoch 0, Step 7840, Loss 0.1465, Avg Loss: 0.2546\n","Step 7841: Loss: 0.0993, requires_grad: True\n","Step 7841: Gradients computed successfully\n","Step 7842: Loss: 0.1418, requires_grad: True\n","Step 7842: Gradients computed successfully\n","Step 7843: Loss: 0.2997, requires_grad: True\n","Step 7843: Gradients computed successfully\n","Step 7844: Loss: 0.1696, requires_grad: True\n","Step 7844: Gradients computed successfully\n","Step 7845: Loss: 0.1953, requires_grad: True\n","Step 7845: Gradients computed successfully\n","Step 7846: Loss: 0.3449, requires_grad: True\n","Step 7846: Gradients computed successfully\n","Step 7847: Loss: 0.4375, requires_grad: True\n","Step 7847: Gradients computed successfully\n","Step 7848: Loss: 0.2695, requires_grad: True\n","Step 7848: Gradients computed successfully\n","Step 7849: Loss: 0.3771, requires_grad: True\n","Step 7849: Gradients computed successfully\n","Step 7850: Loss: 0.1235, requires_grad: True\n","Step 7850: Gradients computed successfully\n","Epoch 0, Step 7850, Loss 0.1235, Avg Loss: 0.2546\n","Step 7851: Loss: 0.1719, requires_grad: True\n","Step 7851: Gradients computed successfully\n","Step 7852: Loss: 0.1953, requires_grad: True\n","Step 7852: Gradients computed successfully\n","Step 7853: Loss: 0.1854, requires_grad: True\n","Step 7853: Gradients computed successfully\n","Step 7854: Loss: 0.1222, requires_grad: True\n","Step 7854: Gradients computed successfully\n","Step 7855: Loss: 0.5408, requires_grad: True\n","Step 7855: Gradients computed successfully\n","Step 7856: Loss: 0.3308, requires_grad: True\n","Step 7856: Gradients computed successfully\n","Step 7857: Loss: 0.2251, requires_grad: True\n","Step 7857: Gradients computed successfully\n","Step 7858: Loss: 0.3343, requires_grad: True\n","Step 7858: Gradients computed successfully\n","Step 7859: Loss: 0.6125, requires_grad: True\n","Step 7859: Gradients computed successfully\n","Step 7860: Loss: 0.3918, requires_grad: True\n","Step 7860: Gradients computed successfully\n","Epoch 0, Step 7860, Loss 0.3918, Avg Loss: 0.2547\n","Step 7861: Loss: 0.3460, requires_grad: True\n","Step 7861: Gradients computed successfully\n","Step 7862: Loss: 0.2257, requires_grad: True\n","Step 7862: Gradients computed successfully\n","Step 7863: Loss: 0.1942, requires_grad: True\n","Step 7863: Gradients computed successfully\n","Step 7864: Loss: 0.2866, requires_grad: True\n","Step 7864: Gradients computed successfully\n","Step 7865: Loss: 0.0959, requires_grad: True\n","Step 7865: Gradients computed successfully\n","Step 7866: Loss: 0.1256, requires_grad: True\n","Step 7866: Gradients computed successfully\n","Step 7867: Loss: 0.1247, requires_grad: True\n","Step 7867: Gradients computed successfully\n","Step 7868: Loss: 0.1978, requires_grad: True\n","Step 7868: Gradients computed successfully\n","Step 7869: Loss: 0.4216, requires_grad: True\n","Step 7869: Gradients computed successfully\n","Step 7870: Loss: 0.2655, requires_grad: True\n","Step 7870: Gradients computed successfully\n","Epoch 0, Step 7870, Loss 0.2655, Avg Loss: 0.2546\n","Step 7871: Loss: 0.1814, requires_grad: True\n","Step 7871: Gradients computed successfully\n","Step 7872: Loss: 0.2327, requires_grad: True\n","Step 7872: Gradients computed successfully\n","Step 7873: Loss: 0.1281, requires_grad: True\n","Step 7873: Gradients computed successfully\n","Step 7874: Loss: 0.2379, requires_grad: True\n","Step 7874: Gradients computed successfully\n","Step 7875: Loss: 0.2566, requires_grad: True\n","Step 7875: Gradients computed successfully\n","Step 7876: Loss: 0.1249, requires_grad: True\n","Step 7876: Gradients computed successfully\n","Step 7877: Loss: 0.3517, requires_grad: True\n","Step 7877: Gradients computed successfully\n","Step 7878: Loss: 0.1894, requires_grad: True\n","Step 7878: Gradients computed successfully\n","Step 7879: Loss: 0.1575, requires_grad: True\n","Step 7879: Gradients computed successfully\n","Step 7880: Loss: 0.2354, requires_grad: True\n","Step 7880: Gradients computed successfully\n","Epoch 0, Step 7880, Loss 0.2354, Avg Loss: 0.2546\n","Step 7881: Loss: 0.1357, requires_grad: True\n","Step 7881: Gradients computed successfully\n","Step 7882: Loss: 0.1211, requires_grad: True\n","Step 7882: Gradients computed successfully\n","Step 7883: Loss: 0.3972, requires_grad: True\n","Step 7883: Gradients computed successfully\n","Step 7884: Loss: 0.1651, requires_grad: True\n","Step 7884: Gradients computed successfully\n","Step 7885: Loss: 0.1153, requires_grad: True\n","Step 7885: Gradients computed successfully\n","Step 7886: Loss: 0.1596, requires_grad: True\n","Step 7886: Gradients computed successfully\n","Step 7887: Loss: 0.1301, requires_grad: True\n","Step 7887: Gradients computed successfully\n","Step 7888: Loss: 0.1601, requires_grad: True\n","Step 7888: Gradients computed successfully\n","Step 7889: Loss: 0.2461, requires_grad: True\n","Step 7889: Gradients computed successfully\n","Step 7890: Loss: 0.1509, requires_grad: True\n","Step 7890: Gradients computed successfully\n","Epoch 0, Step 7890, Loss 0.1509, Avg Loss: 0.2545\n","Step 7891: Loss: 0.3472, requires_grad: True\n","Step 7891: Gradients computed successfully\n","Step 7892: Loss: 0.2012, requires_grad: True\n","Step 7892: Gradients computed successfully\n","Step 7893: Loss: 0.3968, requires_grad: True\n","Step 7893: Gradients computed successfully\n","Step 7894: Loss: 0.4086, requires_grad: True\n","Step 7894: Gradients computed successfully\n","Step 7895: Loss: 0.2348, requires_grad: True\n","Step 7895: Gradients computed successfully\n","Step 7896: Loss: 0.3503, requires_grad: True\n","Step 7896: Gradients computed successfully\n","Step 7897: Loss: 0.2910, requires_grad: True\n","Step 7897: Gradients computed successfully\n","Step 7898: Loss: 0.1710, requires_grad: True\n","Step 7898: Gradients computed successfully\n","Step 7899: Loss: 0.1118, requires_grad: True\n","Step 7899: Gradients computed successfully\n","Step 7900: Loss: 0.1870, requires_grad: True\n","Step 7900: Gradients computed successfully\n","Epoch 0, Step 7900, Loss 0.1870, Avg Loss: 0.2545\n","Step 7901: Loss: 0.1914, requires_grad: True\n","Step 7901: Gradients computed successfully\n","Step 7902: Loss: 0.1554, requires_grad: True\n","Step 7902: Gradients computed successfully\n","Step 7903: Loss: 0.1256, requires_grad: True\n","Step 7903: Gradients computed successfully\n","Step 7904: Loss: 0.4133, requires_grad: True\n","Step 7904: Gradients computed successfully\n","Step 7905: Loss: 0.2119, requires_grad: True\n","Step 7905: Gradients computed successfully\n","Step 7906: Loss: 0.1691, requires_grad: True\n","Step 7906: Gradients computed successfully\n","Step 7907: Loss: 0.1766, requires_grad: True\n","Step 7907: Gradients computed successfully\n","Step 7908: Loss: 0.1549, requires_grad: True\n","Step 7908: Gradients computed successfully\n","Step 7909: Loss: 0.1250, requires_grad: True\n","Step 7909: Gradients computed successfully\n","Step 7910: Loss: 0.3513, requires_grad: True\n","Step 7910: Gradients computed successfully\n","Epoch 0, Step 7910, Loss 0.3513, Avg Loss: 0.2544\n","Step 7911: Loss: 0.3069, requires_grad: True\n","Step 7911: Gradients computed successfully\n","Step 7912: Loss: 0.4002, requires_grad: True\n","Step 7912: Gradients computed successfully\n","Step 7913: Loss: 0.2022, requires_grad: True\n","Step 7913: Gradients computed successfully\n","Step 7914: Loss: 0.0739, requires_grad: True\n","Step 7914: Gradients computed successfully\n","Step 7915: Loss: 0.1934, requires_grad: True\n","Step 7915: Gradients computed successfully\n","Step 7916: Loss: 0.2450, requires_grad: True\n","Step 7916: Gradients computed successfully\n","Step 7917: Loss: 0.2915, requires_grad: True\n","Step 7917: Gradients computed successfully\n","Step 7918: Loss: 0.1885, requires_grad: True\n","Step 7918: Gradients computed successfully\n","Step 7919: Loss: 0.3805, requires_grad: True\n","Step 7919: Gradients computed successfully\n","Step 7920: Loss: 0.1750, requires_grad: True\n","Step 7920: Gradients computed successfully\n","Epoch 0, Step 7920, Loss 0.1750, Avg Loss: 0.2544\n","Step 7921: Loss: 0.1750, requires_grad: True\n","Step 7921: Gradients computed successfully\n","Step 7922: Loss: 0.2212, requires_grad: True\n","Step 7922: Gradients computed successfully\n","Step 7923: Loss: 0.2153, requires_grad: True\n","Step 7923: Gradients computed successfully\n","Step 7924: Loss: 0.4889, requires_grad: True\n","Step 7924: Gradients computed successfully\n","Step 7925: Loss: 0.1321, requires_grad: True\n","Step 7925: Gradients computed successfully\n","Step 7926: Loss: 0.3075, requires_grad: True\n","Step 7926: Gradients computed successfully\n","Step 7927: Loss: 0.2644, requires_grad: True\n","Step 7927: Gradients computed successfully\n","Step 7928: Loss: 0.2399, requires_grad: True\n","Step 7928: Gradients computed successfully\n","Step 7929: Loss: 0.3713, requires_grad: True\n","Step 7929: Gradients computed successfully\n","Step 7930: Loss: 0.1406, requires_grad: True\n","Step 7930: Gradients computed successfully\n","Epoch 0, Step 7930, Loss 0.1406, Avg Loss: 0.2544\n","Step 7931: Loss: 0.1849, requires_grad: True\n","Step 7931: Gradients computed successfully\n","Step 7932: Loss: 0.1596, requires_grad: True\n","Step 7932: Gradients computed successfully\n","Step 7933: Loss: 0.3626, requires_grad: True\n","Step 7933: Gradients computed successfully\n","Step 7934: Loss: 0.1452, requires_grad: True\n","Step 7934: Gradients computed successfully\n","Step 7935: Loss: 0.1716, requires_grad: True\n","Step 7935: Gradients computed successfully\n","Step 7936: Loss: 0.1307, requires_grad: True\n","Step 7936: Gradients computed successfully\n","Step 7937: Loss: 0.2681, requires_grad: True\n","Step 7937: Gradients computed successfully\n","Step 7938: Loss: 0.2286, requires_grad: True\n","Step 7938: Gradients computed successfully\n","Step 7939: Loss: 0.3356, requires_grad: True\n","Step 7939: Gradients computed successfully\n","Step 7940: Loss: 0.2696, requires_grad: True\n","Step 7940: Gradients computed successfully\n","Epoch 0, Step 7940, Loss 0.2696, Avg Loss: 0.2544\n","Step 7941: Loss: 0.2430, requires_grad: True\n","Step 7941: Gradients computed successfully\n","Step 7942: Loss: 0.1381, requires_grad: True\n","Step 7942: Gradients computed successfully\n","Step 7943: Loss: 0.3585, requires_grad: True\n","Step 7943: Gradients computed successfully\n","Step 7944: Loss: 0.1641, requires_grad: True\n","Step 7944: Gradients computed successfully\n","Step 7945: Loss: 0.2173, requires_grad: True\n","Step 7945: Gradients computed successfully\n","Step 7946: Loss: 0.1340, requires_grad: True\n","Step 7946: Gradients computed successfully\n","Step 7947: Loss: 0.1155, requires_grad: True\n","Step 7947: Gradients computed successfully\n","Step 7948: Loss: 0.5355, requires_grad: True\n","Step 7948: Gradients computed successfully\n","Step 7949: Loss: 0.1341, requires_grad: True\n","Step 7949: Gradients computed successfully\n","Step 7950: Loss: 0.4655, requires_grad: True\n","Step 7950: Gradients computed successfully\n","Epoch 0, Step 7950, Loss 0.4655, Avg Loss: 0.2544\n","Step 7951: Loss: 0.1663, requires_grad: True\n","Step 7951: Gradients computed successfully\n","Step 7952: Loss: 0.2140, requires_grad: True\n","Step 7952: Gradients computed successfully\n","Step 7953: Loss: 0.2038, requires_grad: True\n","Step 7953: Gradients computed successfully\n","Step 7954: Loss: 0.3332, requires_grad: True\n","Step 7954: Gradients computed successfully\n","Step 7955: Loss: 0.3336, requires_grad: True\n","Step 7955: Gradients computed successfully\n","Step 7956: Loss: 0.2889, requires_grad: True\n","Step 7956: Gradients computed successfully\n","Step 7957: Loss: 0.1843, requires_grad: True\n","Step 7957: Gradients computed successfully\n","Step 7958: Loss: 0.2766, requires_grad: True\n","Step 7958: Gradients computed successfully\n","Step 7959: Loss: 0.2558, requires_grad: True\n","Step 7959: Gradients computed successfully\n","Step 7960: Loss: 0.1949, requires_grad: True\n","Step 7960: Gradients computed successfully\n","Epoch 0, Step 7960, Loss 0.1949, Avg Loss: 0.2544\n","Step 7961: Loss: 0.1156, requires_grad: True\n","Step 7961: Gradients computed successfully\n","Step 7962: Loss: 0.2109, requires_grad: True\n","Step 7962: Gradients computed successfully\n","Step 7963: Loss: 0.1245, requires_grad: True\n","Step 7963: Gradients computed successfully\n","Step 7964: Loss: 0.2488, requires_grad: True\n","Step 7964: Gradients computed successfully\n","Step 7965: Loss: 0.2063, requires_grad: True\n","Step 7965: Gradients computed successfully\n","Step 7966: Loss: 0.2819, requires_grad: True\n","Step 7966: Gradients computed successfully\n","Step 7967: Loss: 0.2001, requires_grad: True\n","Step 7967: Gradients computed successfully\n","Step 7968: Loss: 0.2657, requires_grad: True\n","Step 7968: Gradients computed successfully\n","Step 7969: Loss: 0.1421, requires_grad: True\n","Step 7969: Gradients computed successfully\n","Step 7970: Loss: 0.1450, requires_grad: True\n","Step 7970: Gradients computed successfully\n","Epoch 0, Step 7970, Loss 0.1450, Avg Loss: 0.2543\n","Step 7971: Loss: 0.4447, requires_grad: True\n","Step 7971: Gradients computed successfully\n","Step 7972: Loss: 0.1709, requires_grad: True\n","Step 7972: Gradients computed successfully\n","Step 7973: Loss: 0.1925, requires_grad: True\n","Step 7973: Gradients computed successfully\n","Step 7974: Loss: 0.1209, requires_grad: True\n","Step 7974: Gradients computed successfully\n","Step 7975: Loss: 0.1745, requires_grad: True\n","Step 7975: Gradients computed successfully\n","Step 7976: Loss: 0.3438, requires_grad: True\n","Step 7976: Gradients computed successfully\n","Step 7977: Loss: 0.2176, requires_grad: True\n","Step 7977: Gradients computed successfully\n","Step 7978: Loss: 0.2231, requires_grad: True\n","Step 7978: Gradients computed successfully\n","Step 7979: Loss: 0.1301, requires_grad: True\n","Step 7979: Gradients computed successfully\n","Step 7980: Loss: 0.1199, requires_grad: True\n","Step 7980: Gradients computed successfully\n","Epoch 0, Step 7980, Loss 0.1199, Avg Loss: 0.2542\n","Step 7981: Loss: 0.6598, requires_grad: True\n","Step 7981: Gradients computed successfully\n","Step 7982: Loss: 0.3304, requires_grad: True\n","Step 7982: Gradients computed successfully\n","Step 7983: Loss: 0.1147, requires_grad: True\n","Step 7983: Gradients computed successfully\n","Step 7984: Loss: 0.2063, requires_grad: True\n","Step 7984: Gradients computed successfully\n","Step 7985: Loss: 0.2387, requires_grad: True\n","Step 7985: Gradients computed successfully\n","Step 7986: Loss: 0.2489, requires_grad: True\n","Step 7986: Gradients computed successfully\n","Step 7987: Loss: 0.2812, requires_grad: True\n","Step 7987: Gradients computed successfully\n","Step 7988: Loss: 0.1795, requires_grad: True\n","Step 7988: Gradients computed successfully\n","Step 7989: Loss: 0.1847, requires_grad: True\n","Step 7989: Gradients computed successfully\n","Step 7990: Loss: 0.1457, requires_grad: True\n","Step 7990: Gradients computed successfully\n","Epoch 0, Step 7990, Loss 0.1457, Avg Loss: 0.2543\n","Step 7991: Loss: 0.3432, requires_grad: True\n","Step 7991: Gradients computed successfully\n","Step 7992: Loss: 0.2113, requires_grad: True\n","Step 7992: Gradients computed successfully\n","Step 7993: Loss: 0.2203, requires_grad: True\n","Step 7993: Gradients computed successfully\n","Step 7994: Loss: 0.2314, requires_grad: True\n","Step 7994: Gradients computed successfully\n","Step 7995: Loss: 0.1450, requires_grad: True\n","Step 7995: Gradients computed successfully\n","Step 7996: Loss: 0.1804, requires_grad: True\n","Step 7996: Gradients computed successfully\n","Step 7997: Loss: 0.2403, requires_grad: True\n","Step 7997: Gradients computed successfully\n","Step 7998: Loss: 0.1006, requires_grad: True\n","Step 7998: Gradients computed successfully\n","Step 7999: Loss: 0.2701, requires_grad: True\n","Step 7999: Gradients computed successfully\n","Step 8000: Loss: 0.1129, requires_grad: True\n","Step 8000: Gradients computed successfully\n","Epoch 0, Step 8000, Loss 0.1129, Avg Loss: 0.2542\n","Saved checkpoint to /content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_complex_sql_continued/manual-epoch-0-step-8000\n","Step 8001: Loss: 0.2463, requires_grad: True\n","Step 8001: Gradients computed successfully\n","Step 8002: Loss: 0.2341, requires_grad: True\n","Step 8002: Gradients computed successfully\n","Step 8003: Loss: 0.2509, requires_grad: True\n","Step 8003: Gradients computed successfully\n","Step 8004: Loss: 0.3266, requires_grad: True\n","Step 8004: Gradients computed successfully\n","Step 8005: Loss: 0.2599, requires_grad: True\n","Step 8005: Gradients computed successfully\n","Step 8006: Loss: 0.1859, requires_grad: True\n","Step 8006: Gradients computed successfully\n","Step 8007: Loss: 0.1533, requires_grad: True\n","Step 8007: Gradients computed successfully\n","Step 8008: Loss: 0.2347, requires_grad: True\n","Step 8008: Gradients computed successfully\n","Step 8009: Loss: 0.3612, requires_grad: True\n","Step 8009: Gradients computed successfully\n","Step 8010: Loss: 0.1422, requires_grad: True\n","Step 8010: Gradients computed successfully\n","Epoch 0, Step 8010, Loss 0.1422, Avg Loss: 0.2542\n","Step 8011: Loss: 0.4487, requires_grad: True\n","Step 8011: Gradients computed successfully\n","Step 8012: Loss: 0.2989, requires_grad: True\n","Step 8012: Gradients computed successfully\n","Step 8013: Loss: 0.2488, requires_grad: True\n","Step 8013: Gradients computed successfully\n","Step 8014: Loss: 0.1483, requires_grad: True\n","Step 8014: Gradients computed successfully\n","Step 8015: Loss: 0.0799, requires_grad: True\n","Step 8015: Gradients computed successfully\n","Step 8016: Loss: 0.1815, requires_grad: True\n","Step 8016: Gradients computed successfully\n","Step 8017: Loss: 0.2615, requires_grad: True\n","Step 8017: Gradients computed successfully\n","Step 8018: Loss: 0.1490, requires_grad: True\n","Step 8018: Gradients computed successfully\n","Step 8019: Loss: 0.2392, requires_grad: True\n","Step 8019: Gradients computed successfully\n","Step 8020: Loss: 0.2135, requires_grad: True\n","Step 8020: Gradients computed successfully\n","Epoch 0, Step 8020, Loss 0.2135, Avg Loss: 0.2541\n","Step 8021: Loss: 0.3081, requires_grad: True\n","Step 8021: Gradients computed successfully\n","Step 8022: Loss: 0.1877, requires_grad: True\n","Step 8022: Gradients computed successfully\n","Step 8023: Loss: 0.4333, requires_grad: True\n","Step 8023: Gradients computed successfully\n","Step 8024: Loss: 0.2250, requires_grad: True\n","Step 8024: Gradients computed successfully\n","Step 8025: Loss: 0.3476, requires_grad: True\n","Step 8025: Gradients computed successfully\n","Step 8026: Loss: 0.1455, requires_grad: True\n","Step 8026: Gradients computed successfully\n","Step 8027: Loss: 0.1274, requires_grad: True\n","Step 8027: Gradients computed successfully\n","Step 8028: Loss: 0.2107, requires_grad: True\n","Step 8028: Gradients computed successfully\n","Step 8029: Loss: 0.1345, requires_grad: True\n","Step 8029: Gradients computed successfully\n","Step 8030: Loss: 0.1656, requires_grad: True\n","Step 8030: Gradients computed successfully\n","Epoch 0, Step 8030, Loss 0.1656, Avg Loss: 0.2541\n","Step 8031: Loss: 0.4289, requires_grad: True\n","Step 8031: Gradients computed successfully\n","Step 8032: Loss: 0.2134, requires_grad: True\n","Step 8032: Gradients computed successfully\n","Step 8033: Loss: 0.2963, requires_grad: True\n","Step 8033: Gradients computed successfully\n","Step 8034: Loss: 0.2689, requires_grad: True\n","Step 8034: Gradients computed successfully\n","Step 8035: Loss: 0.1874, requires_grad: True\n","Step 8035: Gradients computed successfully\n","Step 8036: Loss: 0.0823, requires_grad: True\n","Step 8036: Gradients computed successfully\n","Step 8037: Loss: 0.2479, requires_grad: True\n","Step 8037: Gradients computed successfully\n","Step 8038: Loss: 0.1575, requires_grad: True\n","Step 8038: Gradients computed successfully\n","Step 8039: Loss: 0.3176, requires_grad: True\n","Step 8039: Gradients computed successfully\n","Step 8040: Loss: 0.1379, requires_grad: True\n","Step 8040: Gradients computed successfully\n","Epoch 0, Step 8040, Loss 0.1379, Avg Loss: 0.2541\n","Step 8041: Loss: 0.1743, requires_grad: True\n","Step 8041: Gradients computed successfully\n","Step 8042: Loss: 0.2271, requires_grad: True\n","Step 8042: Gradients computed successfully\n","Step 8043: Loss: 0.1269, requires_grad: True\n","Step 8043: Gradients computed successfully\n","Step 8044: Loss: 0.1712, requires_grad: True\n","Step 8044: Gradients computed successfully\n","Step 8045: Loss: 0.3086, requires_grad: True\n","Step 8045: Gradients computed successfully\n","Step 8046: Loss: 0.2705, requires_grad: True\n","Step 8046: Gradients computed successfully\n","Step 8047: Loss: 0.3288, requires_grad: True\n","Step 8047: Gradients computed successfully\n","Step 8048: Loss: 0.3314, requires_grad: True\n","Step 8048: Gradients computed successfully\n","Step 8049: Loss: 0.3066, requires_grad: True\n","Step 8049: Gradients computed successfully\n","Step 8050: Loss: 0.4214, requires_grad: True\n","Step 8050: Gradients computed successfully\n","Epoch 0, Step 8050, Loss 0.4214, Avg Loss: 0.2541\n","Step 8051: Loss: 0.2565, requires_grad: True\n","Step 8051: Gradients computed successfully\n","Step 8052: Loss: 0.4200, requires_grad: True\n","Step 8052: Gradients computed successfully\n","Step 8053: Loss: 0.3112, requires_grad: True\n","Step 8053: Gradients computed successfully\n","Step 8054: Loss: 0.2860, requires_grad: True\n","Step 8054: Gradients computed successfully\n","Step 8055: Loss: 0.1410, requires_grad: True\n","Step 8055: Gradients computed successfully\n","Step 8056: Loss: 0.4097, requires_grad: True\n","Step 8056: Gradients computed successfully\n","Step 8057: Loss: 0.2337, requires_grad: True\n","Step 8057: Gradients computed successfully\n","Step 8058: Loss: 0.2778, requires_grad: True\n","Step 8058: Gradients computed successfully\n","Step 8059: Loss: 0.2399, requires_grad: True\n","Step 8059: Gradients computed successfully\n","Step 8060: Loss: 0.1777, requires_grad: True\n","Step 8060: Gradients computed successfully\n","Epoch 0, Step 8060, Loss 0.1777, Avg Loss: 0.2541\n","Step 8061: Loss: 0.2467, requires_grad: True\n","Step 8061: Gradients computed successfully\n","Step 8062: Loss: 0.1426, requires_grad: True\n","Step 8062: Gradients computed successfully\n","Step 8063: Loss: 0.2962, requires_grad: True\n","Step 8063: Gradients computed successfully\n","Step 8064: Loss: 0.2882, requires_grad: True\n","Step 8064: Gradients computed successfully\n","Step 8065: Loss: 0.1010, requires_grad: True\n","Step 8065: Gradients computed successfully\n","Step 8066: Loss: 0.1695, requires_grad: True\n","Step 8066: Gradients computed successfully\n","Step 8067: Loss: 0.1474, requires_grad: True\n","Step 8067: Gradients computed successfully\n","Step 8068: Loss: 0.1372, requires_grad: True\n","Step 8068: Gradients computed successfully\n","Step 8069: Loss: 0.2676, requires_grad: True\n","Step 8069: Gradients computed successfully\n","Step 8070: Loss: 0.4784, requires_grad: True\n","Step 8070: Gradients computed successfully\n","Epoch 0, Step 8070, Loss 0.4784, Avg Loss: 0.2541\n","Step 8071: Loss: 0.1455, requires_grad: True\n","Step 8071: Gradients computed successfully\n","Step 8072: Loss: 0.3718, requires_grad: True\n","Step 8072: Gradients computed successfully\n","Step 8073: Loss: 0.5422, requires_grad: True\n","Step 8073: Gradients computed successfully\n","Step 8074: Loss: 0.2203, requires_grad: True\n","Step 8074: Gradients computed successfully\n","Step 8075: Loss: 0.2360, requires_grad: True\n","Step 8075: Gradients computed successfully\n","Step 8076: Loss: 0.1834, requires_grad: True\n","Step 8076: Gradients computed successfully\n","Step 8077: Loss: 0.1977, requires_grad: True\n","Step 8077: Gradients computed successfully\n","Step 8078: Loss: 0.1853, requires_grad: True\n","Step 8078: Gradients computed successfully\n","Step 8079: Loss: 0.2270, requires_grad: True\n","Step 8079: Gradients computed successfully\n","Step 8080: Loss: 0.2171, requires_grad: True\n","Step 8080: Gradients computed successfully\n","Epoch 0, Step 8080, Loss 0.2171, Avg Loss: 0.2541\n","Step 8081: Loss: 0.1945, requires_grad: True\n","Step 8081: Gradients computed successfully\n","Step 8082: Loss: 0.3720, requires_grad: True\n","Step 8082: Gradients computed successfully\n","Step 8083: Loss: 0.2374, requires_grad: True\n","Step 8083: Gradients computed successfully\n","Step 8084: Loss: 0.2085, requires_grad: True\n","Step 8084: Gradients computed successfully\n","Step 8085: Loss: 0.4391, requires_grad: True\n","Step 8085: Gradients computed successfully\n","Step 8086: Loss: 0.1679, requires_grad: True\n","Step 8086: Gradients computed successfully\n","Step 8087: Loss: 0.2178, requires_grad: True\n","Step 8087: Gradients computed successfully\n","Step 8088: Loss: 0.2882, requires_grad: True\n","Step 8088: Gradients computed successfully\n","Step 8089: Loss: 0.2685, requires_grad: True\n","Step 8089: Gradients computed successfully\n","Step 8090: Loss: 0.2757, requires_grad: True\n","Step 8090: Gradients computed successfully\n","Epoch 0, Step 8090, Loss 0.2757, Avg Loss: 0.2541\n","Step 8091: Loss: 0.4215, requires_grad: True\n","Step 8091: Gradients computed successfully\n","Step 8092: Loss: 0.4492, requires_grad: True\n","Step 8092: Gradients computed successfully\n","Step 8093: Loss: 0.1707, requires_grad: True\n","Step 8093: Gradients computed successfully\n","Step 8094: Loss: 0.2642, requires_grad: True\n","Step 8094: Gradients computed successfully\n","Step 8095: Loss: 0.3515, requires_grad: True\n","Step 8095: Gradients computed successfully\n","Step 8096: Loss: 0.6452, requires_grad: True\n","Step 8096: Gradients computed successfully\n","Step 8097: Loss: 0.4114, requires_grad: True\n","Step 8097: Gradients computed successfully\n","Step 8098: Loss: 0.1244, requires_grad: True\n","Step 8098: Gradients computed successfully\n","Step 8099: Loss: 0.1794, requires_grad: True\n","Step 8099: Gradients computed successfully\n","Step 8100: Loss: 0.1010, requires_grad: True\n","Step 8100: Gradients computed successfully\n","Epoch 0, Step 8100, Loss 0.1010, Avg Loss: 0.2542\n","Step 8101: Loss: 0.1274, requires_grad: True\n","Step 8101: Gradients computed successfully\n","Step 8102: Loss: 0.2994, requires_grad: True\n","Step 8102: Gradients computed successfully\n","Step 8103: Loss: 0.1945, requires_grad: True\n","Step 8103: Gradients computed successfully\n","Step 8104: Loss: 0.1368, requires_grad: True\n","Step 8104: Gradients computed successfully\n","Step 8105: Loss: 0.1455, requires_grad: True\n","Step 8105: Gradients computed successfully\n","Step 8106: Loss: 0.1949, requires_grad: True\n","Step 8106: Gradients computed successfully\n","Step 8107: Loss: 0.1828, requires_grad: True\n","Step 8107: Gradients computed successfully\n","Step 8108: Loss: 0.2272, requires_grad: True\n","Step 8108: Gradients computed successfully\n","Step 8109: Loss: 0.2242, requires_grad: True\n","Step 8109: Gradients computed successfully\n","Step 8110: Loss: 0.1545, requires_grad: True\n","Step 8110: Gradients computed successfully\n","Epoch 0, Step 8110, Loss 0.1545, Avg Loss: 0.2541\n","Step 8111: Loss: 0.1273, requires_grad: True\n","Step 8111: Gradients computed successfully\n","Step 8112: Loss: 0.2389, requires_grad: True\n","Step 8112: Gradients computed successfully\n","Step 8113: Loss: 0.1285, requires_grad: True\n","Step 8113: Gradients computed successfully\n","Step 8114: Loss: 0.1815, requires_grad: True\n","Step 8114: Gradients computed successfully\n","Step 8115: Loss: 0.2567, requires_grad: True\n","Step 8115: Gradients computed successfully\n","Step 8116: Loss: 0.2811, requires_grad: True\n","Step 8116: Gradients computed successfully\n","Step 8117: Loss: 0.2028, requires_grad: True\n","Step 8117: Gradients computed successfully\n","Step 8118: Loss: 0.3882, requires_grad: True\n","Step 8118: Gradients computed successfully\n","Step 8119: Loss: 0.0877, requires_grad: True\n","Step 8119: Gradients computed successfully\n","Step 8120: Loss: 0.2338, requires_grad: True\n","Step 8120: Gradients computed successfully\n","Epoch 0, Step 8120, Loss 0.2338, Avg Loss: 0.2540\n","Step 8121: Loss: 0.1443, requires_grad: True\n","Step 8121: Gradients computed successfully\n","Step 8122: Loss: 0.2385, requires_grad: True\n","Step 8122: Gradients computed successfully\n","Step 8123: Loss: 0.1229, requires_grad: True\n","Step 8123: Gradients computed successfully\n","Step 8124: Loss: 0.1521, requires_grad: True\n","Step 8124: Gradients computed successfully\n","Step 8125: Loss: 0.2410, requires_grad: True\n","Step 8125: Gradients computed successfully\n","Step 8126: Loss: 0.1931, requires_grad: True\n","Step 8126: Gradients computed successfully\n","Step 8127: Loss: 0.1468, requires_grad: True\n","Step 8127: Gradients computed successfully\n","Step 8128: Loss: 0.1495, requires_grad: True\n","Step 8128: Gradients computed successfully\n","Step 8129: Loss: 0.2626, requires_grad: True\n","Step 8129: Gradients computed successfully\n","Step 8130: Loss: 0.6880, requires_grad: True\n","Step 8130: Gradients computed successfully\n","Epoch 0, Step 8130, Loss 0.6880, Avg Loss: 0.2540\n","Step 8131: Loss: 0.3051, requires_grad: True\n","Step 8131: Gradients computed successfully\n","Step 8132: Loss: 0.2602, requires_grad: True\n","Step 8132: Gradients computed successfully\n","Step 8133: Loss: 0.2273, requires_grad: True\n","Step 8133: Gradients computed successfully\n","Step 8134: Loss: 0.0775, requires_grad: True\n","Step 8134: Gradients computed successfully\n","Step 8135: Loss: 0.1116, requires_grad: True\n","Step 8135: Gradients computed successfully\n","Step 8136: Loss: 0.0968, requires_grad: True\n","Step 8136: Gradients computed successfully\n","Step 8137: Loss: 0.2577, requires_grad: True\n","Step 8137: Gradients computed successfully\n","Step 8138: Loss: 0.1505, requires_grad: True\n","Step 8138: Gradients computed successfully\n","Step 8139: Loss: 0.0982, requires_grad: True\n","Step 8139: Gradients computed successfully\n","Step 8140: Loss: 0.2145, requires_grad: True\n","Step 8140: Gradients computed successfully\n","Epoch 0, Step 8140, Loss 0.2145, Avg Loss: 0.2539\n","Step 8141: Loss: 0.1580, requires_grad: True\n","Step 8141: Gradients computed successfully\n","Step 8142: Loss: 0.3521, requires_grad: True\n","Step 8142: Gradients computed successfully\n","Step 8143: Loss: 0.1984, requires_grad: True\n","Step 8143: Gradients computed successfully\n","Step 8144: Loss: 0.2294, requires_grad: True\n","Step 8144: Gradients computed successfully\n","Step 8145: Loss: 0.1150, requires_grad: True\n","Step 8145: Gradients computed successfully\n","Step 8146: Loss: 0.3105, requires_grad: True\n","Step 8146: Gradients computed successfully\n","Step 8147: Loss: 0.4604, requires_grad: True\n","Step 8147: Gradients computed successfully\n","Step 8148: Loss: 0.1794, requires_grad: True\n","Step 8148: Gradients computed successfully\n","Step 8149: Loss: 0.2846, requires_grad: True\n","Step 8149: Gradients computed successfully\n","Step 8150: Loss: 0.3064, requires_grad: True\n","Step 8150: Gradients computed successfully\n","Epoch 0, Step 8150, Loss 0.3064, Avg Loss: 0.2539\n","Step 8151: Loss: 0.3186, requires_grad: True\n","Step 8151: Gradients computed successfully\n","Step 8152: Loss: 0.3073, requires_grad: True\n","Step 8152: Gradients computed successfully\n","Step 8153: Loss: 0.1732, requires_grad: True\n","Step 8153: Gradients computed successfully\n","Step 8154: Loss: 0.5455, requires_grad: True\n","Step 8154: Gradients computed successfully\n","Step 8155: Loss: 0.3462, requires_grad: True\n","Step 8155: Gradients computed successfully\n","Step 8156: Loss: 0.0920, requires_grad: True\n","Step 8156: Gradients computed successfully\n","Step 8157: Loss: 0.3029, requires_grad: True\n","Step 8157: Gradients computed successfully\n","Step 8158: Loss: 0.1243, requires_grad: True\n","Step 8158: Gradients computed successfully\n","Step 8159: Loss: 0.2244, requires_grad: True\n","Step 8159: Gradients computed successfully\n","Step 8160: Loss: 0.3191, requires_grad: True\n","Step 8160: Gradients computed successfully\n","Epoch 0, Step 8160, Loss 0.3191, Avg Loss: 0.2540\n","Step 8161: Loss: 0.1283, requires_grad: True\n","Step 8161: Gradients computed successfully\n","Step 8162: Loss: 0.1821, requires_grad: True\n","Step 8162: Gradients computed successfully\n","Step 8163: Loss: 0.2330, requires_grad: True\n","Step 8163: Gradients computed successfully\n","Step 8164: Loss: 0.1436, requires_grad: True\n","Step 8164: Gradients computed successfully\n","Step 8165: Loss: 0.2512, requires_grad: True\n","Step 8165: Gradients computed successfully\n","Step 8166: Loss: 0.2211, requires_grad: True\n","Step 8166: Gradients computed successfully\n","Step 8167: Loss: 0.5173, requires_grad: True\n","Step 8167: Gradients computed successfully\n","Step 8168: Loss: 0.1840, requires_grad: True\n","Step 8168: Gradients computed successfully\n","Step 8169: Loss: 0.3757, requires_grad: True\n","Step 8169: Gradients computed successfully\n","Step 8170: Loss: 0.1848, requires_grad: True\n","Step 8170: Gradients computed successfully\n","Epoch 0, Step 8170, Loss 0.1848, Avg Loss: 0.2540\n","Step 8171: Loss: 0.1131, requires_grad: True\n","Step 8171: Gradients computed successfully\n","Step 8172: Loss: 0.1535, requires_grad: True\n","Step 8172: Gradients computed successfully\n","Step 8173: Loss: 0.1166, requires_grad: True\n","Step 8173: Gradients computed successfully\n","Step 8174: Loss: 0.2573, requires_grad: True\n","Step 8174: Gradients computed successfully\n","Step 8175: Loss: 0.2362, requires_grad: True\n","Step 8175: Gradients computed successfully\n","Step 8176: Loss: 0.3831, requires_grad: True\n","Step 8176: Gradients computed successfully\n","Step 8177: Loss: 0.1848, requires_grad: True\n","Step 8177: Gradients computed successfully\n","Step 8178: Loss: 0.1968, requires_grad: True\n","Step 8178: Gradients computed successfully\n","Step 8179: Loss: 0.2766, requires_grad: True\n","Step 8179: Gradients computed successfully\n","Step 8180: Loss: 0.1875, requires_grad: True\n","Step 8180: Gradients computed successfully\n","Epoch 0, Step 8180, Loss 0.1875, Avg Loss: 0.2539\n","Step 8181: Loss: 0.3295, requires_grad: True\n","Step 8181: Gradients computed successfully\n","Step 8182: Loss: 0.0981, requires_grad: True\n","Step 8182: Gradients computed successfully\n","Step 8183: Loss: 0.2528, requires_grad: True\n","Step 8183: Gradients computed successfully\n","Step 8184: Loss: 0.0879, requires_grad: True\n","Step 8184: Gradients computed successfully\n","Step 8185: Loss: 0.2007, requires_grad: True\n","Step 8185: Gradients computed successfully\n","Step 8186: Loss: 0.2495, requires_grad: True\n","Step 8186: Gradients computed successfully\n","Step 8187: Loss: 0.3968, requires_grad: True\n","Step 8187: Gradients computed successfully\n","Step 8188: Loss: 0.1235, requires_grad: True\n","Step 8188: Gradients computed successfully\n","Step 8189: Loss: 0.2383, requires_grad: True\n","Step 8189: Gradients computed successfully\n","Step 8190: Loss: 0.2284, requires_grad: True\n","Step 8190: Gradients computed successfully\n","Epoch 0, Step 8190, Loss 0.2284, Avg Loss: 0.2539\n","Step 8191: Loss: 0.2337, requires_grad: True\n","Step 8191: Gradients computed successfully\n","Step 8192: Loss: 0.3216, requires_grad: True\n","Step 8192: Gradients computed successfully\n","Step 8193: Loss: 0.4518, requires_grad: True\n","Step 8193: Gradients computed successfully\n","Step 8194: Loss: 0.1615, requires_grad: True\n","Step 8194: Gradients computed successfully\n","Step 8195: Loss: 0.4067, requires_grad: True\n","Step 8195: Gradients computed successfully\n","Step 8196: Loss: 0.1852, requires_grad: True\n","Step 8196: Gradients computed successfully\n","Step 8197: Loss: 0.1475, requires_grad: True\n","Step 8197: Gradients computed successfully\n","Step 8198: Loss: 0.1374, requires_grad: True\n","Step 8198: Gradients computed successfully\n","Step 8199: Loss: 0.1616, requires_grad: True\n","Step 8199: Gradients computed successfully\n","Step 8200: Loss: 0.2395, requires_grad: True\n","Step 8200: Gradients computed successfully\n","Epoch 0, Step 8200, Loss 0.2395, Avg Loss: 0.2538\n","Step 8201: Loss: 0.2271, requires_grad: True\n","Step 8201: Gradients computed successfully\n","Step 8202: Loss: 0.6475, requires_grad: True\n","Step 8202: Gradients computed successfully\n","Step 8203: Loss: 0.2989, requires_grad: True\n","Step 8203: Gradients computed successfully\n","Step 8204: Loss: 0.4517, requires_grad: True\n","Step 8204: Gradients computed successfully\n","Step 8205: Loss: 0.2994, requires_grad: True\n","Step 8205: Gradients computed successfully\n","Step 8206: Loss: 0.1535, requires_grad: True\n","Step 8206: Gradients computed successfully\n","Step 8207: Loss: 0.2630, requires_grad: True\n","Step 8207: Gradients computed successfully\n","Step 8208: Loss: 0.1918, requires_grad: True\n","Step 8208: Gradients computed successfully\n","Step 8209: Loss: 0.0951, requires_grad: True\n","Step 8209: Gradients computed successfully\n","Step 8210: Loss: 0.2081, requires_grad: True\n","Step 8210: Gradients computed successfully\n","Epoch 0, Step 8210, Loss 0.2081, Avg Loss: 0.2539\n","Step 8211: Loss: 0.1698, requires_grad: True\n","Step 8211: Gradients computed successfully\n","Step 8212: Loss: 0.1646, requires_grad: True\n","Step 8212: Gradients computed successfully\n","Step 8213: Loss: 0.2601, requires_grad: True\n","Step 8213: Gradients computed successfully\n","Step 8214: Loss: 0.1167, requires_grad: True\n","Step 8214: Gradients computed successfully\n","Step 8215: Loss: 0.2895, requires_grad: True\n","Step 8215: Gradients computed successfully\n","Step 8216: Loss: 0.2169, requires_grad: True\n","Step 8216: Gradients computed successfully\n","Step 8217: Loss: 0.1913, requires_grad: True\n","Step 8217: Gradients computed successfully\n","Step 8218: Loss: 0.1212, requires_grad: True\n","Step 8218: Gradients computed successfully\n","Step 8219: Loss: 0.1947, requires_grad: True\n","Step 8219: Gradients computed successfully\n","Step 8220: Loss: 0.4308, requires_grad: True\n","Step 8220: Gradients computed successfully\n","Epoch 0, Step 8220, Loss 0.4308, Avg Loss: 0.2538\n","Step 8221: Loss: 0.1979, requires_grad: True\n","Step 8221: Gradients computed successfully\n","Step 8222: Loss: 0.1669, requires_grad: True\n","Step 8222: Gradients computed successfully\n","Step 8223: Loss: 0.2340, requires_grad: True\n","Step 8223: Gradients computed successfully\n","Step 8224: Loss: 0.2208, requires_grad: True\n","Step 8224: Gradients computed successfully\n","Step 8225: Loss: 0.1570, requires_grad: True\n","Step 8225: Gradients computed successfully\n","Step 8226: Loss: 0.1471, requires_grad: True\n","Step 8226: Gradients computed successfully\n","Step 8227: Loss: 0.1679, requires_grad: True\n","Step 8227: Gradients computed successfully\n","Step 8228: Loss: 0.1515, requires_grad: True\n","Step 8228: Gradients computed successfully\n","Step 8229: Loss: 0.1850, requires_grad: True\n","Step 8229: Gradients computed successfully\n","Step 8230: Loss: 0.1120, requires_grad: True\n","Step 8230: Gradients computed successfully\n","Epoch 0, Step 8230, Loss 0.1120, Avg Loss: 0.2537\n","Step 8231: Loss: 0.3289, requires_grad: True\n","Step 8231: Gradients computed successfully\n","Step 8232: Loss: 0.2392, requires_grad: True\n","Step 8232: Gradients computed successfully\n","Step 8233: Loss: 0.1891, requires_grad: True\n","Step 8233: Gradients computed successfully\n","Step 8234: Loss: 0.1137, requires_grad: True\n","Step 8234: Gradients computed successfully\n","Step 8235: Loss: 0.1960, requires_grad: True\n","Step 8235: Gradients computed successfully\n","Step 8236: Loss: 0.2035, requires_grad: True\n","Step 8236: Gradients computed successfully\n","Step 8237: Loss: 0.2503, requires_grad: True\n","Step 8237: Gradients computed successfully\n","Step 8238: Loss: 0.0979, requires_grad: True\n","Step 8238: Gradients computed successfully\n","Step 8239: Loss: 0.1901, requires_grad: True\n","Step 8239: Gradients computed successfully\n","Step 8240: Loss: 0.4365, requires_grad: True\n","Step 8240: Gradients computed successfully\n","Epoch 0, Step 8240, Loss 0.4365, Avg Loss: 0.2537\n","Step 8241: Loss: 0.0971, requires_grad: True\n","Step 8241: Gradients computed successfully\n","Step 8242: Loss: 0.4930, requires_grad: True\n","Step 8242: Gradients computed successfully\n","Step 8243: Loss: 0.4780, requires_grad: True\n","Step 8243: Gradients computed successfully\n","Step 8244: Loss: 0.3934, requires_grad: True\n","Step 8244: Gradients computed successfully\n","Step 8245: Loss: 0.0917, requires_grad: True\n","Step 8245: Gradients computed successfully\n","Step 8246: Loss: 0.1925, requires_grad: True\n","Step 8246: Gradients computed successfully\n","Step 8247: Loss: 0.2550, requires_grad: True\n","Step 8247: Gradients computed successfully\n","Step 8248: Loss: 0.2725, requires_grad: True\n","Step 8248: Gradients computed successfully\n","Step 8249: Loss: 0.4029, requires_grad: True\n","Step 8249: Gradients computed successfully\n","Step 8250: Loss: 0.2511, requires_grad: True\n","Step 8250: Gradients computed successfully\n","Epoch 0, Step 8250, Loss 0.2511, Avg Loss: 0.2537\n","Step 8251: Loss: 0.2130, requires_grad: True\n","Step 8251: Gradients computed successfully\n","Step 8252: Loss: 0.1232, requires_grad: True\n","Step 8252: Gradients computed successfully\n","Step 8253: Loss: 0.1790, requires_grad: True\n","Step 8253: Gradients computed successfully\n","Step 8254: Loss: 0.1210, requires_grad: True\n","Step 8254: Gradients computed successfully\n","Step 8255: Loss: 0.1971, requires_grad: True\n","Step 8255: Gradients computed successfully\n","Step 8256: Loss: 0.3262, requires_grad: True\n","Step 8256: Gradients computed successfully\n","Step 8257: Loss: 0.1764, requires_grad: True\n","Step 8257: Gradients computed successfully\n","Step 8258: Loss: 0.3149, requires_grad: True\n","Step 8258: Gradients computed successfully\n","Step 8259: Loss: 0.1444, requires_grad: True\n","Step 8259: Gradients computed successfully\n","Step 8260: Loss: 0.4760, requires_grad: True\n","Step 8260: Gradients computed successfully\n","Epoch 0, Step 8260, Loss 0.4760, Avg Loss: 0.2537\n","Step 8261: Loss: 0.1173, requires_grad: True\n","Step 8261: Gradients computed successfully\n","Step 8262: Loss: 0.1969, requires_grad: True\n","Step 8262: Gradients computed successfully\n","Step 8263: Loss: 0.2204, requires_grad: True\n","Step 8263: Gradients computed successfully\n","Step 8264: Loss: 0.1300, requires_grad: True\n","Step 8264: Gradients computed successfully\n","Step 8265: Loss: 0.2111, requires_grad: True\n","Step 8265: Gradients computed successfully\n","Step 8266: Loss: 0.4115, requires_grad: True\n","Step 8266: Gradients computed successfully\n","Step 8267: Loss: 0.3699, requires_grad: True\n","Step 8267: Gradients computed successfully\n","Step 8268: Loss: 0.4300, requires_grad: True\n","Step 8268: Gradients computed successfully\n","Step 8269: Loss: 0.1993, requires_grad: True\n","Step 8269: Gradients computed successfully\n","Step 8270: Loss: 0.4315, requires_grad: True\n","Step 8270: Gradients computed successfully\n","Epoch 0, Step 8270, Loss 0.4315, Avg Loss: 0.2537\n","Step 8271: Loss: 0.1962, requires_grad: True\n","Step 8271: Gradients computed successfully\n","Step 8272: Loss: 0.2660, requires_grad: True\n","Step 8272: Gradients computed successfully\n","Step 8273: Loss: 0.2089, requires_grad: True\n","Step 8273: Gradients computed successfully\n","Step 8274: Loss: 0.2915, requires_grad: True\n","Step 8274: Gradients computed successfully\n","Step 8275: Loss: 0.3789, requires_grad: True\n","Step 8275: Gradients computed successfully\n","Step 8276: Loss: 0.1969, requires_grad: True\n","Step 8276: Gradients computed successfully\n","Step 8277: Loss: 0.3481, requires_grad: True\n","Step 8277: Gradients computed successfully\n","Step 8278: Loss: 0.0974, requires_grad: True\n","Step 8278: Gradients computed successfully\n","Step 8279: Loss: 0.2126, requires_grad: True\n","Step 8279: Gradients computed successfully\n","Step 8280: Loss: 0.1556, requires_grad: True\n","Step 8280: Gradients computed successfully\n","Epoch 0, Step 8280, Loss 0.1556, Avg Loss: 0.2537\n","Step 8281: Loss: 0.1116, requires_grad: True\n","Step 8281: Gradients computed successfully\n","Step 8282: Loss: 0.2000, requires_grad: True\n","Step 8282: Gradients computed successfully\n","Step 8283: Loss: 0.4052, requires_grad: True\n","Step 8283: Gradients computed successfully\n","Step 8284: Loss: 0.2090, requires_grad: True\n","Step 8284: Gradients computed successfully\n","Step 8285: Loss: 0.0954, requires_grad: True\n","Step 8285: Gradients computed successfully\n","Step 8286: Loss: 0.1471, requires_grad: True\n","Step 8286: Gradients computed successfully\n","Step 8287: Loss: 0.2306, requires_grad: True\n","Step 8287: Gradients computed successfully\n","Step 8288: Loss: 0.2042, requires_grad: True\n","Step 8288: Gradients computed successfully\n","Step 8289: Loss: 0.2535, requires_grad: True\n","Step 8289: Gradients computed successfully\n","Step 8290: Loss: 0.1842, requires_grad: True\n","Step 8290: Gradients computed successfully\n","Epoch 0, Step 8290, Loss 0.1842, Avg Loss: 0.2537\n","Step 8291: Loss: 0.1744, requires_grad: True\n","Step 8291: Gradients computed successfully\n","Step 8292: Loss: 0.1134, requires_grad: True\n","Step 8292: Gradients computed successfully\n","Step 8293: Loss: 0.2845, requires_grad: True\n","Step 8293: Gradients computed successfully\n","Step 8294: Loss: 0.3120, requires_grad: True\n","Step 8294: Gradients computed successfully\n","Step 8295: Loss: 0.4641, requires_grad: True\n","Step 8295: Gradients computed successfully\n","Step 8296: Loss: 0.2676, requires_grad: True\n","Step 8296: Gradients computed successfully\n","Step 8297: Loss: 0.2901, requires_grad: True\n","Step 8297: Gradients computed successfully\n","Step 8298: Loss: 0.1462, requires_grad: True\n","Step 8298: Gradients computed successfully\n","Step 8299: Loss: 0.1744, requires_grad: True\n","Step 8299: Gradients computed successfully\n","Step 8300: Loss: 0.5864, requires_grad: True\n","Step 8300: Gradients computed successfully\n","Epoch 0, Step 8300, Loss 0.5864, Avg Loss: 0.2537\n","Step 8301: Loss: 0.1776, requires_grad: True\n","Step 8301: Gradients computed successfully\n","Step 8302: Loss: 0.2386, requires_grad: True\n","Step 8302: Gradients computed successfully\n","Step 8303: Loss: 0.2401, requires_grad: True\n","Step 8303: Gradients computed successfully\n","Step 8304: Loss: 0.1510, requires_grad: True\n","Step 8304: Gradients computed successfully\n","Step 8305: Loss: 0.0763, requires_grad: True\n","Step 8305: Gradients computed successfully\n","Step 8306: Loss: 0.2555, requires_grad: True\n","Step 8306: Gradients computed successfully\n","Step 8307: Loss: 0.1145, requires_grad: True\n","Step 8307: Gradients computed successfully\n","Step 8308: Loss: 0.2681, requires_grad: True\n","Step 8308: Gradients computed successfully\n","Step 8309: Loss: 0.2289, requires_grad: True\n","Step 8309: Gradients computed successfully\n","Step 8310: Loss: 0.2549, requires_grad: True\n","Step 8310: Gradients computed successfully\n","Epoch 0, Step 8310, Loss 0.2549, Avg Loss: 0.2536\n","Step 8311: Loss: 0.2147, requires_grad: True\n","Step 8311: Gradients computed successfully\n","Step 8312: Loss: 0.1392, requires_grad: True\n","Step 8312: Gradients computed successfully\n","Step 8313: Loss: 0.1974, requires_grad: True\n","Step 8313: Gradients computed successfully\n","Step 8314: Loss: 0.3066, requires_grad: True\n","Step 8314: Gradients computed successfully\n","Step 8315: Loss: 0.4167, requires_grad: True\n","Step 8315: Gradients computed successfully\n","Step 8316: Loss: 0.1361, requires_grad: True\n","Step 8316: Gradients computed successfully\n","Step 8317: Loss: 0.3575, requires_grad: True\n","Step 8317: Gradients computed successfully\n","Step 8318: Loss: 0.1563, requires_grad: True\n","Step 8318: Gradients computed successfully\n","Step 8319: Loss: 0.1662, requires_grad: True\n","Step 8319: Gradients computed successfully\n","Step 8320: Loss: 0.0772, requires_grad: True\n","Step 8320: Gradients computed successfully\n","Epoch 0, Step 8320, Loss 0.0772, Avg Loss: 0.2536\n","Step 8321: Loss: 0.2241, requires_grad: True\n","Step 8321: Gradients computed successfully\n","Step 8322: Loss: 0.5021, requires_grad: True\n","Step 8322: Gradients computed successfully\n","Step 8323: Loss: 0.3360, requires_grad: True\n","Step 8323: Gradients computed successfully\n","Step 8324: Loss: 0.4753, requires_grad: True\n","Step 8324: Gradients computed successfully\n","Step 8325: Loss: 0.2177, requires_grad: True\n","Step 8325: Gradients computed successfully\n","Step 8326: Loss: 0.2352, requires_grad: True\n","Step 8326: Gradients computed successfully\n","Step 8327: Loss: 0.3859, requires_grad: True\n","Step 8327: Gradients computed successfully\n","Step 8328: Loss: 0.1143, requires_grad: True\n","Step 8328: Gradients computed successfully\n","Step 8329: Loss: 0.2317, requires_grad: True\n","Step 8329: Gradients computed successfully\n","Step 8330: Loss: 0.1027, requires_grad: True\n","Step 8330: Gradients computed successfully\n","Epoch 0, Step 8330, Loss 0.1027, Avg Loss: 0.2536\n","Step 8331: Loss: 0.1810, requires_grad: True\n","Step 8331: Gradients computed successfully\n","Step 8332: Loss: 0.2953, requires_grad: True\n","Step 8332: Gradients computed successfully\n","Step 8333: Loss: 0.1783, requires_grad: True\n","Step 8333: Gradients computed successfully\n","Step 8334: Loss: 0.3493, requires_grad: True\n","Step 8334: Gradients computed successfully\n","Step 8335: Loss: 0.4032, requires_grad: True\n","Step 8335: Gradients computed successfully\n","Step 8336: Loss: 0.3347, requires_grad: True\n","Step 8336: Gradients computed successfully\n","Step 8337: Loss: 0.1595, requires_grad: True\n","Step 8337: Gradients computed successfully\n","Step 8338: Loss: 0.1976, requires_grad: True\n","Step 8338: Gradients computed successfully\n","Step 8339: Loss: 0.7318, requires_grad: True\n","Step 8339: Gradients computed successfully\n","Step 8340: Loss: 0.2243, requires_grad: True\n","Step 8340: Gradients computed successfully\n","Epoch 0, Step 8340, Loss 0.2243, Avg Loss: 0.2537\n","Step 8341: Loss: 0.1712, requires_grad: True\n","Step 8341: Gradients computed successfully\n","Step 8342: Loss: 0.1597, requires_grad: True\n","Step 8342: Gradients computed successfully\n","Step 8343: Loss: 0.3840, requires_grad: True\n","Step 8343: Gradients computed successfully\n","Step 8344: Loss: 0.1793, requires_grad: True\n","Step 8344: Gradients computed successfully\n","Step 8345: Loss: 0.2569, requires_grad: True\n","Step 8345: Gradients computed successfully\n","Step 8346: Loss: 0.1412, requires_grad: True\n","Step 8346: Gradients computed successfully\n","Step 8347: Loss: 0.5318, requires_grad: True\n","Step 8347: Gradients computed successfully\n","Step 8348: Loss: 0.1874, requires_grad: True\n","Step 8348: Gradients computed successfully\n","Step 8349: Loss: 0.4436, requires_grad: True\n","Step 8349: Gradients computed successfully\n","Step 8350: Loss: 0.1346, requires_grad: True\n","Step 8350: Gradients computed successfully\n","Epoch 0, Step 8350, Loss 0.1346, Avg Loss: 0.2537\n","Step 8351: Loss: 0.1568, requires_grad: True\n","Step 8351: Gradients computed successfully\n","Step 8352: Loss: 0.3112, requires_grad: True\n","Step 8352: Gradients computed successfully\n","Step 8353: Loss: 0.5244, requires_grad: True\n","Step 8353: Gradients computed successfully\n","Step 8354: Loss: 0.3114, requires_grad: True\n","Step 8354: Gradients computed successfully\n","Step 8355: Loss: 0.2084, requires_grad: True\n","Step 8355: Gradients computed successfully\n","Step 8356: Loss: 0.2580, requires_grad: True\n","Step 8356: Gradients computed successfully\n","Step 8357: Loss: 0.3065, requires_grad: True\n","Step 8357: Gradients computed successfully\n","Step 8358: Loss: 0.2145, requires_grad: True\n","Step 8358: Gradients computed successfully\n","Step 8359: Loss: 0.2883, requires_grad: True\n","Step 8359: Gradients computed successfully\n","Step 8360: Loss: 0.1162, requires_grad: True\n","Step 8360: Gradients computed successfully\n","Epoch 0, Step 8360, Loss 0.1162, Avg Loss: 0.2537\n","Step 8361: Loss: 0.1801, requires_grad: True\n","Step 8361: Gradients computed successfully\n","Step 8362: Loss: 0.1204, requires_grad: True\n","Step 8362: Gradients computed successfully\n","Step 8363: Loss: 0.3013, requires_grad: True\n","Step 8363: Gradients computed successfully\n","Step 8364: Loss: 0.3787, requires_grad: True\n","Step 8364: Gradients computed successfully\n","Step 8365: Loss: 0.1631, requires_grad: True\n","Step 8365: Gradients computed successfully\n","Step 8366: Loss: 0.2273, requires_grad: True\n","Step 8366: Gradients computed successfully\n","Step 8367: Loss: 0.1934, requires_grad: True\n","Step 8367: Gradients computed successfully\n","Step 8368: Loss: 0.1872, requires_grad: True\n","Step 8368: Gradients computed successfully\n","Step 8369: Loss: 0.1271, requires_grad: True\n","Step 8369: Gradients computed successfully\n","Step 8370: Loss: 0.1891, requires_grad: True\n","Step 8370: Gradients computed successfully\n","Epoch 0, Step 8370, Loss 0.1891, Avg Loss: 0.2536\n","Step 8371: Loss: 0.1231, requires_grad: True\n","Step 8371: Gradients computed successfully\n","Step 8372: Loss: 0.2139, requires_grad: True\n","Step 8372: Gradients computed successfully\n","Step 8373: Loss: 0.1130, requires_grad: True\n","Step 8373: Gradients computed successfully\n","Step 8374: Loss: 0.3570, requires_grad: True\n","Step 8374: Gradients computed successfully\n","Step 8375: Loss: 0.2486, requires_grad: True\n","Step 8375: Gradients computed successfully\n","Step 8376: Loss: 0.2235, requires_grad: True\n","Step 8376: Gradients computed successfully\n","Step 8377: Loss: 0.1462, requires_grad: True\n","Step 8377: Gradients computed successfully\n","Step 8378: Loss: 0.1524, requires_grad: True\n","Step 8378: Gradients computed successfully\n","Step 8379: Loss: 0.1138, requires_grad: True\n","Step 8379: Gradients computed successfully\n","Step 8380: Loss: 0.2745, requires_grad: True\n","Step 8380: Gradients computed successfully\n","Epoch 0, Step 8380, Loss 0.2745, Avg Loss: 0.2536\n","Step 8381: Loss: 0.2392, requires_grad: True\n","Step 8381: Gradients computed successfully\n","Step 8382: Loss: 0.4401, requires_grad: True\n","Step 8382: Gradients computed successfully\n","Step 8383: Loss: 0.1772, requires_grad: True\n","Step 8383: Gradients computed successfully\n","Step 8384: Loss: 0.1680, requires_grad: True\n","Step 8384: Gradients computed successfully\n","Step 8385: Loss: 0.2750, requires_grad: True\n","Step 8385: Gradients computed successfully\n","Step 8386: Loss: 0.2371, requires_grad: True\n","Step 8386: Gradients computed successfully\n","Step 8387: Loss: 0.1292, requires_grad: True\n","Step 8387: Gradients computed successfully\n","Step 8388: Loss: 0.1807, requires_grad: True\n","Step 8388: Gradients computed successfully\n","Step 8389: Loss: 0.1497, requires_grad: True\n","Step 8389: Gradients computed successfully\n","Step 8390: Loss: 0.6240, requires_grad: True\n","Step 8390: Gradients computed successfully\n","Epoch 0, Step 8390, Loss 0.6240, Avg Loss: 0.2536\n","Step 8391: Loss: 0.3197, requires_grad: True\n","Step 8391: Gradients computed successfully\n","Step 8392: Loss: 0.3948, requires_grad: True\n","Step 8392: Gradients computed successfully\n","Step 8393: Loss: 0.3038, requires_grad: True\n","Step 8393: Gradients computed successfully\n","Step 8394: Loss: 0.2339, requires_grad: True\n","Step 8394: Gradients computed successfully\n","Step 8395: Loss: 0.0801, requires_grad: True\n","Step 8395: Gradients computed successfully\n","Step 8396: Loss: 0.1897, requires_grad: True\n","Step 8396: Gradients computed successfully\n","Step 8397: Loss: 0.3469, requires_grad: True\n","Step 8397: Gradients computed successfully\n","Step 8398: Loss: 0.4390, requires_grad: True\n","Step 8398: Gradients computed successfully\n","Step 8399: Loss: 0.2982, requires_grad: True\n","Step 8399: Gradients computed successfully\n","Step 8400: Loss: 0.2865, requires_grad: True\n","Step 8400: Gradients computed successfully\n","Epoch 0, Step 8400, Loss 0.2865, Avg Loss: 0.2536\n","Step 8401: Loss: 0.1875, requires_grad: True\n","Step 8401: Gradients computed successfully\n","Step 8402: Loss: 0.1071, requires_grad: True\n","Step 8402: Gradients computed successfully\n","Step 8403: Loss: 0.3954, requires_grad: True\n","Step 8403: Gradients computed successfully\n","Step 8404: Loss: 0.2188, requires_grad: True\n","Step 8404: Gradients computed successfully\n","Step 8405: Loss: 0.2513, requires_grad: True\n","Step 8405: Gradients computed successfully\n","Step 8406: Loss: 0.3636, requires_grad: True\n","Step 8406: Gradients computed successfully\n","Step 8407: Loss: 0.1443, requires_grad: True\n","Step 8407: Gradients computed successfully\n","Step 8408: Loss: 0.2410, requires_grad: True\n","Step 8408: Gradients computed successfully\n","Step 8409: Loss: 0.2504, requires_grad: True\n","Step 8409: Gradients computed successfully\n","Step 8410: Loss: 0.3681, requires_grad: True\n","Step 8410: Gradients computed successfully\n","Epoch 0, Step 8410, Loss 0.3681, Avg Loss: 0.2536\n","Step 8411: Loss: 0.1641, requires_grad: True\n","Step 8411: Gradients computed successfully\n","Step 8412: Loss: 0.1591, requires_grad: True\n","Step 8412: Gradients computed successfully\n","Step 8413: Loss: 0.5178, requires_grad: True\n","Step 8413: Gradients computed successfully\n","Step 8414: Loss: 0.2257, requires_grad: True\n","Step 8414: Gradients computed successfully\n","Step 8415: Loss: 0.2066, requires_grad: True\n","Step 8415: Gradients computed successfully\n","Step 8416: Loss: 0.2819, requires_grad: True\n","Step 8416: Gradients computed successfully\n","Step 8417: Loss: 0.1973, requires_grad: True\n","Step 8417: Gradients computed successfully\n","Step 8418: Loss: 0.0686, requires_grad: True\n","Step 8418: Gradients computed successfully\n","Step 8419: Loss: 0.1927, requires_grad: True\n","Step 8419: Gradients computed successfully\n","Step 8420: Loss: 0.3713, requires_grad: True\n","Step 8420: Gradients computed successfully\n","Epoch 0, Step 8420, Loss 0.3713, Avg Loss: 0.2536\n","Step 8421: Loss: 0.2539, requires_grad: True\n","Step 8421: Gradients computed successfully\n","Step 8422: Loss: 0.5481, requires_grad: True\n","Step 8422: Gradients computed successfully\n","Step 8423: Loss: 0.1380, requires_grad: True\n","Step 8423: Gradients computed successfully\n","Step 8424: Loss: 0.2000, requires_grad: True\n","Step 8424: Gradients computed successfully\n","Step 8425: Loss: 0.1575, requires_grad: True\n","Step 8425: Gradients computed successfully\n","Step 8426: Loss: 0.2738, requires_grad: True\n","Step 8426: Gradients computed successfully\n","Step 8427: Loss: 0.2985, requires_grad: True\n","Step 8427: Gradients computed successfully\n","Step 8428: Loss: 0.3979, requires_grad: True\n","Step 8428: Gradients computed successfully\n","Step 8429: Loss: 0.2881, requires_grad: True\n","Step 8429: Gradients computed successfully\n","Step 8430: Loss: 0.1480, requires_grad: True\n","Step 8430: Gradients computed successfully\n","Epoch 0, Step 8430, Loss 0.1480, Avg Loss: 0.2536\n","Step 8431: Loss: 0.1365, requires_grad: True\n","Step 8431: Gradients computed successfully\n","Step 8432: Loss: 0.1571, requires_grad: True\n","Step 8432: Gradients computed successfully\n","Step 8433: Loss: 0.1696, requires_grad: True\n","Step 8433: Gradients computed successfully\n","Step 8434: Loss: 0.2918, requires_grad: True\n","Step 8434: Gradients computed successfully\n","Step 8435: Loss: 0.7761, requires_grad: True\n","Step 8435: Gradients computed successfully\n","Step 8436: Loss: 0.3743, requires_grad: True\n","Step 8436: Gradients computed successfully\n","Step 8437: Loss: 0.3398, requires_grad: True\n","Step 8437: Gradients computed successfully\n","Step 8438: Loss: 0.3018, requires_grad: True\n","Step 8438: Gradients computed successfully\n","Step 8439: Loss: 0.1072, requires_grad: True\n","Step 8439: Gradients computed successfully\n","Step 8440: Loss: 0.1953, requires_grad: True\n","Step 8440: Gradients computed successfully\n","Epoch 0, Step 8440, Loss 0.1953, Avg Loss: 0.2537\n","Step 8441: Loss: 0.2182, requires_grad: True\n","Step 8441: Gradients computed successfully\n","Step 8442: Loss: 0.0961, requires_grad: True\n","Step 8442: Gradients computed successfully\n","Step 8443: Loss: 0.2520, requires_grad: True\n","Step 8443: Gradients computed successfully\n","Step 8444: Loss: 0.3299, requires_grad: True\n","Step 8444: Gradients computed successfully\n","Step 8445: Loss: 0.2025, requires_grad: True\n","Step 8445: Gradients computed successfully\n","Step 8446: Loss: 0.3061, requires_grad: True\n","Step 8446: Gradients computed successfully\n","Step 8447: Loss: 0.6454, requires_grad: True\n","Step 8447: Gradients computed successfully\n","Step 8448: Loss: 0.2414, requires_grad: True\n","Step 8448: Gradients computed successfully\n","Step 8449: Loss: 0.0841, requires_grad: True\n","Step 8449: Gradients computed successfully\n","Step 8450: Loss: 0.6983, requires_grad: True\n","Step 8450: Gradients computed successfully\n","Epoch 0, Step 8450, Loss 0.6983, Avg Loss: 0.2537\n","Step 8451: Loss: 0.1782, requires_grad: True\n","Step 8451: Gradients computed successfully\n","Step 8452: Loss: 0.1088, requires_grad: True\n","Step 8452: Gradients computed successfully\n","Step 8453: Loss: 0.2635, requires_grad: True\n","Step 8453: Gradients computed successfully\n","Step 8454: Loss: 0.1018, requires_grad: True\n","Step 8454: Gradients computed successfully\n","Step 8455: Loss: 0.1844, requires_grad: True\n","Step 8455: Gradients computed successfully\n","Step 8456: Loss: 0.3916, requires_grad: True\n","Step 8456: Gradients computed successfully\n","Step 8457: Loss: 0.1792, requires_grad: True\n","Step 8457: Gradients computed successfully\n","Step 8458: Loss: 0.1049, requires_grad: True\n","Step 8458: Gradients computed successfully\n","Step 8459: Loss: 0.2703, requires_grad: True\n","Step 8459: Gradients computed successfully\n","Step 8460: Loss: 0.0758, requires_grad: True\n","Step 8460: Gradients computed successfully\n","Epoch 0, Step 8460, Loss 0.0758, Avg Loss: 0.2537\n","Step 8461: Loss: 0.1922, requires_grad: True\n","Step 8461: Gradients computed successfully\n","Step 8462: Loss: 0.2619, requires_grad: True\n","Step 8462: Gradients computed successfully\n","Step 8463: Loss: 0.1183, requires_grad: True\n","Step 8463: Gradients computed successfully\n","Step 8464: Loss: 0.1653, requires_grad: True\n","Step 8464: Gradients computed successfully\n","Step 8465: Loss: 0.2317, requires_grad: True\n","Step 8465: Gradients computed successfully\n","Step 8466: Loss: 0.1585, requires_grad: True\n","Step 8466: Gradients computed successfully\n","Step 8467: Loss: 0.2566, requires_grad: True\n","Step 8467: Gradients computed successfully\n","Step 8468: Loss: 0.2223, requires_grad: True\n","Step 8468: Gradients computed successfully\n","Step 8469: Loss: 0.2156, requires_grad: True\n","Step 8469: Gradients computed successfully\n","Step 8470: Loss: 0.1440, requires_grad: True\n","Step 8470: Gradients computed successfully\n","Epoch 0, Step 8470, Loss 0.1440, Avg Loss: 0.2536\n","Step 8471: Loss: 0.1253, requires_grad: True\n","Step 8471: Gradients computed successfully\n","Step 8472: Loss: 0.1199, requires_grad: True\n","Step 8472: Gradients computed successfully\n","Step 8473: Loss: 0.1430, requires_grad: True\n","Step 8473: Gradients computed successfully\n","Step 8474: Loss: 0.2060, requires_grad: True\n","Step 8474: Gradients computed successfully\n","Step 8475: Loss: 0.2937, requires_grad: True\n","Step 8475: Gradients computed successfully\n","Step 8476: Loss: 0.6005, requires_grad: True\n","Step 8476: Gradients computed successfully\n","Step 8477: Loss: 0.4476, requires_grad: True\n","Step 8477: Gradients computed successfully\n","Step 8478: Loss: 0.3102, requires_grad: True\n","Step 8478: Gradients computed successfully\n","Step 8479: Loss: 0.2821, requires_grad: True\n","Step 8479: Gradients computed successfully\n","Step 8480: Loss: 0.0793, requires_grad: True\n","Step 8480: Gradients computed successfully\n","Epoch 0, Step 8480, Loss 0.0793, Avg Loss: 0.2536\n","Step 8481: Loss: 0.3029, requires_grad: True\n","Step 8481: Gradients computed successfully\n","Step 8482: Loss: 0.4037, requires_grad: True\n","Step 8482: Gradients computed successfully\n","Step 8483: Loss: 0.1949, requires_grad: True\n","Step 8483: Gradients computed successfully\n","Step 8484: Loss: 0.2703, requires_grad: True\n","Step 8484: Gradients computed successfully\n","Step 8485: Loss: 0.1666, requires_grad: True\n","Step 8485: Gradients computed successfully\n","Step 8486: Loss: 0.2022, requires_grad: True\n","Step 8486: Gradients computed successfully\n","Step 8487: Loss: 0.1910, requires_grad: True\n","Step 8487: Gradients computed successfully\n","Step 8488: Loss: 0.1471, requires_grad: True\n","Step 8488: Gradients computed successfully\n","Step 8489: Loss: 0.0572, requires_grad: True\n","Step 8489: Gradients computed successfully\n","Step 8490: Loss: 0.1909, requires_grad: True\n","Step 8490: Gradients computed successfully\n","Epoch 0, Step 8490, Loss 0.1909, Avg Loss: 0.2535\n","Step 8491: Loss: 0.5014, requires_grad: True\n","Step 8491: Gradients computed successfully\n","Step 8492: Loss: 0.1683, requires_grad: True\n","Step 8492: Gradients computed successfully\n","Step 8493: Loss: 0.1694, requires_grad: True\n","Step 8493: Gradients computed successfully\n","Step 8494: Loss: 0.2076, requires_grad: True\n","Step 8494: Gradients computed successfully\n","Step 8495: Loss: 0.1291, requires_grad: True\n","Step 8495: Gradients computed successfully\n","Step 8496: Loss: 0.2102, requires_grad: True\n","Step 8496: Gradients computed successfully\n","Step 8497: Loss: 0.1137, requires_grad: True\n","Step 8497: Gradients computed successfully\n","Step 8498: Loss: 0.0989, requires_grad: True\n","Step 8498: Gradients computed successfully\n","Step 8499: Loss: 0.2332, requires_grad: True\n","Step 8499: Gradients computed successfully\n","Step 8500: Loss: 0.0819, requires_grad: True\n","Step 8500: Gradients computed successfully\n","Epoch 0, Step 8500, Loss 0.0819, Avg Loss: 0.2535\n","Saved checkpoint to /content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_complex_sql_continued/manual-epoch-0-step-8500\n","Step 8501: Loss: 0.1293, requires_grad: True\n","Step 8501: Gradients computed successfully\n","Step 8502: Loss: 0.3941, requires_grad: True\n","Step 8502: Gradients computed successfully\n","Step 8503: Loss: 0.4535, requires_grad: True\n","Step 8503: Gradients computed successfully\n","Step 8504: Loss: 0.1622, requires_grad: True\n","Step 8504: Gradients computed successfully\n","Step 8505: Loss: 0.2738, requires_grad: True\n","Step 8505: Gradients computed successfully\n","Step 8506: Loss: 0.2090, requires_grad: True\n","Step 8506: Gradients computed successfully\n","Step 8507: Loss: 0.2168, requires_grad: True\n","Step 8507: Gradients computed successfully\n","Step 8508: Loss: 0.2784, requires_grad: True\n","Step 8508: Gradients computed successfully\n","Step 8509: Loss: 0.1001, requires_grad: True\n","Step 8509: Gradients computed successfully\n","Step 8510: Loss: 0.3127, requires_grad: True\n","Step 8510: Gradients computed successfully\n","Epoch 0, Step 8510, Loss 0.3127, Avg Loss: 0.2535\n","Step 8511: Loss: 0.3243, requires_grad: True\n","Step 8511: Gradients computed successfully\n","Step 8512: Loss: 0.2138, requires_grad: True\n","Step 8512: Gradients computed successfully\n","Step 8513: Loss: 0.1359, requires_grad: True\n","Step 8513: Gradients computed successfully\n","Step 8514: Loss: 0.6283, requires_grad: True\n","Step 8514: Gradients computed successfully\n","Step 8515: Loss: 0.1847, requires_grad: True\n","Step 8515: Gradients computed successfully\n","Step 8516: Loss: 0.1383, requires_grad: True\n","Step 8516: Gradients computed successfully\n","Step 8517: Loss: 0.2165, requires_grad: True\n","Step 8517: Gradients computed successfully\n","Step 8518: Loss: 0.2847, requires_grad: True\n","Step 8518: Gradients computed successfully\n","Step 8519: Loss: 0.2977, requires_grad: True\n","Step 8519: Gradients computed successfully\n","Step 8520: Loss: 0.8195, requires_grad: True\n","Step 8520: Gradients computed successfully\n","Epoch 0, Step 8520, Loss 0.8195, Avg Loss: 0.2536\n","Step 8521: Loss: 0.3834, requires_grad: True\n","Step 8521: Gradients computed successfully\n","Step 8522: Loss: 0.1575, requires_grad: True\n","Step 8522: Gradients computed successfully\n","Step 8523: Loss: 0.1514, requires_grad: True\n","Step 8523: Gradients computed successfully\n","Step 8524: Loss: 0.0935, requires_grad: True\n","Step 8524: Gradients computed successfully\n","Step 8525: Loss: 0.3261, requires_grad: True\n","Step 8525: Gradients computed successfully\n","Step 8526: Loss: 0.1287, requires_grad: True\n","Step 8526: Gradients computed successfully\n","Step 8527: Loss: 0.3172, requires_grad: True\n","Step 8527: Gradients computed successfully\n","Step 8528: Loss: 0.1770, requires_grad: True\n","Step 8528: Gradients computed successfully\n","Step 8529: Loss: 0.0794, requires_grad: True\n","Step 8529: Gradients computed successfully\n","Step 8530: Loss: 0.2132, requires_grad: True\n","Step 8530: Gradients computed successfully\n","Epoch 0, Step 8530, Loss 0.2132, Avg Loss: 0.2535\n","Step 8531: Loss: 0.2232, requires_grad: True\n","Step 8531: Gradients computed successfully\n","Step 8532: Loss: 0.1305, requires_grad: True\n","Step 8532: Gradients computed successfully\n","Step 8533: Loss: 0.2594, requires_grad: True\n","Step 8533: Gradients computed successfully\n","Step 8534: Loss: 0.3085, requires_grad: True\n","Step 8534: Gradients computed successfully\n","Step 8535: Loss: 0.0835, requires_grad: True\n","Step 8535: Gradients computed successfully\n","Step 8536: Loss: 0.1994, requires_grad: True\n","Step 8536: Gradients computed successfully\n","Step 8537: Loss: 0.1366, requires_grad: True\n","Step 8537: Gradients computed successfully\n","Step 8538: Loss: 0.2133, requires_grad: True\n","Step 8538: Gradients computed successfully\n","Step 8539: Loss: 0.3129, requires_grad: True\n","Step 8539: Gradients computed successfully\n","Step 8540: Loss: 0.1648, requires_grad: True\n","Step 8540: Gradients computed successfully\n","Epoch 0, Step 8540, Loss 0.1648, Avg Loss: 0.2534\n","Step 8541: Loss: 0.2398, requires_grad: True\n","Step 8541: Gradients computed successfully\n","Step 8542: Loss: 0.3959, requires_grad: True\n","Step 8542: Gradients computed successfully\n","Step 8543: Loss: 0.2236, requires_grad: True\n","Step 8543: Gradients computed successfully\n","Step 8544: Loss: 0.2386, requires_grad: True\n","Step 8544: Gradients computed successfully\n","Step 8545: Loss: 0.1662, requires_grad: True\n","Step 8545: Gradients computed successfully\n","Step 8546: Loss: 0.1800, requires_grad: True\n","Step 8546: Gradients computed successfully\n","Step 8547: Loss: 0.3797, requires_grad: True\n","Step 8547: Gradients computed successfully\n","Step 8548: Loss: 0.2039, requires_grad: True\n","Step 8548: Gradients computed successfully\n","Step 8549: Loss: 0.2022, requires_grad: True\n","Step 8549: Gradients computed successfully\n","Step 8550: Loss: 0.2260, requires_grad: True\n","Step 8550: Gradients computed successfully\n","Epoch 0, Step 8550, Loss 0.2260, Avg Loss: 0.2534\n","Step 8551: Loss: 0.1532, requires_grad: True\n","Step 8551: Gradients computed successfully\n","Step 8552: Loss: 0.2717, requires_grad: True\n","Step 8552: Gradients computed successfully\n","Step 8553: Loss: 0.1136, requires_grad: True\n","Step 8553: Gradients computed successfully\n","Step 8554: Loss: 0.1154, requires_grad: True\n","Step 8554: Gradients computed successfully\n","Step 8555: Loss: 0.1743, requires_grad: True\n","Step 8555: Gradients computed successfully\n","Step 8556: Loss: 0.1933, requires_grad: True\n","Step 8556: Gradients computed successfully\n","Step 8557: Loss: 0.3322, requires_grad: True\n","Step 8557: Gradients computed successfully\n","Step 8558: Loss: 0.2706, requires_grad: True\n","Step 8558: Gradients computed successfully\n","Step 8559: Loss: 0.3021, requires_grad: True\n","Step 8559: Gradients computed successfully\n","Step 8560: Loss: 0.3095, requires_grad: True\n","Step 8560: Gradients computed successfully\n","Epoch 0, Step 8560, Loss 0.3095, Avg Loss: 0.2534\n","Step 8561: Loss: 0.1544, requires_grad: True\n","Step 8561: Gradients computed successfully\n","Step 8562: Loss: 0.1644, requires_grad: True\n","Step 8562: Gradients computed successfully\n","Step 8563: Loss: 0.6151, requires_grad: True\n","Step 8563: Gradients computed successfully\n","Step 8564: Loss: 0.1272, requires_grad: True\n","Step 8564: Gradients computed successfully\n","Step 8565: Loss: 0.3750, requires_grad: True\n","Step 8565: Gradients computed successfully\n","Step 8566: Loss: 0.2110, requires_grad: True\n","Step 8566: Gradients computed successfully\n","Step 8567: Loss: 0.1765, requires_grad: True\n","Step 8567: Gradients computed successfully\n","Step 8568: Loss: 0.2009, requires_grad: True\n","Step 8568: Gradients computed successfully\n","Step 8569: Loss: 0.1073, requires_grad: True\n","Step 8569: Gradients computed successfully\n","Step 8570: Loss: 0.1459, requires_grad: True\n","Step 8570: Gradients computed successfully\n","Epoch 0, Step 8570, Loss 0.1459, Avg Loss: 0.2534\n","Step 8571: Loss: 0.3031, requires_grad: True\n","Step 8571: Gradients computed successfully\n","Step 8572: Loss: 0.1417, requires_grad: True\n","Step 8572: Gradients computed successfully\n","Step 8573: Loss: 0.2963, requires_grad: True\n","Step 8573: Gradients computed successfully\n","Step 8574: Loss: 0.1591, requires_grad: True\n","Step 8574: Gradients computed successfully\n","Step 8575: Loss: 0.4004, requires_grad: True\n","Step 8575: Gradients computed successfully\n","Step 8576: Loss: 0.1611, requires_grad: True\n","Step 8576: Gradients computed successfully\n","Step 8577: Loss: 0.3622, requires_grad: True\n","Step 8577: Gradients computed successfully\n","Step 8578: Loss: 0.3864, requires_grad: True\n","Step 8578: Gradients computed successfully\n","Step 8579: Loss: 0.1760, requires_grad: True\n","Step 8579: Gradients computed successfully\n","Step 8580: Loss: 0.1345, requires_grad: True\n","Step 8580: Gradients computed successfully\n","Epoch 0, Step 8580, Loss 0.1345, Avg Loss: 0.2534\n","Step 8581: Loss: 0.3001, requires_grad: True\n","Step 8581: Gradients computed successfully\n","Step 8582: Loss: 0.3217, requires_grad: True\n","Step 8582: Gradients computed successfully\n","Step 8583: Loss: 0.2957, requires_grad: True\n","Step 8583: Gradients computed successfully\n","Step 8584: Loss: 0.1516, requires_grad: True\n","Step 8584: Gradients computed successfully\n","Step 8585: Loss: 0.2825, requires_grad: True\n","Step 8585: Gradients computed successfully\n","Step 8586: Loss: 0.1704, requires_grad: True\n","Step 8586: Gradients computed successfully\n","Step 8587: Loss: 0.1468, requires_grad: True\n","Step 8587: Gradients computed successfully\n","Step 8588: Loss: 0.2004, requires_grad: True\n","Step 8588: Gradients computed successfully\n","Step 8589: Loss: 0.2431, requires_grad: True\n","Step 8589: Gradients computed successfully\n","Step 8590: Loss: 0.0650, requires_grad: True\n","Step 8590: Gradients computed successfully\n","Epoch 0, Step 8590, Loss 0.0650, Avg Loss: 0.2533\n","Step 8591: Loss: 0.1548, requires_grad: True\n","Step 8591: Gradients computed successfully\n","Step 8592: Loss: 0.2272, requires_grad: True\n","Step 8592: Gradients computed successfully\n","Step 8593: Loss: 0.3416, requires_grad: True\n","Step 8593: Gradients computed successfully\n","Step 8594: Loss: 0.2063, requires_grad: True\n","Step 8594: Gradients computed successfully\n","Step 8595: Loss: 0.1187, requires_grad: True\n","Step 8595: Gradients computed successfully\n","Step 8596: Loss: 0.0823, requires_grad: True\n","Step 8596: Gradients computed successfully\n","Step 8597: Loss: 0.1492, requires_grad: True\n","Step 8597: Gradients computed successfully\n","Step 8598: Loss: 0.1972, requires_grad: True\n","Step 8598: Gradients computed successfully\n","Step 8599: Loss: 0.0949, requires_grad: True\n","Step 8599: Gradients computed successfully\n","Step 8600: Loss: 0.1980, requires_grad: True\n","Step 8600: Gradients computed successfully\n","Epoch 0, Step 8600, Loss 0.1980, Avg Loss: 0.2532\n","Step 8601: Loss: 0.2289, requires_grad: True\n","Step 8601: Gradients computed successfully\n","Step 8602: Loss: 0.2376, requires_grad: True\n","Step 8602: Gradients computed successfully\n","Step 8603: Loss: 0.3878, requires_grad: True\n","Step 8603: Gradients computed successfully\n","Step 8604: Loss: 0.3165, requires_grad: True\n","Step 8604: Gradients computed successfully\n","Step 8605: Loss: 0.2963, requires_grad: True\n","Step 8605: Gradients computed successfully\n","Step 8606: Loss: 0.2806, requires_grad: True\n","Step 8606: Gradients computed successfully\n","Step 8607: Loss: 0.1523, requires_grad: True\n","Step 8607: Gradients computed successfully\n","Step 8608: Loss: 0.1647, requires_grad: True\n","Step 8608: Gradients computed successfully\n","Step 8609: Loss: 0.1909, requires_grad: True\n","Step 8609: Gradients computed successfully\n","Step 8610: Loss: 0.2549, requires_grad: True\n","Step 8610: Gradients computed successfully\n","Epoch 0, Step 8610, Loss 0.2549, Avg Loss: 0.2532\n","Step 8611: Loss: 0.1530, requires_grad: True\n","Step 8611: Gradients computed successfully\n","Step 8612: Loss: 0.3550, requires_grad: True\n","Step 8612: Gradients computed successfully\n","Step 8613: Loss: 0.1293, requires_grad: True\n","Step 8613: Gradients computed successfully\n","Step 8614: Loss: 0.1813, requires_grad: True\n","Step 8614: Gradients computed successfully\n","Step 8615: Loss: 0.1265, requires_grad: True\n","Step 8615: Gradients computed successfully\n","Step 8616: Loss: 0.1667, requires_grad: True\n","Step 8616: Gradients computed successfully\n","Step 8617: Loss: 0.4881, requires_grad: True\n","Step 8617: Gradients computed successfully\n","Step 8618: Loss: 0.5935, requires_grad: True\n","Step 8618: Gradients computed successfully\n","Step 8619: Loss: 0.1602, requires_grad: True\n","Step 8619: Gradients computed successfully\n","Step 8620: Loss: 0.4973, requires_grad: True\n","Step 8620: Gradients computed successfully\n","Epoch 0, Step 8620, Loss 0.4973, Avg Loss: 0.2533\n","Step 8621: Loss: 0.4094, requires_grad: True\n","Step 8621: Gradients computed successfully\n","Step 8622: Loss: 0.1481, requires_grad: True\n","Step 8622: Gradients computed successfully\n","Step 8623: Loss: 0.1486, requires_grad: True\n","Step 8623: Gradients computed successfully\n","Step 8624: Loss: 0.1475, requires_grad: True\n","Step 8624: Gradients computed successfully\n","Step 8625: Loss: 0.1295, requires_grad: True\n","Step 8625: Gradients computed successfully\n","Step 8626: Loss: 0.1055, requires_grad: True\n","Step 8626: Gradients computed successfully\n","Step 8627: Loss: 0.2336, requires_grad: True\n","Step 8627: Gradients computed successfully\n","Step 8628: Loss: 0.2837, requires_grad: True\n","Step 8628: Gradients computed successfully\n","Step 8629: Loss: 0.2144, requires_grad: True\n","Step 8629: Gradients computed successfully\n","Step 8630: Loss: 0.4238, requires_grad: True\n","Step 8630: Gradients computed successfully\n","Epoch 0, Step 8630, Loss 0.4238, Avg Loss: 0.2532\n","Step 8631: Loss: 0.2904, requires_grad: True\n","Step 8631: Gradients computed successfully\n","Step 8632: Loss: 0.0710, requires_grad: True\n","Step 8632: Gradients computed successfully\n","Step 8633: Loss: 0.2523, requires_grad: True\n","Step 8633: Gradients computed successfully\n","Step 8634: Loss: 0.1735, requires_grad: True\n","Step 8634: Gradients computed successfully\n","Step 8635: Loss: 0.4706, requires_grad: True\n","Step 8635: Gradients computed successfully\n","Step 8636: Loss: 0.2489, requires_grad: True\n","Step 8636: Gradients computed successfully\n","Step 8637: Loss: 0.2061, requires_grad: True\n","Step 8637: Gradients computed successfully\n","Step 8638: Loss: 0.1365, requires_grad: True\n","Step 8638: Gradients computed successfully\n","Step 8639: Loss: 0.2860, requires_grad: True\n","Step 8639: Gradients computed successfully\n","Step 8640: Loss: 0.2011, requires_grad: True\n","Step 8640: Gradients computed successfully\n","Epoch 0, Step 8640, Loss 0.2011, Avg Loss: 0.2532\n","Step 8641: Loss: 0.1409, requires_grad: True\n","Step 8641: Gradients computed successfully\n","Step 8642: Loss: 0.4204, requires_grad: True\n","Step 8642: Gradients computed successfully\n","Step 8643: Loss: 0.2576, requires_grad: True\n","Step 8643: Gradients computed successfully\n","Step 8644: Loss: 0.1551, requires_grad: True\n","Step 8644: Gradients computed successfully\n","Step 8645: Loss: 0.1445, requires_grad: True\n","Step 8645: Gradients computed successfully\n","Step 8646: Loss: 0.1420, requires_grad: True\n","Step 8646: Gradients computed successfully\n","Step 8647: Loss: 0.1397, requires_grad: True\n","Step 8647: Gradients computed successfully\n","Step 8648: Loss: 0.1674, requires_grad: True\n","Step 8648: Gradients computed successfully\n","Step 8649: Loss: 0.4528, requires_grad: True\n","Step 8649: Gradients computed successfully\n","Step 8650: Loss: 0.2141, requires_grad: True\n","Step 8650: Gradients computed successfully\n","Epoch 0, Step 8650, Loss 0.2141, Avg Loss: 0.2532\n","Step 8651: Loss: 0.2183, requires_grad: True\n","Step 8651: Gradients computed successfully\n","Step 8652: Loss: 0.2144, requires_grad: True\n","Step 8652: Gradients computed successfully\n","Step 8653: Loss: 0.1723, requires_grad: True\n","Step 8653: Gradients computed successfully\n","Step 8654: Loss: 0.2741, requires_grad: True\n","Step 8654: Gradients computed successfully\n","Step 8655: Loss: 0.5379, requires_grad: True\n","Step 8655: Gradients computed successfully\n","Step 8656: Loss: 0.0788, requires_grad: True\n","Step 8656: Gradients computed successfully\n","Step 8657: Loss: 0.2769, requires_grad: True\n","Step 8657: Gradients computed successfully\n","Step 8658: Loss: 0.2609, requires_grad: True\n","Step 8658: Gradients computed successfully\n","Step 8659: Loss: 0.0974, requires_grad: True\n","Step 8659: Gradients computed successfully\n","Step 8660: Loss: 0.1345, requires_grad: True\n","Step 8660: Gradients computed successfully\n","Epoch 0, Step 8660, Loss 0.1345, Avg Loss: 0.2531\n","Step 8661: Loss: 0.2615, requires_grad: True\n","Step 8661: Gradients computed successfully\n","Step 8662: Loss: 0.2924, requires_grad: True\n","Step 8662: Gradients computed successfully\n","Step 8663: Loss: 0.3194, requires_grad: True\n","Step 8663: Gradients computed successfully\n","Step 8664: Loss: 0.2166, requires_grad: True\n","Step 8664: Gradients computed successfully\n","Step 8665: Loss: 0.1903, requires_grad: True\n","Step 8665: Gradients computed successfully\n","Step 8666: Loss: 0.1375, requires_grad: True\n","Step 8666: Gradients computed successfully\n","Step 8667: Loss: 0.1817, requires_grad: True\n","Step 8667: Gradients computed successfully\n","Step 8668: Loss: 0.1636, requires_grad: True\n","Step 8668: Gradients computed successfully\n","Step 8669: Loss: 0.0970, requires_grad: True\n","Step 8669: Gradients computed successfully\n","Step 8670: Loss: 0.2444, requires_grad: True\n","Step 8670: Gradients computed successfully\n","Epoch 0, Step 8670, Loss 0.2444, Avg Loss: 0.2531\n","Step 8671: Loss: 0.1674, requires_grad: True\n","Step 8671: Gradients computed successfully\n","Step 8672: Loss: 0.1869, requires_grad: True\n","Step 8672: Gradients computed successfully\n","Step 8673: Loss: 0.1400, requires_grad: True\n","Step 8673: Gradients computed successfully\n","Step 8674: Loss: 0.1288, requires_grad: True\n","Step 8674: Gradients computed successfully\n","Step 8675: Loss: 0.2161, requires_grad: True\n","Step 8675: Gradients computed successfully\n","Step 8676: Loss: 0.1071, requires_grad: True\n","Step 8676: Gradients computed successfully\n","Step 8677: Loss: 0.1050, requires_grad: True\n","Step 8677: Gradients computed successfully\n","Step 8678: Loss: 0.4371, requires_grad: True\n","Step 8678: Gradients computed successfully\n","Step 8679: Loss: 0.1457, requires_grad: True\n","Step 8679: Gradients computed successfully\n","Step 8680: Loss: 0.4099, requires_grad: True\n","Step 8680: Gradients computed successfully\n","Epoch 0, Step 8680, Loss 0.4099, Avg Loss: 0.2530\n","Step 8681: Loss: 0.1787, requires_grad: True\n","Step 8681: Gradients computed successfully\n","Step 8682: Loss: 0.0483, requires_grad: True\n","Step 8682: Gradients computed successfully\n","Step 8683: Loss: 0.2792, requires_grad: True\n","Step 8683: Gradients computed successfully\n","Step 8684: Loss: 0.2962, requires_grad: True\n","Step 8684: Gradients computed successfully\n","Step 8685: Loss: 0.4755, requires_grad: True\n","Step 8685: Gradients computed successfully\n","Step 8686: Loss: 0.3540, requires_grad: True\n","Step 8686: Gradients computed successfully\n","Step 8687: Loss: 0.1003, requires_grad: True\n","Step 8687: Gradients computed successfully\n","Step 8688: Loss: 0.3481, requires_grad: True\n","Step 8688: Gradients computed successfully\n","Step 8689: Loss: 0.3901, requires_grad: True\n","Step 8689: Gradients computed successfully\n","Step 8690: Loss: 0.2602, requires_grad: True\n","Step 8690: Gradients computed successfully\n","Epoch 0, Step 8690, Loss 0.2602, Avg Loss: 0.2531\n","Step 8691: Loss: 0.2892, requires_grad: True\n","Step 8691: Gradients computed successfully\n","Step 8692: Loss: 0.1228, requires_grad: True\n","Step 8692: Gradients computed successfully\n","Step 8693: Loss: 0.3127, requires_grad: True\n","Step 8693: Gradients computed successfully\n","Step 8694: Loss: 0.3146, requires_grad: True\n","Step 8694: Gradients computed successfully\n","Step 8695: Loss: 0.0717, requires_grad: True\n","Step 8695: Gradients computed successfully\n","Step 8696: Loss: 0.1155, requires_grad: True\n","Step 8696: Gradients computed successfully\n","Step 8697: Loss: 0.2834, requires_grad: True\n","Step 8697: Gradients computed successfully\n","Step 8698: Loss: 0.3087, requires_grad: True\n","Step 8698: Gradients computed successfully\n","Step 8699: Loss: 0.3042, requires_grad: True\n","Step 8699: Gradients computed successfully\n","Step 8700: Loss: 0.3140, requires_grad: True\n","Step 8700: Gradients computed successfully\n","Epoch 0, Step 8700, Loss 0.3140, Avg Loss: 0.2531\n","Step 8701: Loss: 0.1756, requires_grad: True\n","Step 8701: Gradients computed successfully\n","Step 8702: Loss: 0.2339, requires_grad: True\n","Step 8702: Gradients computed successfully\n","Step 8703: Loss: 0.5498, requires_grad: True\n","Step 8703: Gradients computed successfully\n","Step 8704: Loss: 0.0516, requires_grad: True\n","Step 8704: Gradients computed successfully\n","Step 8705: Loss: 0.1037, requires_grad: True\n","Step 8705: Gradients computed successfully\n","Step 8706: Loss: 0.3053, requires_grad: True\n","Step 8706: Gradients computed successfully\n","Step 8707: Loss: 0.5775, requires_grad: True\n","Step 8707: Gradients computed successfully\n","Step 8708: Loss: 0.2818, requires_grad: True\n","Step 8708: Gradients computed successfully\n","Step 8709: Loss: 0.1774, requires_grad: True\n","Step 8709: Gradients computed successfully\n","Step 8710: Loss: 0.2766, requires_grad: True\n","Step 8710: Gradients computed successfully\n","Epoch 0, Step 8710, Loss 0.2766, Avg Loss: 0.2531\n","Step 8711: Loss: 0.2858, requires_grad: True\n","Step 8711: Gradients computed successfully\n","Step 8712: Loss: 0.2396, requires_grad: True\n","Step 8712: Gradients computed successfully\n","Step 8713: Loss: 0.3224, requires_grad: True\n","Step 8713: Gradients computed successfully\n","Step 8714: Loss: 0.2162, requires_grad: True\n","Step 8714: Gradients computed successfully\n","Step 8715: Loss: 0.3555, requires_grad: True\n","Step 8715: Gradients computed successfully\n","Step 8716: Loss: 0.0827, requires_grad: True\n","Step 8716: Gradients computed successfully\n","Step 8717: Loss: 0.2774, requires_grad: True\n","Step 8717: Gradients computed successfully\n","Step 8718: Loss: 0.1292, requires_grad: True\n","Step 8718: Gradients computed successfully\n","Step 8719: Loss: 0.1774, requires_grad: True\n","Step 8719: Gradients computed successfully\n","Step 8720: Loss: 0.1701, requires_grad: True\n","Step 8720: Gradients computed successfully\n","Epoch 0, Step 8720, Loss 0.1701, Avg Loss: 0.2530\n","Step 8721: Loss: 0.1845, requires_grad: True\n","Step 8721: Gradients computed successfully\n","Step 8722: Loss: 0.1617, requires_grad: True\n","Step 8722: Gradients computed successfully\n","Step 8723: Loss: 0.2275, requires_grad: True\n","Step 8723: Gradients computed successfully\n","Step 8724: Loss: 0.2339, requires_grad: True\n","Step 8724: Gradients computed successfully\n","Step 8725: Loss: 0.4052, requires_grad: True\n","Step 8725: Gradients computed successfully\n","Step 8726: Loss: 0.1274, requires_grad: True\n","Step 8726: Gradients computed successfully\n","Step 8727: Loss: 0.1710, requires_grad: True\n","Step 8727: Gradients computed successfully\n","Step 8728: Loss: 0.3505, requires_grad: True\n","Step 8728: Gradients computed successfully\n","Step 8729: Loss: 0.1675, requires_grad: True\n","Step 8729: Gradients computed successfully\n","Step 8730: Loss: 0.1570, requires_grad: True\n","Step 8730: Gradients computed successfully\n","Epoch 0, Step 8730, Loss 0.1570, Avg Loss: 0.2530\n","Step 8731: Loss: 0.3004, requires_grad: True\n","Step 8731: Gradients computed successfully\n","Step 8732: Loss: 0.2482, requires_grad: True\n","Step 8732: Gradients computed successfully\n","Step 8733: Loss: 0.4107, requires_grad: True\n","Step 8733: Gradients computed successfully\n","Step 8734: Loss: 0.2998, requires_grad: True\n","Step 8734: Gradients computed successfully\n","Step 8735: Loss: 0.3544, requires_grad: True\n","Step 8735: Gradients computed successfully\n","Step 8736: Loss: 0.1690, requires_grad: True\n","Step 8736: Gradients computed successfully\n","Step 8737: Loss: 0.2191, requires_grad: True\n","Step 8737: Gradients computed successfully\n","Step 8738: Loss: 0.3372, requires_grad: True\n","Step 8738: Gradients computed successfully\n","Step 8739: Loss: 0.3988, requires_grad: True\n","Step 8739: Gradients computed successfully\n","Step 8740: Loss: 0.2574, requires_grad: True\n","Step 8740: Gradients computed successfully\n","Epoch 0, Step 8740, Loss 0.2574, Avg Loss: 0.2531\n","Step 8741: Loss: 0.2470, requires_grad: True\n","Step 8741: Gradients computed successfully\n","Step 8742: Loss: 0.2785, requires_grad: True\n","Step 8742: Gradients computed successfully\n","Step 8743: Loss: 0.2167, requires_grad: True\n","Step 8743: Gradients computed successfully\n","Step 8744: Loss: 0.2908, requires_grad: True\n","Step 8744: Gradients computed successfully\n","Step 8745: Loss: 0.1972, requires_grad: True\n","Step 8745: Gradients computed successfully\n","Step 8746: Loss: 0.1143, requires_grad: True\n","Step 8746: Gradients computed successfully\n","Step 8747: Loss: 0.2650, requires_grad: True\n","Step 8747: Gradients computed successfully\n","Step 8748: Loss: 0.2163, requires_grad: True\n","Step 8748: Gradients computed successfully\n","Step 8749: Loss: 0.2794, requires_grad: True\n","Step 8749: Gradients computed successfully\n","Step 8750: Loss: 0.4915, requires_grad: True\n","Step 8750: Gradients computed successfully\n","Epoch 0, Step 8750, Loss 0.4915, Avg Loss: 0.2531\n","Step 8751: Loss: 0.1177, requires_grad: True\n","Step 8751: Gradients computed successfully\n","Step 8752: Loss: 0.1447, requires_grad: True\n","Step 8752: Gradients computed successfully\n","Step 8753: Loss: 0.3682, requires_grad: True\n","Step 8753: Gradients computed successfully\n","Step 8754: Loss: 0.3088, requires_grad: True\n","Step 8754: Gradients computed successfully\n","Step 8755: Loss: 0.1935, requires_grad: True\n","Step 8755: Gradients computed successfully\n","Step 8756: Loss: 0.6259, requires_grad: True\n","Step 8756: Gradients computed successfully\n","Step 8757: Loss: 0.1321, requires_grad: True\n","Step 8757: Gradients computed successfully\n","Step 8758: Loss: 0.1619, requires_grad: True\n","Step 8758: Gradients computed successfully\n","Step 8759: Loss: 0.2757, requires_grad: True\n","Step 8759: Gradients computed successfully\n","Step 8760: Loss: 0.1298, requires_grad: True\n","Step 8760: Gradients computed successfully\n","Epoch 0, Step 8760, Loss 0.1298, Avg Loss: 0.2531\n","Step 8761: Loss: 0.0954, requires_grad: True\n","Step 8761: Gradients computed successfully\n","Step 8762: Loss: 0.4120, requires_grad: True\n","Step 8762: Gradients computed successfully\n","Step 8763: Loss: 0.3115, requires_grad: True\n","Step 8763: Gradients computed successfully\n","Step 8764: Loss: 0.1882, requires_grad: True\n","Step 8764: Gradients computed successfully\n","Step 8765: Loss: 0.1691, requires_grad: True\n","Step 8765: Gradients computed successfully\n","Step 8766: Loss: 0.2063, requires_grad: True\n","Step 8766: Gradients computed successfully\n","Step 8767: Loss: 0.1855, requires_grad: True\n","Step 8767: Gradients computed successfully\n","Step 8768: Loss: 0.1180, requires_grad: True\n","Step 8768: Gradients computed successfully\n","Step 8769: Loss: 0.1125, requires_grad: True\n","Step 8769: Gradients computed successfully\n","Step 8770: Loss: 0.2060, requires_grad: True\n","Step 8770: Gradients computed successfully\n","Epoch 0, Step 8770, Loss 0.2060, Avg Loss: 0.2530\n","Step 8771: Loss: 0.3259, requires_grad: True\n","Step 8771: Gradients computed successfully\n","Step 8772: Loss: 0.1379, requires_grad: True\n","Step 8772: Gradients computed successfully\n","Step 8773: Loss: 0.3015, requires_grad: True\n","Step 8773: Gradients computed successfully\n","Step 8774: Loss: 0.2116, requires_grad: True\n","Step 8774: Gradients computed successfully\n","Step 8775: Loss: 0.2142, requires_grad: True\n","Step 8775: Gradients computed successfully\n","Step 8776: Loss: 0.1744, requires_grad: True\n","Step 8776: Gradients computed successfully\n","Step 8777: Loss: 0.2034, requires_grad: True\n","Step 8777: Gradients computed successfully\n","Step 8778: Loss: 0.1734, requires_grad: True\n","Step 8778: Gradients computed successfully\n","Step 8779: Loss: 0.3592, requires_grad: True\n","Step 8779: Gradients computed successfully\n","Step 8780: Loss: 0.0754, requires_grad: True\n","Step 8780: Gradients computed successfully\n","Epoch 0, Step 8780, Loss 0.0754, Avg Loss: 0.2530\n","Step 8781: Loss: 0.3050, requires_grad: True\n","Step 8781: Gradients computed successfully\n","Step 8782: Loss: 0.0898, requires_grad: True\n","Step 8782: Gradients computed successfully\n","Step 8783: Loss: 0.3228, requires_grad: True\n","Step 8783: Gradients computed successfully\n","Step 8784: Loss: 0.1703, requires_grad: True\n","Step 8784: Gradients computed successfully\n","Step 8785: Loss: 0.1287, requires_grad: True\n","Step 8785: Gradients computed successfully\n","Step 8786: Loss: 0.0780, requires_grad: True\n","Step 8786: Gradients computed successfully\n","Step 8787: Loss: 0.1238, requires_grad: True\n","Step 8787: Gradients computed successfully\n","Step 8788: Loss: 0.1066, requires_grad: True\n","Step 8788: Gradients computed successfully\n","Step 8789: Loss: 0.2196, requires_grad: True\n","Step 8789: Gradients computed successfully\n","Step 8790: Loss: 0.2800, requires_grad: True\n","Step 8790: Gradients computed successfully\n","Epoch 0, Step 8790, Loss 0.2800, Avg Loss: 0.2529\n","Step 8791: Loss: 0.1125, requires_grad: True\n","Step 8791: Gradients computed successfully\n","Step 8792: Loss: 0.3421, requires_grad: True\n","Step 8792: Gradients computed successfully\n","Step 8793: Loss: 0.3199, requires_grad: True\n","Step 8793: Gradients computed successfully\n","Step 8794: Loss: 0.3067, requires_grad: True\n","Step 8794: Gradients computed successfully\n","Step 8795: Loss: 0.2435, requires_grad: True\n","Step 8795: Gradients computed successfully\n","Step 8796: Loss: 0.2435, requires_grad: True\n","Step 8796: Gradients computed successfully\n","Step 8797: Loss: 0.2596, requires_grad: True\n","Step 8797: Gradients computed successfully\n","Step 8798: Loss: 0.3993, requires_grad: True\n","Step 8798: Gradients computed successfully\n","Step 8799: Loss: 0.1273, requires_grad: True\n","Step 8799: Gradients computed successfully\n","Step 8800: Loss: 0.3139, requires_grad: True\n","Step 8800: Gradients computed successfully\n","Epoch 0, Step 8800, Loss 0.3139, Avg Loss: 0.2529\n","Step 8801: Loss: 0.2009, requires_grad: True\n","Step 8801: Gradients computed successfully\n","Step 8802: Loss: 0.1536, requires_grad: True\n","Step 8802: Gradients computed successfully\n","Step 8803: Loss: 0.3149, requires_grad: True\n","Step 8803: Gradients computed successfully\n","Step 8804: Loss: 0.1910, requires_grad: True\n","Step 8804: Gradients computed successfully\n","Step 8805: Loss: 0.3098, requires_grad: True\n","Step 8805: Gradients computed successfully\n","Step 8806: Loss: 0.1206, requires_grad: True\n","Step 8806: Gradients computed successfully\n","Step 8807: Loss: 0.3401, requires_grad: True\n","Step 8807: Gradients computed successfully\n","Step 8808: Loss: 0.2908, requires_grad: True\n","Step 8808: Gradients computed successfully\n","Step 8809: Loss: 0.1182, requires_grad: True\n","Step 8809: Gradients computed successfully\n","Step 8810: Loss: 0.1600, requires_grad: True\n","Step 8810: Gradients computed successfully\n","Epoch 0, Step 8810, Loss 0.1600, Avg Loss: 0.2529\n","Step 8811: Loss: 0.1787, requires_grad: True\n","Step 8811: Gradients computed successfully\n","Step 8812: Loss: 0.2075, requires_grad: True\n","Step 8812: Gradients computed successfully\n","Step 8813: Loss: 0.1707, requires_grad: True\n","Step 8813: Gradients computed successfully\n","Step 8814: Loss: 0.1746, requires_grad: True\n","Step 8814: Gradients computed successfully\n","Step 8815: Loss: 0.2826, requires_grad: True\n","Step 8815: Gradients computed successfully\n","Step 8816: Loss: 0.7304, requires_grad: True\n","Step 8816: Gradients computed successfully\n","Step 8817: Loss: 0.1362, requires_grad: True\n","Step 8817: Gradients computed successfully\n","Step 8818: Loss: 0.2096, requires_grad: True\n","Step 8818: Gradients computed successfully\n","Step 8819: Loss: 0.1697, requires_grad: True\n","Step 8819: Gradients computed successfully\n","Step 8820: Loss: 0.3675, requires_grad: True\n","Step 8820: Gradients computed successfully\n","Epoch 0, Step 8820, Loss 0.3675, Avg Loss: 0.2529\n","Step 8821: Loss: 0.2038, requires_grad: True\n","Step 8821: Gradients computed successfully\n","Step 8822: Loss: 0.1667, requires_grad: True\n","Step 8822: Gradients computed successfully\n","Step 8823: Loss: 0.1321, requires_grad: True\n","Step 8823: Gradients computed successfully\n","Step 8824: Loss: 0.1579, requires_grad: True\n","Step 8824: Gradients computed successfully\n","Step 8825: Loss: 0.0870, requires_grad: True\n","Step 8825: Gradients computed successfully\n","Step 8826: Loss: 0.2978, requires_grad: True\n","Step 8826: Gradients computed successfully\n","Step 8827: Loss: 0.2991, requires_grad: True\n","Step 8827: Gradients computed successfully\n","Step 8828: Loss: 0.4609, requires_grad: True\n","Step 8828: Gradients computed successfully\n","Step 8829: Loss: 0.2358, requires_grad: True\n","Step 8829: Gradients computed successfully\n","Step 8830: Loss: 0.2048, requires_grad: True\n","Step 8830: Gradients computed successfully\n","Epoch 0, Step 8830, Loss 0.2048, Avg Loss: 0.2528\n","Step 8831: Loss: 0.1729, requires_grad: True\n","Step 8831: Gradients computed successfully\n","Step 8832: Loss: 0.2728, requires_grad: True\n","Step 8832: Gradients computed successfully\n","Step 8833: Loss: 0.1262, requires_grad: True\n","Step 8833: Gradients computed successfully\n","Step 8834: Loss: 0.2390, requires_grad: True\n","Step 8834: Gradients computed successfully\n","Step 8835: Loss: 0.1449, requires_grad: True\n","Step 8835: Gradients computed successfully\n","Step 8836: Loss: 0.2001, requires_grad: True\n","Step 8836: Gradients computed successfully\n","Step 8837: Loss: 0.2106, requires_grad: True\n","Step 8837: Gradients computed successfully\n","Step 8838: Loss: 0.1727, requires_grad: True\n","Step 8838: Gradients computed successfully\n","Step 8839: Loss: 0.1383, requires_grad: True\n","Step 8839: Gradients computed successfully\n","Step 8840: Loss: 0.2364, requires_grad: True\n","Step 8840: Gradients computed successfully\n","Epoch 0, Step 8840, Loss 0.2364, Avg Loss: 0.2528\n","Step 8841: Loss: 0.3200, requires_grad: True\n","Step 8841: Gradients computed successfully\n","Step 8842: Loss: 0.2811, requires_grad: True\n","Step 8842: Gradients computed successfully\n","Step 8843: Loss: 0.2745, requires_grad: True\n","Step 8843: Gradients computed successfully\n","Step 8844: Loss: 0.1930, requires_grad: True\n","Step 8844: Gradients computed successfully\n","Step 8845: Loss: 0.2902, requires_grad: True\n","Step 8845: Gradients computed successfully\n","Step 8846: Loss: 0.2233, requires_grad: True\n","Step 8846: Gradients computed successfully\n","Step 8847: Loss: 0.2275, requires_grad: True\n","Step 8847: Gradients computed successfully\n","Step 8848: Loss: 0.1680, requires_grad: True\n","Step 8848: Gradients computed successfully\n","Step 8849: Loss: 0.1276, requires_grad: True\n","Step 8849: Gradients computed successfully\n","Step 8850: Loss: 0.1153, requires_grad: True\n","Step 8850: Gradients computed successfully\n","Epoch 0, Step 8850, Loss 0.1153, Avg Loss: 0.2527\n","Step 8851: Loss: 0.2534, requires_grad: True\n","Step 8851: Gradients computed successfully\n","Step 8852: Loss: 0.4711, requires_grad: True\n","Step 8852: Gradients computed successfully\n","Step 8853: Loss: 0.3671, requires_grad: True\n","Step 8853: Gradients computed successfully\n","Step 8854: Loss: 0.2823, requires_grad: True\n","Step 8854: Gradients computed successfully\n","Step 8855: Loss: 0.1189, requires_grad: True\n","Step 8855: Gradients computed successfully\n","Step 8856: Loss: 0.2012, requires_grad: True\n","Step 8856: Gradients computed successfully\n","Step 8857: Loss: 0.1670, requires_grad: True\n","Step 8857: Gradients computed successfully\n","Step 8858: Loss: 0.4038, requires_grad: True\n","Step 8858: Gradients computed successfully\n","Step 8859: Loss: 0.1754, requires_grad: True\n","Step 8859: Gradients computed successfully\n","Step 8860: Loss: 0.1696, requires_grad: True\n","Step 8860: Gradients computed successfully\n","Epoch 0, Step 8860, Loss 0.1696, Avg Loss: 0.2527\n","Step 8861: Loss: 0.1435, requires_grad: True\n","Step 8861: Gradients computed successfully\n","Step 8862: Loss: 0.2480, requires_grad: True\n","Step 8862: Gradients computed successfully\n","Step 8863: Loss: 0.1688, requires_grad: True\n","Step 8863: Gradients computed successfully\n","Step 8864: Loss: 0.1715, requires_grad: True\n","Step 8864: Gradients computed successfully\n","Step 8865: Loss: 0.2578, requires_grad: True\n","Step 8865: Gradients computed successfully\n","Step 8866: Loss: 0.0890, requires_grad: True\n","Step 8866: Gradients computed successfully\n","Step 8867: Loss: 0.4937, requires_grad: True\n","Step 8867: Gradients computed successfully\n","Step 8868: Loss: 0.2590, requires_grad: True\n","Step 8868: Gradients computed successfully\n","Step 8869: Loss: 0.0960, requires_grad: True\n","Step 8869: Gradients computed successfully\n","Step 8870: Loss: 0.1151, requires_grad: True\n","Step 8870: Gradients computed successfully\n","Epoch 0, Step 8870, Loss 0.1151, Avg Loss: 0.2527\n","Step 8871: Loss: 0.1660, requires_grad: True\n","Step 8871: Gradients computed successfully\n","Step 8872: Loss: 0.1508, requires_grad: True\n","Step 8872: Gradients computed successfully\n","Step 8873: Loss: 0.1477, requires_grad: True\n","Step 8873: Gradients computed successfully\n","Step 8874: Loss: 0.3052, requires_grad: True\n","Step 8874: Gradients computed successfully\n","Step 8875: Loss: 0.3292, requires_grad: True\n","Step 8875: Gradients computed successfully\n","Step 8876: Loss: 0.4350, requires_grad: True\n","Step 8876: Gradients computed successfully\n","Step 8877: Loss: 0.1905, requires_grad: True\n","Step 8877: Gradients computed successfully\n","Step 8878: Loss: 0.4438, requires_grad: True\n","Step 8878: Gradients computed successfully\n","Step 8879: Loss: 0.2438, requires_grad: True\n","Step 8879: Gradients computed successfully\n","Step 8880: Loss: 0.3212, requires_grad: True\n","Step 8880: Gradients computed successfully\n","Epoch 0, Step 8880, Loss 0.3212, Avg Loss: 0.2527\n","Step 8881: Loss: 0.1791, requires_grad: True\n","Step 8881: Gradients computed successfully\n","Step 8882: Loss: 0.1773, requires_grad: True\n","Step 8882: Gradients computed successfully\n","Step 8883: Loss: 0.1936, requires_grad: True\n","Step 8883: Gradients computed successfully\n","Step 8884: Loss: 0.0749, requires_grad: True\n","Step 8884: Gradients computed successfully\n","Step 8885: Loss: 0.1797, requires_grad: True\n","Step 8885: Gradients computed successfully\n","Step 8886: Loss: 0.3784, requires_grad: True\n","Step 8886: Gradients computed successfully\n","Step 8887: Loss: 0.2125, requires_grad: True\n","Step 8887: Gradients computed successfully\n","Step 8888: Loss: 0.0982, requires_grad: True\n","Step 8888: Gradients computed successfully\n","Step 8889: Loss: 0.1548, requires_grad: True\n","Step 8889: Gradients computed successfully\n","Step 8890: Loss: 0.5115, requires_grad: True\n","Step 8890: Gradients computed successfully\n","Epoch 0, Step 8890, Loss 0.5115, Avg Loss: 0.2527\n","Step 8891: Loss: 0.1324, requires_grad: True\n","Step 8891: Gradients computed successfully\n","Step 8892: Loss: 0.3729, requires_grad: True\n","Step 8892: Gradients computed successfully\n","Step 8893: Loss: 0.3327, requires_grad: True\n","Step 8893: Gradients computed successfully\n","Step 8894: Loss: 0.1992, requires_grad: True\n","Step 8894: Gradients computed successfully\n","Step 8895: Loss: 0.2517, requires_grad: True\n","Step 8895: Gradients computed successfully\n","Step 8896: Loss: 0.1607, requires_grad: True\n","Step 8896: Gradients computed successfully\n","Step 8897: Loss: 0.0838, requires_grad: True\n","Step 8897: Gradients computed successfully\n","Step 8898: Loss: 0.3382, requires_grad: True\n","Step 8898: Gradients computed successfully\n","Step 8899: Loss: 0.2074, requires_grad: True\n","Step 8899: Gradients computed successfully\n","Step 8900: Loss: 0.7819, requires_grad: True\n","Step 8900: Gradients computed successfully\n","Epoch 0, Step 8900, Loss 0.7819, Avg Loss: 0.2527\n","Step 8901: Loss: 0.1468, requires_grad: True\n","Step 8901: Gradients computed successfully\n","Step 8902: Loss: 0.1802, requires_grad: True\n","Step 8902: Gradients computed successfully\n","Step 8903: Loss: 0.2973, requires_grad: True\n","Step 8903: Gradients computed successfully\n","Step 8904: Loss: 0.3630, requires_grad: True\n","Step 8904: Gradients computed successfully\n","Step 8905: Loss: 0.2659, requires_grad: True\n","Step 8905: Gradients computed successfully\n","Step 8906: Loss: 0.3180, requires_grad: True\n","Step 8906: Gradients computed successfully\n","Step 8907: Loss: 0.6578, requires_grad: True\n","Step 8907: Gradients computed successfully\n","Step 8908: Loss: 0.1201, requires_grad: True\n","Step 8908: Gradients computed successfully\n","Step 8909: Loss: 0.1580, requires_grad: True\n","Step 8909: Gradients computed successfully\n","Step 8910: Loss: 0.1872, requires_grad: True\n","Step 8910: Gradients computed successfully\n","Epoch 0, Step 8910, Loss 0.1872, Avg Loss: 0.2527\n","Step 8911: Loss: 0.2522, requires_grad: True\n","Step 8911: Gradients computed successfully\n","Step 8912: Loss: 0.3900, requires_grad: True\n","Step 8912: Gradients computed successfully\n","Step 8913: Loss: 0.2694, requires_grad: True\n","Step 8913: Gradients computed successfully\n","Step 8914: Loss: 0.1749, requires_grad: True\n","Step 8914: Gradients computed successfully\n","Step 8915: Loss: 0.1806, requires_grad: True\n","Step 8915: Gradients computed successfully\n","Step 8916: Loss: 0.1534, requires_grad: True\n","Step 8916: Gradients computed successfully\n","Step 8917: Loss: 0.3185, requires_grad: True\n","Step 8917: Gradients computed successfully\n","Step 8918: Loss: 0.2018, requires_grad: True\n","Step 8918: Gradients computed successfully\n","Step 8919: Loss: 0.2777, requires_grad: True\n","Step 8919: Gradients computed successfully\n","Step 8920: Loss: 0.2636, requires_grad: True\n","Step 8920: Gradients computed successfully\n","Epoch 0, Step 8920, Loss 0.2636, Avg Loss: 0.2527\n","Step 8921: Loss: 0.2486, requires_grad: True\n","Step 8921: Gradients computed successfully\n","Step 8922: Loss: 0.1646, requires_grad: True\n","Step 8922: Gradients computed successfully\n","Step 8923: Loss: 0.2998, requires_grad: True\n","Step 8923: Gradients computed successfully\n","Step 8924: Loss: 0.3028, requires_grad: True\n","Step 8924: Gradients computed successfully\n","Step 8925: Loss: 0.4680, requires_grad: True\n","Step 8925: Gradients computed successfully\n","Step 8926: Loss: 0.3057, requires_grad: True\n","Step 8926: Gradients computed successfully\n","Step 8927: Loss: 0.2901, requires_grad: True\n","Step 8927: Gradients computed successfully\n","Step 8928: Loss: 0.3170, requires_grad: True\n","Step 8928: Gradients computed successfully\n","Step 8929: Loss: 0.3185, requires_grad: True\n","Step 8929: Gradients computed successfully\n","Step 8930: Loss: 0.1578, requires_grad: True\n","Step 8930: Gradients computed successfully\n","Epoch 0, Step 8930, Loss 0.1578, Avg Loss: 0.2528\n","Step 8931: Loss: 0.2017, requires_grad: True\n","Step 8931: Gradients computed successfully\n","Step 8932: Loss: 0.1777, requires_grad: True\n","Step 8932: Gradients computed successfully\n","Step 8933: Loss: 0.2233, requires_grad: True\n","Step 8933: Gradients computed successfully\n","Step 8934: Loss: 0.1394, requires_grad: True\n","Step 8934: Gradients computed successfully\n","Step 8935: Loss: 0.2510, requires_grad: True\n","Step 8935: Gradients computed successfully\n","Step 8936: Loss: 0.2020, requires_grad: True\n","Step 8936: Gradients computed successfully\n","Step 8937: Loss: 0.4503, requires_grad: True\n","Step 8937: Gradients computed successfully\n","Step 8938: Loss: 0.1372, requires_grad: True\n","Step 8938: Gradients computed successfully\n","Step 8939: Loss: 0.2885, requires_grad: True\n","Step 8939: Gradients computed successfully\n","Step 8940: Loss: 0.1641, requires_grad: True\n","Step 8940: Gradients computed successfully\n","Epoch 0, Step 8940, Loss 0.1641, Avg Loss: 0.2527\n","Step 8941: Loss: 0.3379, requires_grad: True\n","Step 8941: Gradients computed successfully\n","Step 8942: Loss: 0.2166, requires_grad: True\n","Step 8942: Gradients computed successfully\n","Step 8943: Loss: 0.1393, requires_grad: True\n","Step 8943: Gradients computed successfully\n","Step 8944: Loss: 0.1111, requires_grad: True\n","Step 8944: Gradients computed successfully\n","Step 8945: Loss: 0.1946, requires_grad: True\n","Step 8945: Gradients computed successfully\n","Step 8946: Loss: 0.2214, requires_grad: True\n","Step 8946: Gradients computed successfully\n","Step 8947: Loss: 0.3564, requires_grad: True\n","Step 8947: Gradients computed successfully\n","Step 8948: Loss: 0.3539, requires_grad: True\n","Step 8948: Gradients computed successfully\n","Step 8949: Loss: 0.5311, requires_grad: True\n","Step 8949: Gradients computed successfully\n","Step 8950: Loss: 0.1997, requires_grad: True\n","Step 8950: Gradients computed successfully\n","Epoch 0, Step 8950, Loss 0.1997, Avg Loss: 0.2527\n","Step 8951: Loss: 0.4143, requires_grad: True\n","Step 8951: Gradients computed successfully\n","Step 8952: Loss: 0.2254, requires_grad: True\n","Step 8952: Gradients computed successfully\n","Step 8953: Loss: 0.4478, requires_grad: True\n","Step 8953: Gradients computed successfully\n","Step 8954: Loss: 0.3324, requires_grad: True\n","Step 8954: Gradients computed successfully\n","Step 8955: Loss: 0.1357, requires_grad: True\n","Step 8955: Gradients computed successfully\n","Step 8956: Loss: 0.1426, requires_grad: True\n","Step 8956: Gradients computed successfully\n","Step 8957: Loss: 0.2234, requires_grad: True\n","Step 8957: Gradients computed successfully\n","Step 8958: Loss: 0.1640, requires_grad: True\n","Step 8958: Gradients computed successfully\n","Step 8959: Loss: 0.1716, requires_grad: True\n","Step 8959: Gradients computed successfully\n","Step 8960: Loss: 0.2129, requires_grad: True\n","Step 8960: Gradients computed successfully\n","Epoch 0, Step 8960, Loss 0.2129, Avg Loss: 0.2527\n","Step 8961: Loss: 0.2994, requires_grad: True\n","Step 8961: Gradients computed successfully\n","Step 8962: Loss: 0.2757, requires_grad: True\n","Step 8962: Gradients computed successfully\n","Step 8963: Loss: 0.1722, requires_grad: True\n","Step 8963: Gradients computed successfully\n","Step 8964: Loss: 0.1548, requires_grad: True\n","Step 8964: Gradients computed successfully\n","Step 8965: Loss: 0.1105, requires_grad: True\n","Step 8965: Gradients computed successfully\n","Step 8966: Loss: 0.0571, requires_grad: True\n","Step 8966: Gradients computed successfully\n","Step 8967: Loss: 0.1807, requires_grad: True\n","Step 8967: Gradients computed successfully\n","Step 8968: Loss: 0.2819, requires_grad: True\n","Step 8968: Gradients computed successfully\n","Step 8969: Loss: 0.1017, requires_grad: True\n","Step 8969: Gradients computed successfully\n","Step 8970: Loss: 0.3132, requires_grad: True\n","Step 8970: Gradients computed successfully\n","Epoch 0, Step 8970, Loss 0.3132, Avg Loss: 0.2527\n","Step 8971: Loss: 0.1915, requires_grad: True\n","Step 8971: Gradients computed successfully\n","Step 8972: Loss: 0.1749, requires_grad: True\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-e0a8506dbe0d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# Check if gradients were computed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Merge the adapter into the base model\n","print(\"Merging adapter into base model...\")\n","merged_model = model.merge_and_unload()\n","\n","# Save the merged model\n","merged_model_dir = \"/content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_merged_ready\"\n","merged_model.save_pretrained(merged_model_dir)\n","tokenizer.save_pretrained(merged_model_dir)\n","print(f\"Saved merged model to {merged_model_dir}\")\n","\n","# Load the merged model fresh\n","from transformers import AutoModelForCausalLM\n","import torch\n","from peft import LoraConfig, get_peft_model, TaskType\n","\n","print(\"Loading merged model...\")\n","fresh_model = AutoModelForCausalLM.from_pretrained(\n","    merged_model_dir,\n","    torch_dtype=torch.float16,\n",")\n","fresh_model = fresh_model.cuda()\n","\n","# Apply a new LoRA adapter with more targets\n","new_peft_config = LoraConfig(\n","    r=32,                          # Higher rank\n","    lora_alpha=64,                 # Scaled with rank\n","    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # All attention components\n","    lora_dropout=0.05,\n","    bias=\"none\",                   # No bias training\n","    task_type=TaskType.CAUSAL_LM\n",")\n","\n","# Create new LoRA model\n","new_model = get_peft_model(fresh_model, new_peft_config)\n","print(\"Created fresh model with new LoRA configuration\")\n","new_model.print_trainable_parameters()\n","\n","# Create a simple dataloader with a small batch size\n","from torch.utils.data import DataLoader\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=1,  # Start with batch size 1\n","    shuffle=True\n",")\n","\n","# Set up optimizer just for the LoRA parameters\n","optimizer = torch.optim.AdamW(\n","    [p for p in new_model.parameters() if p.requires_grad],\n","    lr=2e-5,\n","    weight_decay=0.01\n",")\n","\n","# Simple training loop\n","print(\"Starting training with merged model + fresh LoRA\")\n","new_model.train()\n","\n","for epoch in range(2):  # 2 epochs\n","    running_loss = 0.0\n","    for step, batch in enumerate(train_dataloader):\n","        # Move batch to GPU\n","        batch = {k: v.cuda() for k, v in batch.items()}\n","\n","        # Forward pass\n","        outputs = new_model(\n","            input_ids=batch[\"input_ids\"],\n","            attention_mask=batch[\"attention_mask\"],\n","            labels=batch[\"labels\"],\n","            use_cache=False  # Disable cache\n","        )\n","\n","        loss = outputs.loss\n","        print(f\"Step {step}: Loss: {loss.item():.4f}, requires_grad: {loss.requires_grad}\")\n","\n","        # Backward pass\n","        loss.backward()\n","\n","        # Check if gradients were computed\n","        has_grad = any(p.grad is not None for p in new_model.parameters() if p.requires_grad)\n","        if has_grad:\n","            print(f\"Step {step}: Gradients computed successfully\")\n","        else:\n","            print(f\"Step {step}: No gradients computed!\")\n","            continue  # Skip this step if no gradients\n","\n","        # Update weights\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","        # Track loss\n","        running_loss += loss.item()\n","\n","        # Log progress\n","        if step % 10 == 0:\n","            print(f\"Epoch {epoch}, Step {step}, Loss {loss.item():.4f}, Avg Loss: {running_loss/(step+1):.4f}\")\n","\n","        # Save checkpoint periodically\n","        if step % 500 == 0 and step > 0:\n","            checkpoint_dir = f\"/content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_complex_sql_continued/manual-epoch-{epoch}-step-{step}\"\n","            new_model.save_pretrained(checkpoint_dir)\n","            print(f\"Saved checkpoint to {checkpoint_dir}\")\n","\n","    # Save at end of epoch\n","    epoch_dir = f\"/content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_complex_sql_continued/manual-epoch-{epoch}\"\n","    new_model.save_pretrained(epoch_dir)\n","    print(f\"Epoch {epoch} completed. Average loss: {running_loss / len(train_dataloader):.4f}\")\n","\n","# Save final model\n","final_dir = f\"/content/drive/Shareddrives/DATA266_Project/Checkpoints/mistral_complex_sql_continued/spider_x_synsql_model\"\n","new_model.save_pretrained(final_dir)\n","tokenizer.save_pretrained(final_dir)\n","print(f\"Training complete! Final model saved to {final_dir}\")"]},{"cell_type":"markdown","metadata":{"id":"Ub8K9rF-eBIb"},"source":["Our Attempted Approach\n","In our recent attempts, we were:\n","\n","Starting with an already merged model (where the adapter weights had been merged into the base model)\n","Creating a new LoRA adapter on top of this merged model\n","Trying to train the new adapter\n","\n","The Critical Difference\n","The merged model approach introduces complexity:\n","\n","When you merge a LoRA adapter into a base model, the adapter weights become \"baked in\" to the base model weights\n","This creates a new starting point that has already incorporated the adapter's changes\n","Then adding a new LoRA adapter on top of this merged model creates a different training dynamic\n","\n","In contrast, the working approach:\n","\n","Keeps a clean separation between base model and adapter\n","Directly loads and continues training with the existing adapter\n","Maintains the original adapter structure and training pattern\n","\n","Why This Matters\n","\n","Gradient Flow: The working approach maintains the original gradient flow paths established in the initial training\n","Parameter Structure: It preserves the exact parameter structure that was successfully used before\n","Continuity: It's truly \"continuing\" training rather than starting fresh on top of a merged model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y3PWx3Y8eBIb","outputId":"bd227cd3-56a9-4abf-9ba7-89bb6f631145"},"outputs":[{"name":"stdout","output_type":"stream","text":["Setup complete. Using GPU: NVIDIA GeForce RTX 4090\n"]}],"source":["import os\n","import torch\n","import gc\n","import json\n","import random\n","import numpy as np\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from peft import PeftModel, LoraConfig, get_peft_model, TaskType\n","from safetensors.torch import load_file\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm.auto import tqdm\n","\n","# Set random seeds for reproducibility\n","def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","\n","set_seed(42)\n","\n","# Set paths directly without a base path\n","checkpoint_path = \"Checkpoints/mistral_spider_lora/checkpoint-1311\"\n","output_dir = \"Checkpoints/mistral_complex_sql_training_4090\"\n","preprocessed_data_path = \"data/preprocessed_data\"\n","\n","# Create output directory if it doesn't exist\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Clean up GPU memory\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","print(\"Setup complete. Using GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yVXfAAoBeBIb","outputId":"bce2f45c-6917-49a1-dc2d-501ecab646aa","colab":{"referenced_widgets":["8a5a84c0ba0e4ba685eaa691d998cd96"]}},"outputs":[{"name":"stdout","output_type":"stream","text":["BF16 precision support: True\n","Loading base Mistral-7B model...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8a5a84c0ba0e4ba685eaa691d998cd96","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Base model loaded and moved to CUDA\n","Created LoRA model with configuration matching previous training\n","Loading adapter weights from Checkpoints/mistral_spider_lora/checkpoint-1311/adapter_model.safetensors\n","Adapter weights loaded successfully\n","Enabled gradients for base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight\n","Enabled gradients for base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight\n","Enabled gradients for base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight\n","Number of trainable parameters: 128\n","Total trainable parameter count: 6815744\n","trainable params: 6,815,744 || all params: 7,248,547,840 || trainable%: 0.0940\n"]}],"source":["# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n","tokenizer.pad_token = tokenizer.eos_token  # Ensure padding works correctly\n","\n","# Determine if BF16 is supported\n","def is_bf16_supported():\n","    if torch.cuda.is_available():\n","        compute_capability = torch.cuda.get_device_capability()[0]\n","        return compute_capability >= 8  # Ampere (RTX 30xx) and newer support BF16\n","    return False\n","\n","has_bf16 = is_bf16_supported()\n","print(f\"BF16 precision support: {has_bf16}\")\n","\n","# Load base model without device mapping\n","print(\"Loading base Mistral-7B model...\")\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    \"mistralai/Mistral-7B-Instruct-v0.1\",\n","    torch_dtype=torch.bfloat16 if has_bf16 else torch.float16,\n","    # Don't use device_map here\n",")\n","\n","# Move to CUDA explicitly\n","base_model = base_model.cuda()\n","print(\"Base model loaded and moved to CUDA\")\n","\n","# Create LoRA configuration matching your previous training\n","peft_config = LoraConfig(\n","    r=16,                      # Match original adapter config\n","    lora_alpha=32,             # Match original adapter config\n","    target_modules=[\"q_proj\", \"v_proj\"],  # Match original target modules\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=TaskType.CAUSAL_LM\n",")\n","\n","# Apply LoRA\n","model = get_peft_model(base_model, peft_config)\n","print(\"Created LoRA model with configuration matching previous training\")\n","\n","# Load adapter weights from checkpoint\n","safetensors_path = os.path.join(checkpoint_path, \"adapter_model.safetensors\")\n","if os.path.exists(safetensors_path):\n","    print(f\"Loading adapter weights from {safetensors_path}\")\n","    state_dict = load_file(safetensors_path)\n","    model.load_state_dict(state_dict, strict=False)\n","    print(\"Adapter weights loaded successfully\")\n","else:\n","    print(f\"WARNING: Could not find adapter weights at {safetensors_path}\")\n","\n","# CRITICAL: Make sure parameters are set to require gradients\n","for name, param in model.named_parameters():\n","    if 'lora' in name:  # Only LoRA parameters should be trained\n","        param.requires_grad = True\n","        print(f\"Enabled gradients for {name}\")\n","\n","# Verify trainable parameters\n","trainable_params = [p for p in model.parameters() if p.requires_grad]\n","print(f\"Number of trainable parameters: {len(trainable_params)}\")\n","print(f\"Total trainable parameter count: {sum(p.numel() for p in trainable_params)}\")\n","\n","# Print model structure info\n","model.print_trainable_parameters()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n71BrOSSeBIb","outputId":"ee7b9a7a-795a-467f-c34e-c2df4e3390c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading tokenized data from data/preprocessed_data/tokenized_combined_data.pt\n","Loaded 37000 tokenized examples\n","Loading dataset splits from data/preprocessed_data/dataset_splits.json\n","Creating datasets with 35150 training examples and 1850 validation examples\n","Created training dataset with 35150 examples\n","Created validation dataset with 1850 examples\n","Created DataLoader with batch size 1\n","Training steps per epoch: 35150\n"]}],"source":["# Custom dataset class\n","class SQLDataset(Dataset):\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            \"input_ids\": self.data[idx][\"input_ids\"],\n","            \"attention_mask\": self.data[idx][\"attention_mask\"],\n","            \"labels\": self.data[idx][\"labels\"]\n","        }\n","\n","# Load the tokenized data and splits\n","tokenized_file = os.path.join(preprocessed_data_path, \"tokenized_combined_data.pt\")\n","splits_file = os.path.join(preprocessed_data_path, \"dataset_splits.json\")\n","\n","print(f\"Loading tokenized data from {tokenized_file}\")\n","tokenized_data = torch.load(tokenized_file)\n","print(f\"Loaded {len(tokenized_data)} tokenized examples\")\n","\n","print(f\"Loading dataset splits from {splits_file}\")\n","with open(splits_file, 'r') as f:\n","    splits = json.load(f)\n","\n","# Create the datasets based on the saved splits\n","train_indices = splits[\"train_indices\"]\n","val_indices = splits[\"val_indices\"]\n","\n","print(f\"Creating datasets with {len(train_indices)} training examples and {len(val_indices)} validation examples\")\n","train_data = [tokenized_data[i] for i in train_indices]\n","val_data = [tokenized_data[i] for i in val_indices]\n","\n","train_dataset = SQLDataset(train_data)\n","val_dataset = SQLDataset(val_data)\n","\n","print(f\"Created training dataset with {len(train_dataset)} examples\")\n","print(f\"Created validation dataset with {len(val_dataset)} examples\")\n","\n","# Free up memory\n","del tokenized_data\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","# Create DataLoader for training\n","batch_size = 1  # Start with a small batch size for stability\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=batch_size,\n","    shuffle=True,\n","    num_workers=0  # No parallel workers to avoid memory issues\n",")\n","\n","# Create DataLoader for validation (optional)\n","val_dataloader = DataLoader(\n","    val_dataset,\n","    batch_size=batch_size,\n","    shuffle=False,\n","    num_workers=0\n",")\n","\n","print(f\"Created DataLoader with batch size {batch_size}\")\n","print(f\"Training steps per epoch: {len(train_dataloader)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cRDV9lxXeBIb","outputId":"8a074896-400b-4cd9-8859-1097ca2aa501"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training configuration:\n","- Learning rate: 2e-05\n","- Weight decay: 0.01\n","- Epochs: 2\n","- Gradient accumulation steps: 8\n","- Effective batch size: 8\n","- Total training steps: 8787\n","- Warmup steps: 263\n"]}],"source":["# Training hyperparameters\n","learning_rate = 2e-5\n","weight_decay = 0.01\n","num_epochs = 2\n","grad_accumulation_steps = 8  # Accumulate gradients over multiple steps\n","\n","# Create optimizer for LoRA parameters only\n","optimizer = torch.optim.AdamW(\n","    [p for p in model.parameters() if p.requires_grad],\n","    lr=learning_rate,\n","    weight_decay=weight_decay,\n","    betas=(0.9, 0.999)\n",")\n","\n","# Create a simple linear learning rate scheduler with warmup\n","from transformers import get_scheduler\n","num_training_steps = len(train_dataloader) * num_epochs // grad_accumulation_steps\n","num_warmup_steps = int(num_training_steps * 0.03)  # 3% warmup\n","\n","# Create scheduler\n","scheduler = get_scheduler(\n","    \"linear\",\n","    optimizer=optimizer,\n","    num_warmup_steps=num_warmup_steps,\n","    num_training_steps=num_training_steps\n",")\n","\n","print(\"Training configuration:\")\n","print(f\"- Learning rate: {learning_rate}\")\n","print(f\"- Weight decay: {weight_decay}\")\n","print(f\"- Epochs: {num_epochs}\")\n","print(f\"- Gradient accumulation steps: {grad_accumulation_steps}\")\n","print(f\"- Effective batch size: {batch_size * grad_accumulation_steps}\")\n","print(f\"- Total training steps: {num_training_steps}\")\n","print(f\"- Warmup steps: {num_warmup_steps}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"INVfBhqTeBIb","outputId":"74480468-e67d-4c25-a680-33894b3426e1","colab":{"referenced_widgets":["92be57de81894c43ae9324b78e58ae92","f303be7a5e1a48a39a8f1e1d7299bb6c"]}},"outputs":[{"name":"stdout","output_type":"stream","text":["\n"," Starting training \n","\n","Epoch 1/2\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"92be57de81894c43ae9324b78e58ae92","version_major":2,"version_minor":0},"text/plain":["Epoch 1:   0%|          | 0/35150 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Step 0: Loss: 0.3508, requires_grad: True\n","Step 0: Gradients computed successfully\n","Step 10: Loss: 0.4638, requires_grad: True\n","Step 20: Loss: 0.3787, requires_grad: True\n","Step 30: Loss: 0.5463, requires_grad: True\n","Step 40: Loss: 0.4859, requires_grad: True\n","Step 50: Loss: 0.5323, requires_grad: True\n","Step 50: Gradients computed successfully\n","Step 60: Loss: 0.4579, requires_grad: True\n","Step 70: Loss: 0.5387, requires_grad: True\n","Step 80: Loss: 0.5212, requires_grad: True\n","Step 90: Loss: 0.5180, requires_grad: True\n","Step 100: Loss: 0.4386, requires_grad: True\n","Step 100: Gradients computed successfully\n","Step 110: Loss: 0.5440, requires_grad: True\n","Step 120: Loss: 0.6301, requires_grad: True\n","Step 130: Loss: 0.4099, requires_grad: True\n","Step 140: Loss: 0.4736, requires_grad: True\n","Step 150: Loss: 0.5047, requires_grad: True\n","Step 150: Gradients computed successfully\n","Step 160: Loss: 0.4262, requires_grad: True\n","Step 170: Loss: 0.6133, requires_grad: True\n","Step 180: Loss: 0.5972, requires_grad: True\n","Step 190: Loss: 0.6734, requires_grad: True\n","Step 200: Loss: 0.8345, requires_grad: True\n","Step 200: Gradients computed successfully\n","Step 210: Loss: 0.4884, requires_grad: True\n","Step 220: Loss: 0.7987, requires_grad: True\n","Step 230: Loss: 0.4975, requires_grad: True\n","Step 240: Loss: 0.4197, requires_grad: True\n","Step 250: Loss: 0.6603, requires_grad: True\n","Step 250: Gradients computed successfully\n","Step 260: Loss: 0.8782, requires_grad: True\n","Step 270: Loss: 0.9113, requires_grad: True\n","Step 280: Loss: 0.6068, requires_grad: True\n","Step 290: Loss: 0.6626, requires_grad: True\n","Step 300: Loss: 0.4466, requires_grad: True\n","Step 300: Gradients computed successfully\n","Step 310: Loss: 0.9243, requires_grad: True\n","Step 320: Loss: 0.5476, requires_grad: True\n","Step 330: Loss: 0.4363, requires_grad: True\n","Step 340: Loss: 0.5071, requires_grad: True\n","Step 350: Loss: 0.9533, requires_grad: True\n","Step 350: Gradients computed successfully\n","Step 360: Loss: 0.8543, requires_grad: True\n","Step 370: Loss: 0.4303, requires_grad: True\n","Step 380: Loss: 0.5623, requires_grad: True\n","Step 390: Loss: 0.4756, requires_grad: True\n","Step 400: Loss: 0.3865, requires_grad: True\n","Step 400: Gradients computed successfully\n","Step 410: Loss: 0.5255, requires_grad: True\n","Step 420: Loss: 0.5469, requires_grad: True\n","Step 430: Loss: 0.6494, requires_grad: True\n","Step 440: Loss: 0.7222, requires_grad: True\n","Step 450: Loss: 0.7789, requires_grad: True\n","Step 450: Gradients computed successfully\n","Step 460: Loss: 0.3498, requires_grad: True\n","Step 470: Loss: 0.4030, requires_grad: True\n","Step 480: Loss: 0.4457, requires_grad: True\n","Step 490: Loss: 0.5663, requires_grad: True\n","Step 500: Loss: 0.7460, requires_grad: True\n","Step 500: Gradients computed successfully\n","Step 510: Loss: 0.6228, requires_grad: True\n","Step 520: Loss: 0.5008, requires_grad: True\n","Step 530: Loss: 0.4249, requires_grad: True\n","Step 540: Loss: 0.6791, requires_grad: True\n","Step 550: Loss: 0.3765, requires_grad: True\n","Step 550: Gradients computed successfully\n","Step 560: Loss: 0.8489, requires_grad: True\n","Step 570: Loss: 0.6003, requires_grad: True\n","Step 580: Loss: 0.6727, requires_grad: True\n","Step 590: Loss: 0.6525, requires_grad: True\n","Step 600: Loss: 0.7682, requires_grad: True\n","Step 600: Gradients computed successfully\n","Step 610: Loss: 0.6378, requires_grad: True\n","Step 620: Loss: 0.8825, requires_grad: True\n","Step 630: Loss: 0.3816, requires_grad: True\n","Step 640: Loss: 0.6856, requires_grad: True\n","Step 650: Loss: 0.6969, requires_grad: True\n","Step 650: Gradients computed successfully\n","Step 660: Loss: 0.3384, requires_grad: True\n","Step 670: Loss: 0.9263, requires_grad: True\n","Step 680: Loss: 0.4958, requires_grad: True\n","Step 690: Loss: 0.5544, requires_grad: True\n","Step 700: Loss: 0.4633, requires_grad: True\n","Step 700: Gradients computed successfully\n","Step 710: Loss: 0.2595, requires_grad: True\n","Step 720: Loss: 0.8068, requires_grad: True\n","Step 730: Loss: 0.3246, requires_grad: True\n","Step 740: Loss: 0.5199, requires_grad: True\n","Step 750: Loss: 0.3682, requires_grad: True\n","Step 750: Gradients computed successfully\n","Step 760: Loss: 0.2744, requires_grad: True\n","Step 770: Loss: 0.5674, requires_grad: True\n","Step 780: Loss: 0.5704, requires_grad: True\n","Step 790: Loss: 0.3381, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-100\n","Step 800: Loss: 0.4085, requires_grad: True\n","Step 800: Gradients computed successfully\n","Step 810: Loss: 0.3220, requires_grad: True\n","Step 820: Loss: 0.3823, requires_grad: True\n","Step 830: Loss: 0.8096, requires_grad: True\n","Step 840: Loss: 1.0486, requires_grad: True\n","Step 850: Loss: 1.0065, requires_grad: True\n","Step 850: Gradients computed successfully\n","Step 860: Loss: 0.3401, requires_grad: True\n","Step 870: Loss: 0.3519, requires_grad: True\n","Step 880: Loss: 0.6812, requires_grad: True\n","Step 890: Loss: 0.4218, requires_grad: True\n","Step 900: Loss: 0.4685, requires_grad: True\n","Step 900: Gradients computed successfully\n","Step 910: Loss: 0.8162, requires_grad: True\n","Step 920: Loss: 0.3717, requires_grad: True\n","Step 930: Loss: 0.5194, requires_grad: True\n","Step 940: Loss: 0.3666, requires_grad: True\n","Step 950: Loss: 0.5698, requires_grad: True\n","Step 950: Gradients computed successfully\n","Step 960: Loss: 0.3446, requires_grad: True\n","Step 970: Loss: 0.4512, requires_grad: True\n","Step 980: Loss: 0.5010, requires_grad: True\n","Step 990: Loss: 0.4991, requires_grad: True\n","Step 1000: Loss: 0.4040, requires_grad: True\n","Step 1000: Gradients computed successfully\n","Step 1010: Loss: 0.4633, requires_grad: True\n","Step 1020: Loss: 1.1594, requires_grad: True\n","Step 1030: Loss: 0.7041, requires_grad: True\n","Step 1040: Loss: 0.4668, requires_grad: True\n","Step 1050: Loss: 0.5528, requires_grad: True\n","Step 1050: Gradients computed successfully\n","Step 1060: Loss: 1.0753, requires_grad: True\n","Step 1070: Loss: 0.4970, requires_grad: True\n","Step 1080: Loss: 0.4633, requires_grad: True\n","Step 1090: Loss: 0.2238, requires_grad: True\n","Step 1100: Loss: 0.7700, requires_grad: True\n","Step 1100: Gradients computed successfully\n","Step 1110: Loss: 0.5886, requires_grad: True\n","Step 1120: Loss: 0.6217, requires_grad: True\n","Step 1130: Loss: 0.8290, requires_grad: True\n","Step 1140: Loss: 0.2023, requires_grad: True\n","Step 1150: Loss: 0.2934, requires_grad: True\n","Step 1150: Gradients computed successfully\n","Step 1160: Loss: 0.2881, requires_grad: True\n","Step 1170: Loss: 0.4107, requires_grad: True\n","Step 1180: Loss: 0.6093, requires_grad: True\n","Step 1190: Loss: 0.4961, requires_grad: True\n","Step 1200: Loss: 0.3651, requires_grad: True\n","Step 1200: Gradients computed successfully\n","Step 1210: Loss: 0.1334, requires_grad: True\n","Step 1220: Loss: 0.5717, requires_grad: True\n","Step 1230: Loss: 0.4141, requires_grad: True\n","Step 1240: Loss: 0.9310, requires_grad: True\n","Step 1250: Loss: 0.3871, requires_grad: True\n","Step 1250: Gradients computed successfully\n","Step 1260: Loss: 0.3603, requires_grad: True\n","Step 1270: Loss: 0.3035, requires_grad: True\n","Step 1280: Loss: 0.2425, requires_grad: True\n","Step 1290: Loss: 0.1736, requires_grad: True\n","Step 1300: Loss: 0.5592, requires_grad: True\n","Step 1300: Gradients computed successfully\n","Step 1310: Loss: 0.3255, requires_grad: True\n","Step 1320: Loss: 0.2859, requires_grad: True\n","Step 1330: Loss: 0.3682, requires_grad: True\n","Step 1340: Loss: 0.3207, requires_grad: True\n","Step 1350: Loss: 0.5657, requires_grad: True\n","Step 1350: Gradients computed successfully\n","Step 1360: Loss: 0.2792, requires_grad: True\n","Step 1370: Loss: 0.2453, requires_grad: True\n","Step 1380: Loss: 0.3444, requires_grad: True\n","Step 1390: Loss: 0.5478, requires_grad: True\n","Step 1400: Loss: 0.4883, requires_grad: True\n","Step 1400: Gradients computed successfully\n","Step 1410: Loss: 0.2919, requires_grad: True\n","Step 1420: Loss: 0.6371, requires_grad: True\n","Step 1430: Loss: 0.1372, requires_grad: True\n","Step 1440: Loss: 0.3600, requires_grad: True\n","Step 1450: Loss: 0.4305, requires_grad: True\n","Step 1450: Gradients computed successfully\n","Step 1460: Loss: 0.4913, requires_grad: True\n","Step 1470: Loss: 0.2676, requires_grad: True\n","Step 1480: Loss: 0.3083, requires_grad: True\n","Step 1490: Loss: 0.5866, requires_grad: True\n","Step 1500: Loss: 0.6328, requires_grad: True\n","Step 1500: Gradients computed successfully\n","Step 1510: Loss: 0.2541, requires_grad: True\n","Step 1520: Loss: 0.3227, requires_grad: True\n","Step 1530: Loss: 0.8241, requires_grad: True\n","Step 1540: Loss: 0.1320, requires_grad: True\n","Step 1550: Loss: 0.7052, requires_grad: True\n","Step 1550: Gradients computed successfully\n","Step 1560: Loss: 0.5205, requires_grad: True\n","Step 1570: Loss: 0.5613, requires_grad: True\n","Step 1580: Loss: 0.2122, requires_grad: True\n","Step 1590: Loss: 0.6012, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-200\n","Step 1600: Loss: 0.2159, requires_grad: True\n","Step 1600: Gradients computed successfully\n","Step 1610: Loss: 0.1749, requires_grad: True\n","Step 1620: Loss: 0.2212, requires_grad: True\n","Step 1630: Loss: 0.3980, requires_grad: True\n","Step 1640: Loss: 0.2631, requires_grad: True\n","Step 1650: Loss: 0.1553, requires_grad: True\n","Step 1650: Gradients computed successfully\n","Step 1660: Loss: 0.3016, requires_grad: True\n","Step 1670: Loss: 0.2036, requires_grad: True\n","Step 1680: Loss: 0.6284, requires_grad: True\n","Step 1690: Loss: 0.2113, requires_grad: True\n","Step 1700: Loss: 0.2800, requires_grad: True\n","Step 1700: Gradients computed successfully\n","Step 1710: Loss: 0.2281, requires_grad: True\n","Step 1720: Loss: 0.1961, requires_grad: True\n","Step 1730: Loss: 0.2596, requires_grad: True\n","Step 1740: Loss: 0.4365, requires_grad: True\n","Step 1750: Loss: 0.3436, requires_grad: True\n","Step 1750: Gradients computed successfully\n","Step 1760: Loss: 0.6055, requires_grad: True\n","Step 1770: Loss: 0.2510, requires_grad: True\n","Step 1780: Loss: 0.5855, requires_grad: True\n","Step 1790: Loss: 0.2485, requires_grad: True\n","Step 1800: Loss: 0.5697, requires_grad: True\n","Step 1800: Gradients computed successfully\n","Step 1810: Loss: 0.2202, requires_grad: True\n","Step 1820: Loss: 0.1830, requires_grad: True\n","Step 1830: Loss: 0.2979, requires_grad: True\n","Step 1840: Loss: 0.1545, requires_grad: True\n","Step 1850: Loss: 0.3296, requires_grad: True\n","Step 1850: Gradients computed successfully\n","Step 1860: Loss: 0.4441, requires_grad: True\n","Step 1870: Loss: 0.7886, requires_grad: True\n","Step 1880: Loss: 0.3409, requires_grad: True\n","Step 1890: Loss: 0.4074, requires_grad: True\n","Step 1900: Loss: 0.4247, requires_grad: True\n","Step 1900: Gradients computed successfully\n","Step 1910: Loss: 0.2029, requires_grad: True\n","Step 1920: Loss: 0.4629, requires_grad: True\n","Step 1930: Loss: 0.2063, requires_grad: True\n","Step 1940: Loss: 0.1626, requires_grad: True\n","Step 1950: Loss: 0.3930, requires_grad: True\n","Step 1950: Gradients computed successfully\n","Step 1960: Loss: 0.6723, requires_grad: True\n","Step 1970: Loss: 0.2321, requires_grad: True\n","Step 1980: Loss: 0.3655, requires_grad: True\n","Step 1990: Loss: 0.5357, requires_grad: True\n","Step 2000: Loss: 0.3727, requires_grad: True\n","Step 2000: Gradients computed successfully\n","Step 2010: Loss: 0.3147, requires_grad: True\n","Step 2020: Loss: 0.1707, requires_grad: True\n","Step 2030: Loss: 0.4161, requires_grad: True\n","Step 2040: Loss: 0.2064, requires_grad: True\n","Step 2050: Loss: 0.2170, requires_grad: True\n","Step 2050: Gradients computed successfully\n","Step 2060: Loss: 0.4015, requires_grad: True\n","Step 2070: Loss: 0.4730, requires_grad: True\n","Step 2080: Loss: 0.6611, requires_grad: True\n","Step 2090: Loss: 0.5213, requires_grad: True\n","Step 2100: Loss: 0.3147, requires_grad: True\n","Step 2100: Gradients computed successfully\n","Step 2110: Loss: 0.1110, requires_grad: True\n","Step 2120: Loss: 0.2947, requires_grad: True\n","Step 2130: Loss: 0.3355, requires_grad: True\n","Step 2140: Loss: 0.2352, requires_grad: True\n","Step 2150: Loss: 0.6812, requires_grad: True\n","Step 2150: Gradients computed successfully\n","Step 2160: Loss: 0.5794, requires_grad: True\n","Step 2170: Loss: 0.1269, requires_grad: True\n","Step 2180: Loss: 0.8165, requires_grad: True\n","Step 2190: Loss: 0.1964, requires_grad: True\n","Step 2200: Loss: 0.3694, requires_grad: True\n","Step 2200: Gradients computed successfully\n","Step 2210: Loss: 0.4825, requires_grad: True\n","Step 2220: Loss: 0.3379, requires_grad: True\n","Step 2230: Loss: 0.4210, requires_grad: True\n","Step 2240: Loss: 0.2324, requires_grad: True\n","Step 2250: Loss: 0.2950, requires_grad: True\n","Step 2250: Gradients computed successfully\n","Step 2260: Loss: 0.4598, requires_grad: True\n","Step 2270: Loss: 0.2839, requires_grad: True\n","Step 2280: Loss: 0.2730, requires_grad: True\n","Step 2290: Loss: 0.1521, requires_grad: True\n","Step 2300: Loss: 0.2060, requires_grad: True\n","Step 2300: Gradients computed successfully\n","Step 2310: Loss: 0.4456, requires_grad: True\n","Step 2320: Loss: 0.3607, requires_grad: True\n","Step 2330: Loss: 0.2999, requires_grad: True\n","Step 2340: Loss: 0.3267, requires_grad: True\n","Step 2350: Loss: 0.0933, requires_grad: True\n","Step 2350: Gradients computed successfully\n","Step 2360: Loss: 0.3620, requires_grad: True\n","Step 2370: Loss: 0.3243, requires_grad: True\n","Step 2380: Loss: 0.1950, requires_grad: True\n","Step 2390: Loss: 0.2607, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-300\n","Step 2400: Loss: 0.3589, requires_grad: True\n","Step 2400: Gradients computed successfully\n","Step 2410: Loss: 0.2952, requires_grad: True\n","Step 2420: Loss: 0.3015, requires_grad: True\n","Step 2430: Loss: 0.3136, requires_grad: True\n","Step 2440: Loss: 0.3027, requires_grad: True\n","Step 2450: Loss: 0.2730, requires_grad: True\n","Step 2450: Gradients computed successfully\n","Step 2460: Loss: 0.1102, requires_grad: True\n","Step 2470: Loss: 0.4830, requires_grad: True\n","Step 2480: Loss: 0.1356, requires_grad: True\n","Step 2490: Loss: 0.1661, requires_grad: True\n","Step 2500: Loss: 0.4166, requires_grad: True\n","Step 2500: Gradients computed successfully\n","Step 2510: Loss: 0.1646, requires_grad: True\n","Step 2520: Loss: 0.4969, requires_grad: True\n","Step 2530: Loss: 0.4019, requires_grad: True\n","Step 2540: Loss: 0.2615, requires_grad: True\n","Step 2550: Loss: 0.2132, requires_grad: True\n","Step 2550: Gradients computed successfully\n","Step 2560: Loss: 0.3310, requires_grad: True\n","Step 2570: Loss: 0.3115, requires_grad: True\n","Step 2580: Loss: 0.2529, requires_grad: True\n","Step 2590: Loss: 0.3643, requires_grad: True\n","Step 2600: Loss: 0.6597, requires_grad: True\n","Step 2600: Gradients computed successfully\n","Step 2610: Loss: 0.1223, requires_grad: True\n","Step 2620: Loss: 0.1442, requires_grad: True\n","Step 2630: Loss: 0.2963, requires_grad: True\n","Step 2640: Loss: 0.2075, requires_grad: True\n","Step 2650: Loss: 0.5012, requires_grad: True\n","Step 2650: Gradients computed successfully\n","Step 2660: Loss: 0.3092, requires_grad: True\n","Step 2670: Loss: 0.1878, requires_grad: True\n","Step 2680: Loss: 0.5266, requires_grad: True\n","Step 2690: Loss: 0.2389, requires_grad: True\n","Step 2700: Loss: 0.4319, requires_grad: True\n","Step 2700: Gradients computed successfully\n","Step 2710: Loss: 0.2334, requires_grad: True\n","Step 2720: Loss: 0.5141, requires_grad: True\n","Step 2730: Loss: 0.3419, requires_grad: True\n","Step 2740: Loss: 0.6427, requires_grad: True\n","Step 2750: Loss: 0.3779, requires_grad: True\n","Step 2750: Gradients computed successfully\n","Step 2760: Loss: 0.1217, requires_grad: True\n","Step 2770: Loss: 0.3022, requires_grad: True\n","Step 2780: Loss: 0.3588, requires_grad: True\n","Step 2790: Loss: 0.3559, requires_grad: True\n","Step 2800: Loss: 0.2800, requires_grad: True\n","Step 2800: Gradients computed successfully\n","Step 2810: Loss: 0.3326, requires_grad: True\n","Step 2820: Loss: 0.1831, requires_grad: True\n","Step 2830: Loss: 0.2114, requires_grad: True\n","Step 2840: Loss: 0.2298, requires_grad: True\n","Step 2850: Loss: 0.1689, requires_grad: True\n","Step 2850: Gradients computed successfully\n","Step 2860: Loss: 0.2962, requires_grad: True\n","Step 2870: Loss: 0.2238, requires_grad: True\n","Step 2880: Loss: 0.9619, requires_grad: True\n","Step 2890: Loss: 0.1252, requires_grad: True\n","Step 2900: Loss: 0.4046, requires_grad: True\n","Step 2900: Gradients computed successfully\n","Step 2910: Loss: 0.1592, requires_grad: True\n","Step 2920: Loss: 0.5280, requires_grad: True\n","Step 2930: Loss: 0.6523, requires_grad: True\n","Step 2940: Loss: 0.5464, requires_grad: True\n","Step 2950: Loss: 0.3890, requires_grad: True\n","Step 2950: Gradients computed successfully\n","Step 2960: Loss: 0.3102, requires_grad: True\n","Step 2970: Loss: 0.3444, requires_grad: True\n","Step 2980: Loss: 0.1431, requires_grad: True\n","Step 2990: Loss: 0.1567, requires_grad: True\n","Step 3000: Loss: 0.1350, requires_grad: True\n","Step 3000: Gradients computed successfully\n","Step 3010: Loss: 0.2042, requires_grad: True\n","Step 3020: Loss: 0.2687, requires_grad: True\n","Step 3030: Loss: 0.1419, requires_grad: True\n","Step 3040: Loss: 0.0721, requires_grad: True\n","Step 3050: Loss: 0.2344, requires_grad: True\n","Step 3050: Gradients computed successfully\n","Step 3060: Loss: 0.4220, requires_grad: True\n","Step 3070: Loss: 0.1849, requires_grad: True\n","Step 3080: Loss: 0.2594, requires_grad: True\n","Step 3090: Loss: 0.3572, requires_grad: True\n","Step 3100: Loss: 0.4993, requires_grad: True\n","Step 3100: Gradients computed successfully\n","Step 3110: Loss: 0.3538, requires_grad: True\n","Step 3120: Loss: 0.0918, requires_grad: True\n","Step 3130: Loss: 0.1185, requires_grad: True\n","Step 3140: Loss: 0.2923, requires_grad: True\n","Step 3150: Loss: 0.3356, requires_grad: True\n","Step 3150: Gradients computed successfully\n","Step 3160: Loss: 0.3803, requires_grad: True\n","Step 3170: Loss: 0.2634, requires_grad: True\n","Step 3180: Loss: 0.4466, requires_grad: True\n","Step 3190: Loss: 0.2008, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-400\n","Step 3200: Loss: 0.1753, requires_grad: True\n","Step 3200: Gradients computed successfully\n","Step 3210: Loss: 0.1735, requires_grad: True\n","Step 3220: Loss: 0.1801, requires_grad: True\n","Step 3230: Loss: 0.3755, requires_grad: True\n","Step 3240: Loss: 0.2317, requires_grad: True\n","Step 3250: Loss: 0.6593, requires_grad: True\n","Step 3250: Gradients computed successfully\n","Step 3260: Loss: 0.2182, requires_grad: True\n","Step 3270: Loss: 0.2606, requires_grad: True\n","Step 3280: Loss: 0.1614, requires_grad: True\n","Step 3290: Loss: 0.4453, requires_grad: True\n","Step 3300: Loss: 0.4074, requires_grad: True\n","Step 3300: Gradients computed successfully\n","Step 3310: Loss: 0.2213, requires_grad: True\n","Step 3320: Loss: 0.2419, requires_grad: True\n","Step 3330: Loss: 0.1887, requires_grad: True\n","Step 3340: Loss: 0.2327, requires_grad: True\n","Step 3350: Loss: 0.1354, requires_grad: True\n","Step 3350: Gradients computed successfully\n","Step 3360: Loss: 0.4489, requires_grad: True\n","Step 3370: Loss: 0.2691, requires_grad: True\n","Step 3380: Loss: 0.3069, requires_grad: True\n","Step 3390: Loss: 0.2385, requires_grad: True\n","Step 3400: Loss: 0.4391, requires_grad: True\n","Step 3400: Gradients computed successfully\n","Step 3410: Loss: 0.4591, requires_grad: True\n","Step 3420: Loss: 0.3241, requires_grad: True\n","Step 3430: Loss: 0.1521, requires_grad: True\n","Step 3440: Loss: 0.4493, requires_grad: True\n","Step 3450: Loss: 0.2337, requires_grad: True\n","Step 3450: Gradients computed successfully\n","Step 3460: Loss: 0.4944, requires_grad: True\n","Step 3470: Loss: 0.3888, requires_grad: True\n","Step 3480: Loss: 0.3239, requires_grad: True\n","Step 3490: Loss: 0.4428, requires_grad: True\n","Step 3500: Loss: 0.1678, requires_grad: True\n","Step 3500: Gradients computed successfully\n","Step 3510: Loss: 0.2996, requires_grad: True\n","Step 3520: Loss: 0.4506, requires_grad: True\n","Step 3530: Loss: 0.3320, requires_grad: True\n","Step 3540: Loss: 0.1903, requires_grad: True\n","Step 3550: Loss: 0.2017, requires_grad: True\n","Step 3550: Gradients computed successfully\n","Step 3560: Loss: 0.1664, requires_grad: True\n","Step 3570: Loss: 0.4510, requires_grad: True\n","Step 3580: Loss: 0.1826, requires_grad: True\n","Step 3590: Loss: 0.1966, requires_grad: True\n","Step 3600: Loss: 0.1507, requires_grad: True\n","Step 3600: Gradients computed successfully\n","Step 3610: Loss: 0.2310, requires_grad: True\n","Step 3620: Loss: 0.2389, requires_grad: True\n","Step 3630: Loss: 0.5569, requires_grad: True\n","Step 3640: Loss: 0.1827, requires_grad: True\n","Step 3650: Loss: 0.2877, requires_grad: True\n","Step 3650: Gradients computed successfully\n","Step 3660: Loss: 0.3004, requires_grad: True\n","Step 3670: Loss: 0.1982, requires_grad: True\n","Step 3680: Loss: 0.2874, requires_grad: True\n","Step 3690: Loss: 0.1805, requires_grad: True\n","Step 3700: Loss: 0.3366, requires_grad: True\n","Step 3700: Gradients computed successfully\n","Step 3710: Loss: 0.2501, requires_grad: True\n","Step 3720: Loss: 0.7078, requires_grad: True\n","Step 3730: Loss: 0.2442, requires_grad: True\n","Step 3740: Loss: 0.1571, requires_grad: True\n","Step 3750: Loss: 0.3019, requires_grad: True\n","Step 3750: Gradients computed successfully\n","Step 3760: Loss: 0.3759, requires_grad: True\n","Step 3770: Loss: 0.1561, requires_grad: True\n","Step 3780: Loss: 0.4845, requires_grad: True\n","Step 3790: Loss: 0.1972, requires_grad: True\n","Step 3800: Loss: 0.2600, requires_grad: True\n","Step 3800: Gradients computed successfully\n","Step 3810: Loss: 0.1752, requires_grad: True\n","Step 3820: Loss: 0.3290, requires_grad: True\n","Step 3830: Loss: 0.2122, requires_grad: True\n","Step 3840: Loss: 0.7743, requires_grad: True\n","Step 3850: Loss: 0.3147, requires_grad: True\n","Step 3850: Gradients computed successfully\n","Step 3860: Loss: 0.1891, requires_grad: True\n","Step 3870: Loss: 0.8177, requires_grad: True\n","Step 3880: Loss: 0.1758, requires_grad: True\n","Step 3890: Loss: 0.2428, requires_grad: True\n","Step 3900: Loss: 0.2265, requires_grad: True\n","Step 3900: Gradients computed successfully\n","Step 3910: Loss: 0.3129, requires_grad: True\n","Step 3920: Loss: 0.3409, requires_grad: True\n","Step 3930: Loss: 0.2336, requires_grad: True\n","Step 3940: Loss: 0.1717, requires_grad: True\n","Step 3950: Loss: 0.2776, requires_grad: True\n","Step 3950: Gradients computed successfully\n","Step 3960: Loss: 0.2227, requires_grad: True\n","Step 3970: Loss: 0.3278, requires_grad: True\n","Step 3980: Loss: 0.2691, requires_grad: True\n","Step 3990: Loss: 0.5387, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-500\n","Step 4000: Loss: 0.3041, requires_grad: True\n","Step 4000: Gradients computed successfully\n","Step 4010: Loss: 0.2142, requires_grad: True\n","Step 4020: Loss: 0.3513, requires_grad: True\n","Step 4030: Loss: 0.1627, requires_grad: True\n","Step 4040: Loss: 0.3636, requires_grad: True\n","Step 4050: Loss: 0.4682, requires_grad: True\n","Step 4050: Gradients computed successfully\n","Step 4060: Loss: 0.3960, requires_grad: True\n","Step 4070: Loss: 0.2380, requires_grad: True\n","Step 4080: Loss: 0.4529, requires_grad: True\n","Step 4090: Loss: 0.7437, requires_grad: True\n","Step 4100: Loss: 0.3577, requires_grad: True\n","Step 4100: Gradients computed successfully\n","Step 4110: Loss: 0.2457, requires_grad: True\n","Step 4120: Loss: 0.2154, requires_grad: True\n","Step 4130: Loss: 0.2131, requires_grad: True\n","Step 4140: Loss: 0.1728, requires_grad: True\n","Step 4150: Loss: 0.2806, requires_grad: True\n","Step 4150: Gradients computed successfully\n","Step 4160: Loss: 0.3967, requires_grad: True\n","Step 4170: Loss: 0.2098, requires_grad: True\n","Step 4180: Loss: 0.7140, requires_grad: True\n","Step 4190: Loss: 0.2950, requires_grad: True\n","Step 4200: Loss: 0.2715, requires_grad: True\n","Step 4200: Gradients computed successfully\n","Step 4210: Loss: 0.5022, requires_grad: True\n","Step 4220: Loss: 0.3487, requires_grad: True\n","Step 4230: Loss: 0.1843, requires_grad: True\n","Step 4240: Loss: 0.1798, requires_grad: True\n","Step 4250: Loss: 0.2252, requires_grad: True\n","Step 4250: Gradients computed successfully\n","Step 4260: Loss: 0.2190, requires_grad: True\n","Step 4270: Loss: 0.3148, requires_grad: True\n","Step 4280: Loss: 0.3557, requires_grad: True\n","Step 4290: Loss: 0.2931, requires_grad: True\n","Step 4300: Loss: 0.2405, requires_grad: True\n","Step 4300: Gradients computed successfully\n","Step 4310: Loss: 0.4045, requires_grad: True\n","Step 4320: Loss: 0.2908, requires_grad: True\n","Step 4330: Loss: 0.2202, requires_grad: True\n","Step 4340: Loss: 0.3550, requires_grad: True\n","Step 4350: Loss: 0.2855, requires_grad: True\n","Step 4350: Gradients computed successfully\n","Step 4360: Loss: 0.1937, requires_grad: True\n","Step 4370: Loss: 0.2153, requires_grad: True\n","Step 4380: Loss: 0.4089, requires_grad: True\n","Step 4390: Loss: 0.3950, requires_grad: True\n","Step 4400: Loss: 0.3844, requires_grad: True\n","Step 4400: Gradients computed successfully\n","Step 4410: Loss: 0.1238, requires_grad: True\n","Step 4420: Loss: 0.7615, requires_grad: True\n","Step 4430: Loss: 0.1973, requires_grad: True\n","Step 4440: Loss: 0.2617, requires_grad: True\n","Step 4450: Loss: 0.1538, requires_grad: True\n","Step 4450: Gradients computed successfully\n","Step 4460: Loss: 0.3081, requires_grad: True\n","Step 4470: Loss: 0.5853, requires_grad: True\n","Step 4480: Loss: 0.4212, requires_grad: True\n","Step 4490: Loss: 0.2600, requires_grad: True\n","Step 4500: Loss: 0.3106, requires_grad: True\n","Step 4500: Gradients computed successfully\n","Step 4510: Loss: 0.3748, requires_grad: True\n","Step 4520: Loss: 0.2727, requires_grad: True\n","Step 4530: Loss: 0.3056, requires_grad: True\n","Step 4540: Loss: 0.4872, requires_grad: True\n","Step 4550: Loss: 0.6051, requires_grad: True\n","Step 4550: Gradients computed successfully\n","Step 4560: Loss: 0.2150, requires_grad: True\n","Step 4570: Loss: 0.1851, requires_grad: True\n","Step 4580: Loss: 0.1858, requires_grad: True\n","Step 4590: Loss: 0.4583, requires_grad: True\n","Step 4600: Loss: 0.2897, requires_grad: True\n","Step 4600: Gradients computed successfully\n","Step 4610: Loss: 0.2367, requires_grad: True\n","Step 4620: Loss: 0.3237, requires_grad: True\n","Step 4630: Loss: 0.3285, requires_grad: True\n","Step 4640: Loss: 0.7875, requires_grad: True\n","Step 4650: Loss: 0.1039, requires_grad: True\n","Step 4650: Gradients computed successfully\n","Step 4660: Loss: 0.1584, requires_grad: True\n","Step 4670: Loss: 0.1344, requires_grad: True\n","Step 4680: Loss: 0.2180, requires_grad: True\n","Step 4690: Loss: 0.4417, requires_grad: True\n","Step 4700: Loss: 0.3229, requires_grad: True\n","Step 4700: Gradients computed successfully\n","Step 4710: Loss: 0.1731, requires_grad: True\n","Step 4720: Loss: 0.3133, requires_grad: True\n","Step 4730: Loss: 0.1514, requires_grad: True\n","Step 4740: Loss: 0.3832, requires_grad: True\n","Step 4750: Loss: 0.2495, requires_grad: True\n","Step 4750: Gradients computed successfully\n","Step 4760: Loss: 0.2417, requires_grad: True\n","Step 4770: Loss: 0.3198, requires_grad: True\n","Step 4780: Loss: 0.1708, requires_grad: True\n","Step 4790: Loss: 0.3551, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-600\n","Step 4800: Loss: 0.3495, requires_grad: True\n","Step 4800: Gradients computed successfully\n","Step 4810: Loss: 0.2279, requires_grad: True\n","Step 4820: Loss: 0.4187, requires_grad: True\n","Step 4830: Loss: 0.8724, requires_grad: True\n","Step 4840: Loss: 0.0753, requires_grad: True\n","Step 4850: Loss: 0.1815, requires_grad: True\n","Step 4850: Gradients computed successfully\n","Step 4860: Loss: 0.3190, requires_grad: True\n","Step 4870: Loss: 0.2322, requires_grad: True\n","Step 4880: Loss: 0.3539, requires_grad: True\n","Step 4890: Loss: 0.3255, requires_grad: True\n","Step 4900: Loss: 0.2977, requires_grad: True\n","Step 4900: Gradients computed successfully\n","Step 4910: Loss: 0.1638, requires_grad: True\n","Step 4920: Loss: 0.1359, requires_grad: True\n","Step 4930: Loss: 0.2216, requires_grad: True\n","Step 4940: Loss: 0.5065, requires_grad: True\n","Step 4950: Loss: 0.3144, requires_grad: True\n","Step 4950: Gradients computed successfully\n","Step 4960: Loss: 0.1956, requires_grad: True\n","Step 4970: Loss: 0.2336, requires_grad: True\n","Step 4980: Loss: 0.3141, requires_grad: True\n","Step 4990: Loss: 0.1006, requires_grad: True\n","Step 5000: Loss: 0.2508, requires_grad: True\n","Step 5000: Gradients computed successfully\n","Step 5010: Loss: 0.1491, requires_grad: True\n","Step 5020: Loss: 0.1886, requires_grad: True\n","Step 5030: Loss: 0.3485, requires_grad: True\n","Step 5040: Loss: 0.2666, requires_grad: True\n","Step 5050: Loss: 0.2902, requires_grad: True\n","Step 5050: Gradients computed successfully\n","Step 5060: Loss: 0.3565, requires_grad: True\n","Step 5070: Loss: 0.3292, requires_grad: True\n","Step 5080: Loss: 0.4284, requires_grad: True\n","Step 5090: Loss: 0.2095, requires_grad: True\n","Step 5100: Loss: 0.0834, requires_grad: True\n","Step 5100: Gradients computed successfully\n","Step 5110: Loss: 0.3550, requires_grad: True\n","Step 5120: Loss: 0.4043, requires_grad: True\n","Step 5130: Loss: 0.2789, requires_grad: True\n","Step 5140: Loss: 0.2103, requires_grad: True\n","Step 5150: Loss: 0.1998, requires_grad: True\n","Step 5150: Gradients computed successfully\n","Step 5160: Loss: 0.2657, requires_grad: True\n","Step 5170: Loss: 0.1593, requires_grad: True\n","Step 5180: Loss: 0.2199, requires_grad: True\n","Step 5190: Loss: 0.2759, requires_grad: True\n","Step 5200: Loss: 0.4813, requires_grad: True\n","Step 5200: Gradients computed successfully\n","Step 5210: Loss: 0.2163, requires_grad: True\n","Step 5220: Loss: 0.5564, requires_grad: True\n","Step 5230: Loss: 0.5288, requires_grad: True\n","Step 5240: Loss: 0.2592, requires_grad: True\n","Step 5250: Loss: 0.2745, requires_grad: True\n","Step 5250: Gradients computed successfully\n","Step 5260: Loss: 0.1534, requires_grad: True\n","Step 5270: Loss: 0.1651, requires_grad: True\n","Step 5280: Loss: 0.2647, requires_grad: True\n","Step 5290: Loss: 0.1352, requires_grad: True\n","Step 5300: Loss: 0.1683, requires_grad: True\n","Step 5300: Gradients computed successfully\n","Step 5310: Loss: 0.5316, requires_grad: True\n","Step 5320: Loss: 0.1683, requires_grad: True\n","Step 5330: Loss: 0.4172, requires_grad: True\n","Step 5340: Loss: 0.2855, requires_grad: True\n","Step 5350: Loss: 0.3283, requires_grad: True\n","Step 5350: Gradients computed successfully\n","Step 5360: Loss: 0.1467, requires_grad: True\n","Step 5370: Loss: 0.2331, requires_grad: True\n","Step 5380: Loss: 0.5200, requires_grad: True\n","Step 5390: Loss: 0.1915, requires_grad: True\n","Step 5400: Loss: 0.3947, requires_grad: True\n","Step 5400: Gradients computed successfully\n","Step 5410: Loss: 0.2050, requires_grad: True\n","Step 5420: Loss: 0.3655, requires_grad: True\n","Step 5430: Loss: 0.1412, requires_grad: True\n","Step 5440: Loss: 0.2226, requires_grad: True\n","Step 5450: Loss: 0.2129, requires_grad: True\n","Step 5450: Gradients computed successfully\n","Step 5460: Loss: 0.3267, requires_grad: True\n","Step 5470: Loss: 0.1799, requires_grad: True\n","Step 5480: Loss: 0.3189, requires_grad: True\n","Step 5490: Loss: 0.3561, requires_grad: True\n","Step 5500: Loss: 0.1745, requires_grad: True\n","Step 5500: Gradients computed successfully\n","Step 5510: Loss: 0.0986, requires_grad: True\n","Step 5520: Loss: 0.3295, requires_grad: True\n","Step 5530: Loss: 0.1821, requires_grad: True\n","Step 5540: Loss: 0.2136, requires_grad: True\n","Step 5550: Loss: 0.6134, requires_grad: True\n","Step 5550: Gradients computed successfully\n","Step 5560: Loss: 0.4260, requires_grad: True\n","Step 5570: Loss: 0.1877, requires_grad: True\n","Step 5580: Loss: 0.1995, requires_grad: True\n","Step 5590: Loss: 0.2422, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-700\n","Step 5600: Loss: 0.3455, requires_grad: True\n","Step 5600: Gradients computed successfully\n","Step 5610: Loss: 0.2109, requires_grad: True\n","Step 5620: Loss: 0.3812, requires_grad: True\n","Step 5630: Loss: 0.1730, requires_grad: True\n","Step 5640: Loss: 0.3263, requires_grad: True\n","Step 5650: Loss: 0.5739, requires_grad: True\n","Step 5650: Gradients computed successfully\n","Step 5660: Loss: 0.2784, requires_grad: True\n","Step 5670: Loss: 0.1478, requires_grad: True\n","Step 5680: Loss: 0.2846, requires_grad: True\n","Step 5690: Loss: 0.1189, requires_grad: True\n","Step 5700: Loss: 0.2789, requires_grad: True\n","Step 5700: Gradients computed successfully\n","Step 5710: Loss: 0.4629, requires_grad: True\n","Step 5720: Loss: 0.2378, requires_grad: True\n","Step 5730: Loss: 0.2210, requires_grad: True\n","Step 5740: Loss: 0.3140, requires_grad: True\n","Step 5750: Loss: 0.2571, requires_grad: True\n","Step 5750: Gradients computed successfully\n","Step 5760: Loss: 0.2736, requires_grad: True\n","Step 5770: Loss: 0.3664, requires_grad: True\n","Step 5780: Loss: 0.0851, requires_grad: True\n","Step 5790: Loss: 0.2583, requires_grad: True\n","Step 5800: Loss: 0.2978, requires_grad: True\n","Step 5800: Gradients computed successfully\n","Step 5810: Loss: 0.3999, requires_grad: True\n","Step 5820: Loss: 0.3322, requires_grad: True\n","Step 5830: Loss: 0.1175, requires_grad: True\n","Step 5840: Loss: 0.2924, requires_grad: True\n","Step 5850: Loss: 0.3171, requires_grad: True\n","Step 5850: Gradients computed successfully\n","Step 5860: Loss: 0.2587, requires_grad: True\n","Step 5870: Loss: 0.4118, requires_grad: True\n","Step 5880: Loss: 0.2944, requires_grad: True\n","Step 5890: Loss: 0.3090, requires_grad: True\n","Step 5900: Loss: 0.0984, requires_grad: True\n","Step 5900: Gradients computed successfully\n","Step 5910: Loss: 0.6528, requires_grad: True\n","Step 5920: Loss: 0.2692, requires_grad: True\n","Step 5930: Loss: 0.2845, requires_grad: True\n","Step 5940: Loss: 0.4297, requires_grad: True\n","Step 5950: Loss: 0.2388, requires_grad: True\n","Step 5950: Gradients computed successfully\n","Step 5960: Loss: 0.2308, requires_grad: True\n","Step 5970: Loss: 0.5943, requires_grad: True\n","Step 5980: Loss: 0.4194, requires_grad: True\n","Step 5990: Loss: 0.2437, requires_grad: True\n","Step 6000: Loss: 0.3598, requires_grad: True\n","Step 6000: Gradients computed successfully\n","Step 6010: Loss: 0.2104, requires_grad: True\n","Step 6020: Loss: 0.2536, requires_grad: True\n","Step 6030: Loss: 0.5182, requires_grad: True\n","Step 6040: Loss: 0.3017, requires_grad: True\n","Step 6050: Loss: 0.1055, requires_grad: True\n","Step 6050: Gradients computed successfully\n","Step 6060: Loss: 0.1925, requires_grad: True\n","Step 6070: Loss: 0.2603, requires_grad: True\n","Step 6080: Loss: 0.4411, requires_grad: True\n","Step 6090: Loss: 0.5332, requires_grad: True\n","Step 6100: Loss: 0.2543, requires_grad: True\n","Step 6100: Gradients computed successfully\n","Step 6110: Loss: 0.3126, requires_grad: True\n","Step 6120: Loss: 0.0642, requires_grad: True\n","Step 6130: Loss: 0.4826, requires_grad: True\n","Step 6140: Loss: 0.4823, requires_grad: True\n","Step 6150: Loss: 0.3421, requires_grad: True\n","Step 6150: Gradients computed successfully\n","Step 6160: Loss: 0.1831, requires_grad: True\n","Step 6170: Loss: 0.3215, requires_grad: True\n","Step 6180: Loss: 0.1465, requires_grad: True\n","Step 6190: Loss: 0.4353, requires_grad: True\n","Step 6200: Loss: 0.2341, requires_grad: True\n","Step 6200: Gradients computed successfully\n","Step 6210: Loss: 0.3540, requires_grad: True\n","Step 6220: Loss: 0.2520, requires_grad: True\n","Step 6230: Loss: 0.3216, requires_grad: True\n","Step 6240: Loss: 0.2385, requires_grad: True\n","Step 6250: Loss: 0.1530, requires_grad: True\n","Step 6250: Gradients computed successfully\n","Step 6260: Loss: 0.3416, requires_grad: True\n","Step 6270: Loss: 0.3213, requires_grad: True\n","Step 6280: Loss: 0.4846, requires_grad: True\n","Step 6290: Loss: 0.4505, requires_grad: True\n","Step 6300: Loss: 0.3718, requires_grad: True\n","Step 6300: Gradients computed successfully\n","Step 6310: Loss: 0.2586, requires_grad: True\n","Step 6320: Loss: 0.1366, requires_grad: True\n","Step 6330: Loss: 0.5150, requires_grad: True\n","Step 6340: Loss: 0.3154, requires_grad: True\n","Step 6350: Loss: 0.7585, requires_grad: True\n","Step 6350: Gradients computed successfully\n","Step 6360: Loss: 0.3312, requires_grad: True\n","Step 6370: Loss: 0.2999, requires_grad: True\n","Step 6380: Loss: 0.3787, requires_grad: True\n","Step 6390: Loss: 0.1536, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-800\n","Step 6400: Loss: 0.1796, requires_grad: True\n","Step 6400: Gradients computed successfully\n","Step 6410: Loss: 0.6555, requires_grad: True\n","Step 6420: Loss: 0.1521, requires_grad: True\n","Step 6430: Loss: 0.1949, requires_grad: True\n","Step 6440: Loss: 0.7040, requires_grad: True\n","Step 6450: Loss: 0.6517, requires_grad: True\n","Step 6450: Gradients computed successfully\n","Step 6460: Loss: 0.1336, requires_grad: True\n","Step 6470: Loss: 0.3252, requires_grad: True\n","Step 6480: Loss: 0.1833, requires_grad: True\n","Step 6490: Loss: 0.6522, requires_grad: True\n","Step 6500: Loss: 0.1242, requires_grad: True\n","Step 6500: Gradients computed successfully\n","Step 6510: Loss: 0.2595, requires_grad: True\n","Step 6520: Loss: 0.3916, requires_grad: True\n","Step 6530: Loss: 0.4680, requires_grad: True\n","Step 6540: Loss: 0.1656, requires_grad: True\n","Step 6550: Loss: 0.5201, requires_grad: True\n","Step 6550: Gradients computed successfully\n","Step 6560: Loss: 0.1959, requires_grad: True\n","Step 6570: Loss: 0.2541, requires_grad: True\n","Step 6580: Loss: 0.2145, requires_grad: True\n","Step 6590: Loss: 0.4959, requires_grad: True\n","Step 6600: Loss: 0.1761, requires_grad: True\n","Step 6600: Gradients computed successfully\n","Step 6610: Loss: 0.1989, requires_grad: True\n","Step 6620: Loss: 0.1653, requires_grad: True\n","Step 6630: Loss: 0.3279, requires_grad: True\n","Step 6640: Loss: 0.2940, requires_grad: True\n","Step 6650: Loss: 0.2222, requires_grad: True\n","Step 6650: Gradients computed successfully\n","Step 6660: Loss: 0.2275, requires_grad: True\n","Step 6670: Loss: 0.0948, requires_grad: True\n","Step 6680: Loss: 0.3026, requires_grad: True\n","Step 6690: Loss: 0.2113, requires_grad: True\n","Step 6700: Loss: 0.4731, requires_grad: True\n","Step 6700: Gradients computed successfully\n","Step 6710: Loss: 0.2455, requires_grad: True\n","Step 6720: Loss: 0.2358, requires_grad: True\n","Step 6730: Loss: 0.1296, requires_grad: True\n","Step 6740: Loss: 0.4417, requires_grad: True\n","Step 6750: Loss: 0.4653, requires_grad: True\n","Step 6750: Gradients computed successfully\n","Step 6760: Loss: 0.1777, requires_grad: True\n","Step 6770: Loss: 0.2927, requires_grad: True\n","Step 6780: Loss: 0.4075, requires_grad: True\n","Step 6790: Loss: 0.1676, requires_grad: True\n","Step 6800: Loss: 0.2724, requires_grad: True\n","Step 6800: Gradients computed successfully\n","Step 6810: Loss: 0.3542, requires_grad: True\n","Step 6820: Loss: 0.4987, requires_grad: True\n","Step 6830: Loss: 0.2281, requires_grad: True\n","Step 6840: Loss: 0.2006, requires_grad: True\n","Step 6850: Loss: 0.3988, requires_grad: True\n","Step 6850: Gradients computed successfully\n","Step 6860: Loss: 0.2750, requires_grad: True\n","Step 6870: Loss: 0.2578, requires_grad: True\n","Step 6880: Loss: 0.2434, requires_grad: True\n","Step 6890: Loss: 0.3274, requires_grad: True\n","Step 6900: Loss: 0.3777, requires_grad: True\n","Step 6900: Gradients computed successfully\n","Step 6910: Loss: 0.4044, requires_grad: True\n","Step 6920: Loss: 0.2507, requires_grad: True\n","Step 6930: Loss: 0.2770, requires_grad: True\n","Step 6940: Loss: 0.2492, requires_grad: True\n","Step 6950: Loss: 0.2399, requires_grad: True\n","Step 6950: Gradients computed successfully\n","Step 6960: Loss: 0.3005, requires_grad: True\n","Step 6970: Loss: 0.3281, requires_grad: True\n","Step 6980: Loss: 0.1042, requires_grad: True\n","Step 6990: Loss: 0.4349, requires_grad: True\n","Step 7000: Loss: 0.1507, requires_grad: True\n","Step 7000: Gradients computed successfully\n","Step 7010: Loss: 0.2408, requires_grad: True\n","Step 7020: Loss: 0.4810, requires_grad: True\n","Step 7030: Loss: 0.3583, requires_grad: True\n","Step 7040: Loss: 0.0964, requires_grad: True\n","Step 7050: Loss: 0.4228, requires_grad: True\n","Step 7050: Gradients computed successfully\n","Step 7060: Loss: 0.2567, requires_grad: True\n","Step 7070: Loss: 0.1551, requires_grad: True\n","Step 7080: Loss: 0.3270, requires_grad: True\n","Step 7090: Loss: 0.3085, requires_grad: True\n","Step 7100: Loss: 0.3416, requires_grad: True\n","Step 7100: Gradients computed successfully\n","Step 7110: Loss: 0.1143, requires_grad: True\n","Step 7120: Loss: 0.3181, requires_grad: True\n","Step 7130: Loss: 0.2338, requires_grad: True\n","Step 7140: Loss: 0.6575, requires_grad: True\n","Step 7150: Loss: 0.2218, requires_grad: True\n","Step 7150: Gradients computed successfully\n","Step 7160: Loss: 0.2573, requires_grad: True\n","Step 7170: Loss: 0.2028, requires_grad: True\n","Step 7180: Loss: 0.2588, requires_grad: True\n","Step 7190: Loss: 0.2179, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-900\n","Step 7200: Loss: 0.1457, requires_grad: True\n","Step 7200: Gradients computed successfully\n","Step 7210: Loss: 0.2200, requires_grad: True\n","Step 7220: Loss: 0.3145, requires_grad: True\n","Step 7230: Loss: 0.2121, requires_grad: True\n","Step 7240: Loss: 0.2245, requires_grad: True\n","Step 7250: Loss: 0.3639, requires_grad: True\n","Step 7250: Gradients computed successfully\n","Step 7260: Loss: 0.4107, requires_grad: True\n","Step 7270: Loss: 0.1772, requires_grad: True\n","Step 7280: Loss: 0.1980, requires_grad: True\n","Step 7290: Loss: 0.1819, requires_grad: True\n","Step 7300: Loss: 0.2504, requires_grad: True\n","Step 7300: Gradients computed successfully\n","Step 7310: Loss: 0.2623, requires_grad: True\n","Step 7320: Loss: 0.2774, requires_grad: True\n","Step 7330: Loss: 0.3385, requires_grad: True\n","Step 7340: Loss: 0.2712, requires_grad: True\n","Step 7350: Loss: 0.2996, requires_grad: True\n","Step 7350: Gradients computed successfully\n","Step 7360: Loss: 0.1694, requires_grad: True\n","Step 7370: Loss: 0.5233, requires_grad: True\n","Step 7380: Loss: 0.1685, requires_grad: True\n","Step 7390: Loss: 0.1199, requires_grad: True\n","Step 7400: Loss: 0.1374, requires_grad: True\n","Step 7400: Gradients computed successfully\n","Step 7410: Loss: 0.2366, requires_grad: True\n","Step 7420: Loss: 0.2082, requires_grad: True\n","Step 7430: Loss: 0.2505, requires_grad: True\n","Step 7440: Loss: 0.2296, requires_grad: True\n","Step 7450: Loss: 0.4260, requires_grad: True\n","Step 7450: Gradients computed successfully\n","Step 7460: Loss: 0.1945, requires_grad: True\n","Step 7470: Loss: 0.5401, requires_grad: True\n","Step 7480: Loss: 0.5158, requires_grad: True\n","Step 7490: Loss: 0.2983, requires_grad: True\n","Step 7500: Loss: 0.2066, requires_grad: True\n","Step 7500: Gradients computed successfully\n","Step 7510: Loss: 0.2139, requires_grad: True\n","Step 7520: Loss: 0.1680, requires_grad: True\n","Step 7530: Loss: 0.3798, requires_grad: True\n","Step 7540: Loss: 0.3382, requires_grad: True\n","Step 7550: Loss: 0.6495, requires_grad: True\n","Step 7550: Gradients computed successfully\n","Step 7560: Loss: 0.1704, requires_grad: True\n","Step 7570: Loss: 0.4192, requires_grad: True\n","Step 7580: Loss: 0.3100, requires_grad: True\n","Step 7590: Loss: 0.1620, requires_grad: True\n","Step 7600: Loss: 0.2512, requires_grad: True\n","Step 7600: Gradients computed successfully\n","Step 7610: Loss: 0.4208, requires_grad: True\n","Step 7620: Loss: 0.3072, requires_grad: True\n","Step 7630: Loss: 0.3004, requires_grad: True\n","Step 7640: Loss: 0.1887, requires_grad: True\n","Step 7650: Loss: 0.4832, requires_grad: True\n","Step 7650: Gradients computed successfully\n","Step 7660: Loss: 0.1219, requires_grad: True\n","Step 7670: Loss: 0.3546, requires_grad: True\n","Step 7680: Loss: 0.2921, requires_grad: True\n","Step 7690: Loss: 0.2873, requires_grad: True\n","Step 7700: Loss: 0.1571, requires_grad: True\n","Step 7700: Gradients computed successfully\n","Step 7710: Loss: 0.3945, requires_grad: True\n","Step 7720: Loss: 0.3138, requires_grad: True\n","Step 7730: Loss: 0.1827, requires_grad: True\n","Step 7740: Loss: 0.2506, requires_grad: True\n","Step 7750: Loss: 0.4017, requires_grad: True\n","Step 7750: Gradients computed successfully\n","Step 7760: Loss: 0.2838, requires_grad: True\n","Step 7770: Loss: 0.2483, requires_grad: True\n","Step 7780: Loss: 0.2169, requires_grad: True\n","Step 7790: Loss: 0.6890, requires_grad: True\n","Step 7800: Loss: 0.4294, requires_grad: True\n","Step 7800: Gradients computed successfully\n","Step 7810: Loss: 0.2654, requires_grad: True\n","Step 7820: Loss: 0.2625, requires_grad: True\n","Step 7830: Loss: 0.4098, requires_grad: True\n","Step 7840: Loss: 0.1944, requires_grad: True\n","Step 7850: Loss: 0.6000, requires_grad: True\n","Step 7850: Gradients computed successfully\n","Step 7860: Loss: 0.3172, requires_grad: True\n","Step 7870: Loss: 0.3020, requires_grad: True\n","Step 7880: Loss: 0.4118, requires_grad: True\n","Step 7890: Loss: 0.1895, requires_grad: True\n","Step 7900: Loss: 0.1337, requires_grad: True\n","Step 7900: Gradients computed successfully\n","Step 7910: Loss: 0.4065, requires_grad: True\n","Step 7920: Loss: 0.2085, requires_grad: True\n","Step 7930: Loss: 0.2369, requires_grad: True\n","Step 7940: Loss: 0.2208, requires_grad: True\n","Step 7950: Loss: 0.4002, requires_grad: True\n","Step 7950: Gradients computed successfully\n","Step 7960: Loss: 0.3522, requires_grad: True\n","Step 7970: Loss: 0.1476, requires_grad: True\n","Step 7980: Loss: 0.3512, requires_grad: True\n","Step 7990: Loss: 0.1861, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-1000\n","Step 8000: Loss: 0.1698, requires_grad: True\n","Step 8000: Gradients computed successfully\n","Step 8010: Loss: 0.3535, requires_grad: True\n","Step 8020: Loss: 0.6056, requires_grad: True\n","Step 8030: Loss: 0.1831, requires_grad: True\n","Step 8040: Loss: 0.2796, requires_grad: True\n","Step 8050: Loss: 0.3469, requires_grad: True\n","Step 8050: Gradients computed successfully\n","Step 8060: Loss: 0.2147, requires_grad: True\n","Step 8070: Loss: 0.2333, requires_grad: True\n","Step 8080: Loss: 0.3156, requires_grad: True\n","Step 8090: Loss: 0.4809, requires_grad: True\n","Step 8100: Loss: 0.1956, requires_grad: True\n","Step 8100: Gradients computed successfully\n","Step 8110: Loss: 0.3790, requires_grad: True\n","Step 8120: Loss: 0.1284, requires_grad: True\n","Step 8130: Loss: 0.4859, requires_grad: True\n","Step 8140: Loss: 0.2119, requires_grad: True\n","Step 8150: Loss: 0.2413, requires_grad: True\n","Step 8150: Gradients computed successfully\n","Step 8160: Loss: 0.1269, requires_grad: True\n","Step 8170: Loss: 0.2655, requires_grad: True\n","Step 8180: Loss: 0.1366, requires_grad: True\n","Step 8190: Loss: 0.4031, requires_grad: True\n","Step 8200: Loss: 0.4722, requires_grad: True\n","Step 8200: Gradients computed successfully\n","Step 8210: Loss: 0.2770, requires_grad: True\n","Step 8220: Loss: 0.3154, requires_grad: True\n","Step 8230: Loss: 0.3273, requires_grad: True\n","Step 8240: Loss: 0.1769, requires_grad: True\n","Step 8250: Loss: 0.1948, requires_grad: True\n","Step 8250: Gradients computed successfully\n","Step 8260: Loss: 0.3942, requires_grad: True\n","Step 8270: Loss: 0.1553, requires_grad: True\n","Step 8280: Loss: 0.2929, requires_grad: True\n","Step 8290: Loss: 0.3283, requires_grad: True\n","Step 8300: Loss: 0.1889, requires_grad: True\n","Step 8300: Gradients computed successfully\n","Step 8310: Loss: 0.1924, requires_grad: True\n","Step 8320: Loss: 0.3751, requires_grad: True\n","Step 8330: Loss: 0.2919, requires_grad: True\n","Step 8340: Loss: 0.1966, requires_grad: True\n","Step 8350: Loss: 0.2974, requires_grad: True\n","Step 8350: Gradients computed successfully\n","Step 8360: Loss: 0.3042, requires_grad: True\n","Step 8370: Loss: 0.1680, requires_grad: True\n","Step 8380: Loss: 0.1452, requires_grad: True\n","Step 8390: Loss: 0.3064, requires_grad: True\n","Step 8400: Loss: 0.3582, requires_grad: True\n","Step 8400: Gradients computed successfully\n","Step 8410: Loss: 0.1496, requires_grad: True\n","Step 8420: Loss: 0.2414, requires_grad: True\n","Step 8430: Loss: 0.5572, requires_grad: True\n","Step 8440: Loss: 0.1276, requires_grad: True\n","Step 8450: Loss: 0.2385, requires_grad: True\n","Step 8450: Gradients computed successfully\n","Step 8460: Loss: 0.1840, requires_grad: True\n","Step 8470: Loss: 0.1547, requires_grad: True\n","Step 8480: Loss: 0.1222, requires_grad: True\n","Step 8490: Loss: 0.2203, requires_grad: True\n","Step 8500: Loss: 0.2775, requires_grad: True\n","Step 8500: Gradients computed successfully\n","Step 8510: Loss: 0.4318, requires_grad: True\n","Step 8520: Loss: 0.4722, requires_grad: True\n","Step 8530: Loss: 0.1767, requires_grad: True\n","Step 8540: Loss: 0.4272, requires_grad: True\n","Step 8550: Loss: 0.3343, requires_grad: True\n","Step 8550: Gradients computed successfully\n","Step 8560: Loss: 0.1484, requires_grad: True\n","Step 8570: Loss: 0.1537, requires_grad: True\n","Step 8580: Loss: 0.3983, requires_grad: True\n","Step 8590: Loss: 0.4010, requires_grad: True\n","Step 8600: Loss: 0.1675, requires_grad: True\n","Step 8600: Gradients computed successfully\n","Step 8610: Loss: 0.2969, requires_grad: True\n","Step 8620: Loss: 0.2601, requires_grad: True\n","Step 8630: Loss: 0.1363, requires_grad: True\n","Step 8640: Loss: 0.3202, requires_grad: True\n","Step 8650: Loss: 0.7030, requires_grad: True\n","Step 8650: Gradients computed successfully\n","Step 8660: Loss: 0.1887, requires_grad: True\n","Step 8670: Loss: 0.4271, requires_grad: True\n","Step 8680: Loss: 0.2024, requires_grad: True\n","Step 8690: Loss: 0.1510, requires_grad: True\n","Step 8700: Loss: 0.1656, requires_grad: True\n","Step 8700: Gradients computed successfully\n","Step 8710: Loss: 0.2021, requires_grad: True\n","Step 8720: Loss: 0.5159, requires_grad: True\n","Step 8730: Loss: 0.5387, requires_grad: True\n","Step 8740: Loss: 0.2859, requires_grad: True\n","Step 8750: Loss: 0.1778, requires_grad: True\n","Step 8750: Gradients computed successfully\n","Step 8760: Loss: 0.2472, requires_grad: True\n","Step 8770: Loss: 0.1135, requires_grad: True\n","Step 8780: Loss: 0.1546, requires_grad: True\n","Step 8790: Loss: 0.4688, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-1100\n","Step 8800: Loss: 0.3387, requires_grad: True\n","Step 8800: Gradients computed successfully\n","Step 8810: Loss: 0.1860, requires_grad: True\n","Step 8820: Loss: 0.0913, requires_grad: True\n","Step 8830: Loss: 0.4905, requires_grad: True\n","Step 8840: Loss: 0.2950, requires_grad: True\n","Step 8850: Loss: 0.1972, requires_grad: True\n","Step 8850: Gradients computed successfully\n","Step 8860: Loss: 0.3307, requires_grad: True\n","Step 8870: Loss: 0.4130, requires_grad: True\n","Step 8880: Loss: 0.1489, requires_grad: True\n","Step 8890: Loss: 0.1460, requires_grad: True\n","Step 8900: Loss: 0.2126, requires_grad: True\n","Step 8900: Gradients computed successfully\n","Step 8910: Loss: 0.1640, requires_grad: True\n","Step 8920: Loss: 0.2677, requires_grad: True\n","Step 8930: Loss: 0.1966, requires_grad: True\n","Step 8940: Loss: 0.2070, requires_grad: True\n","Step 8950: Loss: 0.2404, requires_grad: True\n","Step 8950: Gradients computed successfully\n","Step 8960: Loss: 0.1595, requires_grad: True\n","Step 8970: Loss: 0.0899, requires_grad: True\n","Step 8980: Loss: 0.2471, requires_grad: True\n","Step 8990: Loss: 0.1518, requires_grad: True\n","Step 9000: Loss: 0.2128, requires_grad: True\n","Step 9000: Gradients computed successfully\n","Step 9010: Loss: 0.3132, requires_grad: True\n","Step 9020: Loss: 0.2313, requires_grad: True\n","Step 9030: Loss: 0.1853, requires_grad: True\n","Step 9040: Loss: 0.1958, requires_grad: True\n","Step 9050: Loss: 0.1611, requires_grad: True\n","Step 9050: Gradients computed successfully\n","Step 9060: Loss: 0.7148, requires_grad: True\n","Step 9070: Loss: 0.3266, requires_grad: True\n","Step 9080: Loss: 0.1706, requires_grad: True\n","Step 9090: Loss: 0.6247, requires_grad: True\n","Step 9100: Loss: 0.2830, requires_grad: True\n","Step 9100: Gradients computed successfully\n","Step 9110: Loss: 0.4468, requires_grad: True\n","Step 9120: Loss: 0.1450, requires_grad: True\n","Step 9130: Loss: 0.1231, requires_grad: True\n","Step 9140: Loss: 0.2806, requires_grad: True\n","Step 9150: Loss: 0.4945, requires_grad: True\n","Step 9150: Gradients computed successfully\n","Step 9160: Loss: 0.2448, requires_grad: True\n","Step 9170: Loss: 0.3430, requires_grad: True\n","Step 9180: Loss: 0.1808, requires_grad: True\n","Step 9190: Loss: 0.2314, requires_grad: True\n","Step 9200: Loss: 0.1324, requires_grad: True\n","Step 9200: Gradients computed successfully\n","Step 9210: Loss: 0.2319, requires_grad: True\n","Step 9220: Loss: 0.1985, requires_grad: True\n","Step 9230: Loss: 0.1267, requires_grad: True\n","Step 9240: Loss: 0.2511, requires_grad: True\n","Step 9250: Loss: 0.1520, requires_grad: True\n","Step 9250: Gradients computed successfully\n","Step 9260: Loss: 0.1899, requires_grad: True\n","Step 9270: Loss: 0.6029, requires_grad: True\n","Step 9280: Loss: 0.2383, requires_grad: True\n","Step 9290: Loss: 0.2309, requires_grad: True\n","Step 9300: Loss: 0.2350, requires_grad: True\n","Step 9300: Gradients computed successfully\n","Step 9310: Loss: 0.3948, requires_grad: True\n","Step 9320: Loss: 0.1425, requires_grad: True\n","Step 9330: Loss: 0.1384, requires_grad: True\n","Step 9340: Loss: 0.1270, requires_grad: True\n","Step 9350: Loss: 0.1294, requires_grad: True\n","Step 9350: Gradients computed successfully\n","Step 9360: Loss: 0.2342, requires_grad: True\n","Step 9370: Loss: 0.1700, requires_grad: True\n","Step 9380: Loss: 0.1446, requires_grad: True\n","Step 9390: Loss: 0.3087, requires_grad: True\n","Step 9400: Loss: 0.4353, requires_grad: True\n","Step 9400: Gradients computed successfully\n","Step 9410: Loss: 0.2967, requires_grad: True\n","Step 9420: Loss: 0.2416, requires_grad: True\n","Step 9430: Loss: 0.2301, requires_grad: True\n","Step 9440: Loss: 0.2067, requires_grad: True\n","Step 9450: Loss: 0.1394, requires_grad: True\n","Step 9450: Gradients computed successfully\n","Step 9460: Loss: 0.2987, requires_grad: True\n","Step 9470: Loss: 0.2884, requires_grad: True\n","Step 9480: Loss: 0.1169, requires_grad: True\n","Step 9490: Loss: 0.0938, requires_grad: True\n","Step 9500: Loss: 0.2383, requires_grad: True\n","Step 9500: Gradients computed successfully\n","Step 9510: Loss: 0.1189, requires_grad: True\n","Step 9520: Loss: 0.6671, requires_grad: True\n","Step 9530: Loss: 0.3634, requires_grad: True\n","Step 9540: Loss: 0.6037, requires_grad: True\n","Step 9550: Loss: 0.4242, requires_grad: True\n","Step 9550: Gradients computed successfully\n","Step 9560: Loss: 0.2863, requires_grad: True\n","Step 9570: Loss: 0.2390, requires_grad: True\n","Step 9580: Loss: 0.1196, requires_grad: True\n","Step 9590: Loss: 0.1886, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-1200\n","Step 9600: Loss: 0.1738, requires_grad: True\n","Step 9600: Gradients computed successfully\n","Step 9610: Loss: 0.3402, requires_grad: True\n","Step 9620: Loss: 0.1428, requires_grad: True\n","Step 9630: Loss: 0.2516, requires_grad: True\n","Step 9640: Loss: 0.2006, requires_grad: True\n","Step 9650: Loss: 0.2811, requires_grad: True\n","Step 9650: Gradients computed successfully\n","Step 9660: Loss: 0.2830, requires_grad: True\n","Step 9670: Loss: 0.1883, requires_grad: True\n","Step 9680: Loss: 0.2097, requires_grad: True\n","Step 9690: Loss: 0.1712, requires_grad: True\n","Step 9700: Loss: 0.2780, requires_grad: True\n","Step 9700: Gradients computed successfully\n","Step 9710: Loss: 0.2191, requires_grad: True\n","Step 9720: Loss: 0.3639, requires_grad: True\n","Step 9730: Loss: 0.3146, requires_grad: True\n","Step 9740: Loss: 0.5106, requires_grad: True\n","Step 9750: Loss: 0.2557, requires_grad: True\n","Step 9750: Gradients computed successfully\n","Step 9760: Loss: 0.2271, requires_grad: True\n","Step 9770: Loss: 0.2726, requires_grad: True\n","Step 9780: Loss: 0.2733, requires_grad: True\n","Step 9790: Loss: 0.3321, requires_grad: True\n","Step 9800: Loss: 0.2102, requires_grad: True\n","Step 9800: Gradients computed successfully\n","Step 9810: Loss: 0.2599, requires_grad: True\n","Step 9820: Loss: 0.3091, requires_grad: True\n","Step 9830: Loss: 0.3201, requires_grad: True\n","Step 9840: Loss: 0.3137, requires_grad: True\n","Step 9850: Loss: 0.2014, requires_grad: True\n","Step 9850: Gradients computed successfully\n","Step 9860: Loss: 0.1515, requires_grad: True\n","Step 9870: Loss: 0.0779, requires_grad: True\n","Step 9880: Loss: 0.1938, requires_grad: True\n","Step 9890: Loss: 0.2173, requires_grad: True\n","Step 9900: Loss: 0.3942, requires_grad: True\n","Step 9900: Gradients computed successfully\n","Step 9910: Loss: 0.1952, requires_grad: True\n","Step 9920: Loss: 0.2086, requires_grad: True\n","Step 9930: Loss: 0.2799, requires_grad: True\n","Step 9940: Loss: 0.3706, requires_grad: True\n","Step 9950: Loss: 0.1705, requires_grad: True\n","Step 9950: Gradients computed successfully\n","Step 9960: Loss: 0.0760, requires_grad: True\n","Step 9970: Loss: 0.4940, requires_grad: True\n","Step 9980: Loss: 0.1656, requires_grad: True\n","Step 9990: Loss: 0.3099, requires_grad: True\n","Step 10000: Loss: 0.4876, requires_grad: True\n","Step 10000: Gradients computed successfully\n","Step 10010: Loss: 0.6258, requires_grad: True\n","Step 10020: Loss: 0.2589, requires_grad: True\n","Step 10030: Loss: 0.1751, requires_grad: True\n","Step 10040: Loss: 0.1251, requires_grad: True\n","Step 10050: Loss: 0.2687, requires_grad: True\n","Step 10050: Gradients computed successfully\n","Step 10060: Loss: 0.2591, requires_grad: True\n","Step 10070: Loss: 0.1984, requires_grad: True\n","Step 10080: Loss: 0.1197, requires_grad: True\n","Step 10090: Loss: 0.2066, requires_grad: True\n","Step 10100: Loss: 0.2579, requires_grad: True\n","Step 10100: Gradients computed successfully\n","Step 10110: Loss: 0.1560, requires_grad: True\n","Step 10120: Loss: 0.1117, requires_grad: True\n","Step 10130: Loss: 0.3900, requires_grad: True\n","Step 10140: Loss: 0.2182, requires_grad: True\n","Step 10150: Loss: 0.1294, requires_grad: True\n","Step 10150: Gradients computed successfully\n","Step 10160: Loss: 0.1029, requires_grad: True\n","Step 10170: Loss: 0.4234, requires_grad: True\n","Step 10180: Loss: 0.2202, requires_grad: True\n","Step 10190: Loss: 0.0883, requires_grad: True\n","Step 10200: Loss: 0.2791, requires_grad: True\n","Step 10200: Gradients computed successfully\n","Step 10210: Loss: 0.2528, requires_grad: True\n","Step 10220: Loss: 0.2126, requires_grad: True\n","Step 10230: Loss: 0.2244, requires_grad: True\n","Step 10240: Loss: 0.2259, requires_grad: True\n","Step 10250: Loss: 0.3556, requires_grad: True\n","Step 10250: Gradients computed successfully\n","Step 10260: Loss: 0.2217, requires_grad: True\n","Step 10270: Loss: 0.2995, requires_grad: True\n","Step 10280: Loss: 0.1334, requires_grad: True\n","Step 10290: Loss: 0.2481, requires_grad: True\n","Step 10300: Loss: 0.2377, requires_grad: True\n","Step 10300: Gradients computed successfully\n","Step 10310: Loss: 0.1699, requires_grad: True\n","Step 10320: Loss: 0.2057, requires_grad: True\n","Step 10330: Loss: 0.2005, requires_grad: True\n","Step 10340: Loss: 0.3318, requires_grad: True\n","Step 10350: Loss: 0.2531, requires_grad: True\n","Step 10350: Gradients computed successfully\n","Step 10360: Loss: 0.1753, requires_grad: True\n","Step 10370: Loss: 0.1950, requires_grad: True\n","Step 10380: Loss: 0.2155, requires_grad: True\n","Step 10390: Loss: 0.5216, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-1300\n","Step 10400: Loss: 0.2615, requires_grad: True\n","Step 10400: Gradients computed successfully\n","Step 10410: Loss: 0.2642, requires_grad: True\n","Step 10420: Loss: 0.3994, requires_grad: True\n","Step 10430: Loss: 0.1728, requires_grad: True\n","Step 10440: Loss: 0.1422, requires_grad: True\n","Step 10450: Loss: 0.1790, requires_grad: True\n","Step 10450: Gradients computed successfully\n","Step 10460: Loss: 0.1902, requires_grad: True\n","Step 10470: Loss: 0.1280, requires_grad: True\n","Step 10480: Loss: 0.1569, requires_grad: True\n","Step 10490: Loss: 0.3393, requires_grad: True\n","Step 10500: Loss: 0.3351, requires_grad: True\n","Step 10500: Gradients computed successfully\n","Step 10510: Loss: 0.2078, requires_grad: True\n","Step 10520: Loss: 0.1179, requires_grad: True\n","Step 10530: Loss: 0.2042, requires_grad: True\n","Step 10540: Loss: 0.1253, requires_grad: True\n","Step 10550: Loss: 0.2540, requires_grad: True\n","Step 10550: Gradients computed successfully\n","Step 10560: Loss: 0.5904, requires_grad: True\n","Step 10570: Loss: 0.3045, requires_grad: True\n","Step 10580: Loss: 0.1833, requires_grad: True\n","Step 10590: Loss: 0.5563, requires_grad: True\n","Step 10600: Loss: 0.4114, requires_grad: True\n","Step 10600: Gradients computed successfully\n","Step 10610: Loss: 0.2916, requires_grad: True\n","Step 10620: Loss: 0.5476, requires_grad: True\n","Step 10630: Loss: 0.2068, requires_grad: True\n","Step 10640: Loss: 0.2646, requires_grad: True\n","Step 10650: Loss: 0.4591, requires_grad: True\n","Step 10650: Gradients computed successfully\n","Step 10660: Loss: 0.2020, requires_grad: True\n","Step 10670: Loss: 0.4785, requires_grad: True\n","Step 10680: Loss: 0.3127, requires_grad: True\n","Step 10690: Loss: 0.1890, requires_grad: True\n","Step 10700: Loss: 0.1176, requires_grad: True\n","Step 10700: Gradients computed successfully\n","Step 10710: Loss: 0.1752, requires_grad: True\n","Step 10720: Loss: 0.3433, requires_grad: True\n","Step 10730: Loss: 0.0806, requires_grad: True\n","Step 10740: Loss: 0.2718, requires_grad: True\n","Step 10750: Loss: 0.2174, requires_grad: True\n","Step 10750: Gradients computed successfully\n","Step 10760: Loss: 0.2168, requires_grad: True\n","Step 10770: Loss: 0.3303, requires_grad: True\n","Step 10780: Loss: 0.5554, requires_grad: True\n","Step 10790: Loss: 0.6028, requires_grad: True\n","Step 10800: Loss: 0.1806, requires_grad: True\n","Step 10800: Gradients computed successfully\n","Step 10810: Loss: 0.2074, requires_grad: True\n","Step 10820: Loss: 0.3426, requires_grad: True\n","Step 10830: Loss: 0.8714, requires_grad: True\n","Step 10840: Loss: 0.2259, requires_grad: True\n","Step 10850: Loss: 0.2625, requires_grad: True\n","Step 10850: Gradients computed successfully\n","Step 10860: Loss: 0.1191, requires_grad: True\n","Step 10870: Loss: 0.2803, requires_grad: True\n","Step 10880: Loss: 0.1873, requires_grad: True\n","Step 10890: Loss: 0.1975, requires_grad: True\n","Step 10900: Loss: 0.2400, requires_grad: True\n","Step 10900: Gradients computed successfully\n","Step 10910: Loss: 0.2170, requires_grad: True\n","Step 10920: Loss: 0.1173, requires_grad: True\n","Step 10930: Loss: 0.1883, requires_grad: True\n","Step 10940: Loss: 0.2376, requires_grad: True\n","Step 10950: Loss: 0.6597, requires_grad: True\n","Step 10950: Gradients computed successfully\n","Step 10960: Loss: 0.1246, requires_grad: True\n","Step 10970: Loss: 0.1176, requires_grad: True\n","Step 10980: Loss: 0.1966, requires_grad: True\n","Step 10990: Loss: 0.2973, requires_grad: True\n","Step 11000: Loss: 0.3407, requires_grad: True\n","Step 11000: Gradients computed successfully\n","Step 11010: Loss: 0.1527, requires_grad: True\n","Step 11020: Loss: 0.1540, requires_grad: True\n","Step 11030: Loss: 0.2729, requires_grad: True\n","Step 11040: Loss: 0.2147, requires_grad: True\n","Step 11050: Loss: 0.5154, requires_grad: True\n","Step 11050: Gradients computed successfully\n","Step 11060: Loss: 0.1328, requires_grad: True\n","Step 11070: Loss: 0.1624, requires_grad: True\n","Step 11080: Loss: 0.2962, requires_grad: True\n","Step 11090: Loss: 0.2522, requires_grad: True\n","Step 11100: Loss: 0.2260, requires_grad: True\n","Step 11100: Gradients computed successfully\n","Step 11110: Loss: 0.2172, requires_grad: True\n","Step 11120: Loss: 0.1341, requires_grad: True\n","Step 11130: Loss: 0.2905, requires_grad: True\n","Step 11140: Loss: 0.1884, requires_grad: True\n","Step 11150: Loss: 0.3328, requires_grad: True\n","Step 11150: Gradients computed successfully\n","Step 11160: Loss: 0.3074, requires_grad: True\n","Step 11170: Loss: 0.2678, requires_grad: True\n","Step 11180: Loss: 0.2921, requires_grad: True\n","Step 11190: Loss: 0.3289, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-1400\n","Step 11200: Loss: 0.2803, requires_grad: True\n","Step 11200: Gradients computed successfully\n","Step 11210: Loss: 0.1483, requires_grad: True\n","Step 11220: Loss: 0.1879, requires_grad: True\n","Step 11230: Loss: 0.2903, requires_grad: True\n","Step 11240: Loss: 0.3525, requires_grad: True\n","Step 11250: Loss: 0.5410, requires_grad: True\n","Step 11250: Gradients computed successfully\n","Step 11260: Loss: 0.1365, requires_grad: True\n","Step 11270: Loss: 0.2111, requires_grad: True\n","Step 11280: Loss: 0.1849, requires_grad: True\n","Step 11290: Loss: 0.1102, requires_grad: True\n","Step 11300: Loss: 0.2373, requires_grad: True\n","Step 11300: Gradients computed successfully\n","Step 11310: Loss: 0.1059, requires_grad: True\n","Step 11320: Loss: 0.1575, requires_grad: True\n","Step 11330: Loss: 0.7024, requires_grad: True\n","Step 11340: Loss: 0.1332, requires_grad: True\n","Step 11350: Loss: 0.3592, requires_grad: True\n","Step 11350: Gradients computed successfully\n","Step 11360: Loss: 0.3598, requires_grad: True\n","Step 11370: Loss: 0.3057, requires_grad: True\n","Step 11380: Loss: 0.1725, requires_grad: True\n","Step 11390: Loss: 0.1148, requires_grad: True\n","Step 11400: Loss: 0.2805, requires_grad: True\n","Step 11400: Gradients computed successfully\n","Step 11410: Loss: 0.1438, requires_grad: True\n","Step 11420: Loss: 0.2087, requires_grad: True\n","Step 11430: Loss: 0.2666, requires_grad: True\n","Step 11440: Loss: 0.2160, requires_grad: True\n","Step 11450: Loss: 0.2757, requires_grad: True\n","Step 11450: Gradients computed successfully\n","Step 11460: Loss: 0.2250, requires_grad: True\n","Step 11470: Loss: 0.2805, requires_grad: True\n","Step 11480: Loss: 0.3865, requires_grad: True\n","Step 11490: Loss: 0.6021, requires_grad: True\n","Step 11500: Loss: 0.2379, requires_grad: True\n","Step 11500: Gradients computed successfully\n","Step 11510: Loss: 0.4564, requires_grad: True\n","Step 11520: Loss: 0.2538, requires_grad: True\n","Step 11530: Loss: 0.6160, requires_grad: True\n","Step 11540: Loss: 0.4311, requires_grad: True\n","Step 11550: Loss: 0.2076, requires_grad: True\n","Step 11550: Gradients computed successfully\n","Step 11560: Loss: 0.2076, requires_grad: True\n","Step 11570: Loss: 0.2428, requires_grad: True\n","Step 11580: Loss: 0.2468, requires_grad: True\n","Step 11590: Loss: 0.1043, requires_grad: True\n","Step 11600: Loss: 0.2113, requires_grad: True\n","Step 11600: Gradients computed successfully\n","Step 11610: Loss: 0.3050, requires_grad: True\n","Step 11620: Loss: 0.2307, requires_grad: True\n","Step 11630: Loss: 0.1841, requires_grad: True\n","Step 11640: Loss: 0.3456, requires_grad: True\n","Step 11650: Loss: 0.0637, requires_grad: True\n","Step 11650: Gradients computed successfully\n","Step 11660: Loss: 0.2238, requires_grad: True\n","Step 11670: Loss: 0.5666, requires_grad: True\n","Step 11680: Loss: 0.2253, requires_grad: True\n","Step 11690: Loss: 0.1726, requires_grad: True\n","Step 11700: Loss: 0.2767, requires_grad: True\n","Step 11700: Gradients computed successfully\n","Step 11710: Loss: 0.3033, requires_grad: True\n","Step 11720: Loss: 0.1702, requires_grad: True\n","Step 11730: Loss: 0.3451, requires_grad: True\n","Step 11740: Loss: 0.4820, requires_grad: True\n","Step 11750: Loss: 0.2680, requires_grad: True\n","Step 11750: Gradients computed successfully\n","Step 11760: Loss: 0.3464, requires_grad: True\n","Step 11770: Loss: 0.2973, requires_grad: True\n","Step 11780: Loss: 0.1870, requires_grad: True\n","Step 11790: Loss: 0.2041, requires_grad: True\n","Step 11800: Loss: 0.2253, requires_grad: True\n","Step 11800: Gradients computed successfully\n","Step 11810: Loss: 0.1961, requires_grad: True\n","Step 11820: Loss: 0.1014, requires_grad: True\n","Step 11830: Loss: 0.2282, requires_grad: True\n","Step 11840: Loss: 0.3409, requires_grad: True\n","Step 11850: Loss: 0.2623, requires_grad: True\n","Step 11850: Gradients computed successfully\n","Step 11860: Loss: 0.1567, requires_grad: True\n","Step 11870: Loss: 0.4266, requires_grad: True\n","Step 11880: Loss: 0.1143, requires_grad: True\n","Step 11890: Loss: 0.1077, requires_grad: True\n","Step 11900: Loss: 0.1501, requires_grad: True\n","Step 11900: Gradients computed successfully\n","Step 11910: Loss: 0.3600, requires_grad: True\n","Step 11920: Loss: 0.3006, requires_grad: True\n","Step 11930: Loss: 0.2542, requires_grad: True\n","Step 11940: Loss: 0.2566, requires_grad: True\n","Step 11950: Loss: 0.2149, requires_grad: True\n","Step 11950: Gradients computed successfully\n","Step 11960: Loss: 0.1887, requires_grad: True\n","Step 11970: Loss: 0.2570, requires_grad: True\n","Step 11980: Loss: 0.2879, requires_grad: True\n","Step 11990: Loss: 0.2253, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-1500\n","Step 12000: Loss: 0.2535, requires_grad: True\n","Step 12000: Gradients computed successfully\n","Step 12010: Loss: 0.3378, requires_grad: True\n","Step 12020: Loss: 0.4135, requires_grad: True\n","Step 12030: Loss: 0.0873, requires_grad: True\n","Step 12040: Loss: 0.4627, requires_grad: True\n","Step 12050: Loss: 0.2718, requires_grad: True\n","Step 12050: Gradients computed successfully\n","Step 12060: Loss: 0.1532, requires_grad: True\n","Step 12070: Loss: 0.2074, requires_grad: True\n","Step 12080: Loss: 0.4879, requires_grad: True\n","Step 12090: Loss: 0.1055, requires_grad: True\n","Step 12100: Loss: 0.3622, requires_grad: True\n","Step 12100: Gradients computed successfully\n","Step 12110: Loss: 0.1905, requires_grad: True\n","Step 12120: Loss: 0.4851, requires_grad: True\n","Step 12130: Loss: 0.4473, requires_grad: True\n","Step 12140: Loss: 0.1202, requires_grad: True\n","Step 12150: Loss: 0.2206, requires_grad: True\n","Step 12150: Gradients computed successfully\n","Step 12160: Loss: 0.2276, requires_grad: True\n","Step 12170: Loss: 0.2227, requires_grad: True\n","Step 12180: Loss: 0.2578, requires_grad: True\n","Step 12190: Loss: 0.5038, requires_grad: True\n","Step 12200: Loss: 0.1962, requires_grad: True\n","Step 12200: Gradients computed successfully\n","Step 12210: Loss: 0.0817, requires_grad: True\n","Step 12220: Loss: 0.1454, requires_grad: True\n","Step 12230: Loss: 0.2364, requires_grad: True\n","Step 12240: Loss: 0.1628, requires_grad: True\n","Step 12250: Loss: 0.1345, requires_grad: True\n","Step 12250: Gradients computed successfully\n","Step 12260: Loss: 0.1508, requires_grad: True\n","Step 12270: Loss: 0.3519, requires_grad: True\n","Step 12280: Loss: 0.1162, requires_grad: True\n","Step 12290: Loss: 0.1596, requires_grad: True\n","Step 12300: Loss: 0.3034, requires_grad: True\n","Step 12300: Gradients computed successfully\n","Step 12310: Loss: 0.2252, requires_grad: True\n","Step 12320: Loss: 0.3493, requires_grad: True\n","Step 12330: Loss: 0.1061, requires_grad: True\n","Step 12340: Loss: 0.2311, requires_grad: True\n","Step 12350: Loss: 0.2257, requires_grad: True\n","Step 12350: Gradients computed successfully\n","Step 12360: Loss: 0.1774, requires_grad: True\n","Step 12370: Loss: 0.1734, requires_grad: True\n","Step 12380: Loss: 0.3506, requires_grad: True\n","Step 12390: Loss: 0.2571, requires_grad: True\n","Step 12400: Loss: 0.2640, requires_grad: True\n","Step 12400: Gradients computed successfully\n","Step 12410: Loss: 0.3078, requires_grad: True\n","Step 12420: Loss: 0.2544, requires_grad: True\n","Step 12430: Loss: 0.2306, requires_grad: True\n","Step 12440: Loss: 0.1262, requires_grad: True\n","Step 12450: Loss: 0.0818, requires_grad: True\n","Step 12450: Gradients computed successfully\n","Step 12460: Loss: 0.3626, requires_grad: True\n","Step 12470: Loss: 0.1700, requires_grad: True\n","Step 12480: Loss: 0.1827, requires_grad: True\n","Step 12490: Loss: 0.4355, requires_grad: True\n","Step 12500: Loss: 0.2843, requires_grad: True\n","Step 12500: Gradients computed successfully\n","Step 12510: Loss: 0.5762, requires_grad: True\n","Step 12520: Loss: 0.2249, requires_grad: True\n","Step 12530: Loss: 0.2581, requires_grad: True\n","Step 12540: Loss: 0.2178, requires_grad: True\n","Step 12550: Loss: 0.2204, requires_grad: True\n","Step 12550: Gradients computed successfully\n","Step 12560: Loss: 0.4685, requires_grad: True\n","Step 12570: Loss: 0.2031, requires_grad: True\n","Step 12580: Loss: 0.2526, requires_grad: True\n","Step 12590: Loss: 0.3213, requires_grad: True\n","Step 12600: Loss: 0.2974, requires_grad: True\n","Step 12600: Gradients computed successfully\n","Step 12610: Loss: 0.3529, requires_grad: True\n","Step 12620: Loss: 0.0926, requires_grad: True\n","Step 12630: Loss: 0.2043, requires_grad: True\n","Step 12640: Loss: 0.3465, requires_grad: True\n","Step 12650: Loss: 0.2755, requires_grad: True\n","Step 12650: Gradients computed successfully\n","Step 12660: Loss: 0.2129, requires_grad: True\n","Step 12670: Loss: 0.3089, requires_grad: True\n","Step 12680: Loss: 0.2832, requires_grad: True\n","Step 12690: Loss: 0.1227, requires_grad: True\n","Step 12700: Loss: 0.2851, requires_grad: True\n","Step 12700: Gradients computed successfully\n","Step 12710: Loss: 0.3961, requires_grad: True\n","Step 12720: Loss: 0.1915, requires_grad: True\n","Step 12730: Loss: 0.2770, requires_grad: True\n","Step 12740: Loss: 0.4155, requires_grad: True\n","Step 12750: Loss: 0.2309, requires_grad: True\n","Step 12750: Gradients computed successfully\n","Step 12760: Loss: 0.4754, requires_grad: True\n","Step 12770: Loss: 0.0723, requires_grad: True\n","Step 12780: Loss: 0.1939, requires_grad: True\n","Step 12790: Loss: 0.2833, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-1600\n","Step 12800: Loss: 0.1982, requires_grad: True\n","Step 12800: Gradients computed successfully\n","Step 12810: Loss: 0.2656, requires_grad: True\n","Step 12820: Loss: 0.2356, requires_grad: True\n","Step 12830: Loss: 0.2199, requires_grad: True\n","Step 12840: Loss: 0.2043, requires_grad: True\n","Step 12850: Loss: 0.7272, requires_grad: True\n","Step 12850: Gradients computed successfully\n","Step 12860: Loss: 0.2209, requires_grad: True\n","Step 12870: Loss: 0.1631, requires_grad: True\n","Step 12880: Loss: 0.3941, requires_grad: True\n","Step 12890: Loss: 0.2897, requires_grad: True\n","Step 12900: Loss: 0.3265, requires_grad: True\n","Step 12900: Gradients computed successfully\n","Step 12910: Loss: 0.3248, requires_grad: True\n","Step 12920: Loss: 0.1159, requires_grad: True\n","Step 12930: Loss: 0.3924, requires_grad: True\n","Step 12940: Loss: 0.0548, requires_grad: True\n","Step 12950: Loss: 0.3181, requires_grad: True\n","Step 12950: Gradients computed successfully\n","Step 12960: Loss: 0.2675, requires_grad: True\n","Step 12970: Loss: 0.3493, requires_grad: True\n","Step 12980: Loss: 0.1742, requires_grad: True\n","Step 12990: Loss: 0.1172, requires_grad: True\n","Step 13000: Loss: 0.1482, requires_grad: True\n","Step 13000: Gradients computed successfully\n","Step 13010: Loss: 0.1842, requires_grad: True\n","Step 13020: Loss: 0.3044, requires_grad: True\n","Step 13030: Loss: 0.4147, requires_grad: True\n","Step 13040: Loss: 0.3219, requires_grad: True\n","Step 13050: Loss: 0.1468, requires_grad: True\n","Step 13050: Gradients computed successfully\n","Step 13060: Loss: 0.3416, requires_grad: True\n","Step 13070: Loss: 0.2386, requires_grad: True\n","Step 13080: Loss: 0.1096, requires_grad: True\n","Step 13090: Loss: 0.0813, requires_grad: True\n","Step 13100: Loss: 0.1742, requires_grad: True\n","Step 13100: Gradients computed successfully\n","Step 13110: Loss: 0.2017, requires_grad: True\n","Step 13120: Loss: 0.1506, requires_grad: True\n","Step 13130: Loss: 0.1808, requires_grad: True\n","Step 13140: Loss: 0.4151, requires_grad: True\n","Step 13150: Loss: 0.1398, requires_grad: True\n","Step 13150: Gradients computed successfully\n","Step 13160: Loss: 0.1406, requires_grad: True\n","Step 13170: Loss: 0.1781, requires_grad: True\n","Step 13180: Loss: 0.1388, requires_grad: True\n","Step 13190: Loss: 0.2052, requires_grad: True\n","Step 13200: Loss: 0.1822, requires_grad: True\n","Step 13200: Gradients computed successfully\n","Step 13210: Loss: 0.4768, requires_grad: True\n","Step 13220: Loss: 0.4542, requires_grad: True\n","Step 13230: Loss: 0.4127, requires_grad: True\n","Step 13240: Loss: 0.2009, requires_grad: True\n","Step 13250: Loss: 0.1518, requires_grad: True\n","Step 13250: Gradients computed successfully\n","Step 13260: Loss: 0.1954, requires_grad: True\n","Step 13270: Loss: 0.1389, requires_grad: True\n","Step 13280: Loss: 0.2361, requires_grad: True\n","Step 13290: Loss: 0.3921, requires_grad: True\n","Step 13300: Loss: 0.4244, requires_grad: True\n","Step 13300: Gradients computed successfully\n","Step 13310: Loss: 0.3795, requires_grad: True\n","Step 13320: Loss: 0.2155, requires_grad: True\n","Step 13330: Loss: 0.2009, requires_grad: True\n","Step 13340: Loss: 0.1603, requires_grad: True\n","Step 13350: Loss: 0.1413, requires_grad: True\n","Step 13350: Gradients computed successfully\n","Step 13360: Loss: 0.3795, requires_grad: True\n","Step 13370: Loss: 0.1321, requires_grad: True\n","Step 13380: Loss: 0.1045, requires_grad: True\n","Step 13390: Loss: 0.2175, requires_grad: True\n","Step 13400: Loss: 0.1137, requires_grad: True\n","Step 13400: Gradients computed successfully\n","Step 13410: Loss: 0.2505, requires_grad: True\n","Step 13420: Loss: 0.4153, requires_grad: True\n","Step 13430: Loss: 0.1907, requires_grad: True\n","Step 13440: Loss: 0.3774, requires_grad: True\n","Step 13450: Loss: 0.0762, requires_grad: True\n","Step 13450: Gradients computed successfully\n","Step 13460: Loss: 0.2214, requires_grad: True\n","Step 13470: Loss: 0.1550, requires_grad: True\n","Step 13480: Loss: 0.1208, requires_grad: True\n","Step 13490: Loss: 0.2020, requires_grad: True\n","Step 13500: Loss: 0.2410, requires_grad: True\n","Step 13500: Gradients computed successfully\n","Step 13510: Loss: 0.1996, requires_grad: True\n","Step 13520: Loss: 0.1827, requires_grad: True\n","Step 13530: Loss: 0.1935, requires_grad: True\n","Step 13540: Loss: 0.1786, requires_grad: True\n","Step 13550: Loss: 0.5120, requires_grad: True\n","Step 13550: Gradients computed successfully\n","Step 13560: Loss: 0.1500, requires_grad: True\n","Step 13570: Loss: 0.5980, requires_grad: True\n","Step 13580: Loss: 0.2876, requires_grad: True\n","Step 13590: Loss: 0.0897, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-1700\n","Step 13600: Loss: 0.1420, requires_grad: True\n","Step 13600: Gradients computed successfully\n","Step 13610: Loss: 0.2155, requires_grad: True\n","Step 13620: Loss: 0.1818, requires_grad: True\n","Step 13630: Loss: 0.0826, requires_grad: True\n","Step 13640: Loss: 0.2766, requires_grad: True\n","Step 13650: Loss: 0.3131, requires_grad: True\n","Step 13650: Gradients computed successfully\n","Step 13660: Loss: 0.2722, requires_grad: True\n","Step 13670: Loss: 0.2381, requires_grad: True\n","Step 13680: Loss: 0.2580, requires_grad: True\n","Step 13690: Loss: 0.3376, requires_grad: True\n","Step 13700: Loss: 0.0761, requires_grad: True\n","Step 13700: Gradients computed successfully\n","Step 13710: Loss: 0.1659, requires_grad: True\n","Step 13720: Loss: 0.1700, requires_grad: True\n","Step 13730: Loss: 0.2742, requires_grad: True\n","Step 13740: Loss: 0.2868, requires_grad: True\n","Step 13750: Loss: 0.1020, requires_grad: True\n","Step 13750: Gradients computed successfully\n","Step 13760: Loss: 0.2546, requires_grad: True\n","Step 13770: Loss: 0.1541, requires_grad: True\n","Step 13780: Loss: 0.0770, requires_grad: True\n","Step 13790: Loss: 0.1193, requires_grad: True\n","Step 13800: Loss: 0.4609, requires_grad: True\n","Step 13800: Gradients computed successfully\n","Step 13810: Loss: 0.2856, requires_grad: True\n","Step 13820: Loss: 0.1799, requires_grad: True\n","Step 13830: Loss: 0.2617, requires_grad: True\n","Step 13840: Loss: 0.1539, requires_grad: True\n","Step 13850: Loss: 0.2101, requires_grad: True\n","Step 13850: Gradients computed successfully\n","Step 13860: Loss: 0.2751, requires_grad: True\n","Step 13870: Loss: 0.3122, requires_grad: True\n","Step 13880: Loss: 0.2836, requires_grad: True\n","Step 13890: Loss: 0.1684, requires_grad: True\n","Step 13900: Loss: 0.1873, requires_grad: True\n","Step 13900: Gradients computed successfully\n","Step 13910: Loss: 0.3085, requires_grad: True\n","Step 13920: Loss: 0.1435, requires_grad: True\n","Step 13930: Loss: 0.2668, requires_grad: True\n","Step 13940: Loss: 0.0998, requires_grad: True\n","Step 13950: Loss: 0.3186, requires_grad: True\n","Step 13950: Gradients computed successfully\n","Step 13960: Loss: 0.4594, requires_grad: True\n","Step 13970: Loss: 0.3470, requires_grad: True\n","Step 13980: Loss: 0.1949, requires_grad: True\n","Step 13990: Loss: 0.1565, requires_grad: True\n","Step 14000: Loss: 0.1888, requires_grad: True\n","Step 14000: Gradients computed successfully\n","Step 14010: Loss: 0.1038, requires_grad: True\n","Step 14020: Loss: 0.1331, requires_grad: True\n","Step 14030: Loss: 0.3301, requires_grad: True\n","Step 14040: Loss: 0.2282, requires_grad: True\n","Step 14050: Loss: 0.2491, requires_grad: True\n","Step 14050: Gradients computed successfully\n","Step 14060: Loss: 0.1139, requires_grad: True\n","Step 14070: Loss: 0.1037, requires_grad: True\n","Step 14080: Loss: 0.1257, requires_grad: True\n","Step 14090: Loss: 0.4201, requires_grad: True\n","Step 14100: Loss: 0.2993, requires_grad: True\n","Step 14100: Gradients computed successfully\n","Step 14110: Loss: 0.2938, requires_grad: True\n","Step 14120: Loss: 0.2369, requires_grad: True\n","Step 14130: Loss: 0.2050, requires_grad: True\n","Step 14140: Loss: 0.1433, requires_grad: True\n","Step 14150: Loss: 0.3224, requires_grad: True\n","Step 14150: Gradients computed successfully\n","Step 14160: Loss: 0.1908, requires_grad: True\n","Step 14170: Loss: 0.1662, requires_grad: True\n","Step 14180: Loss: 0.1248, requires_grad: True\n","Step 14190: Loss: 0.2228, requires_grad: True\n","Step 14200: Loss: 0.4151, requires_grad: True\n","Step 14200: Gradients computed successfully\n","Step 14210: Loss: 0.2178, requires_grad: True\n","Step 14220: Loss: 0.1666, requires_grad: True\n","Step 14230: Loss: 0.2302, requires_grad: True\n","Step 14240: Loss: 0.2684, requires_grad: True\n","Step 14250: Loss: 0.5152, requires_grad: True\n","Step 14250: Gradients computed successfully\n","Step 14260: Loss: 0.1507, requires_grad: True\n","Step 14270: Loss: 0.2289, requires_grad: True\n","Step 14280: Loss: 0.1430, requires_grad: True\n","Step 14290: Loss: 0.1244, requires_grad: True\n","Step 14300: Loss: 0.1519, requires_grad: True\n","Step 14300: Gradients computed successfully\n","Step 14310: Loss: 0.0814, requires_grad: True\n","Step 14320: Loss: 0.2603, requires_grad: True\n","Step 14330: Loss: 0.1958, requires_grad: True\n","Step 14340: Loss: 0.1870, requires_grad: True\n","Step 14350: Loss: 0.3012, requires_grad: True\n","Step 14350: Gradients computed successfully\n","Step 14360: Loss: 0.2744, requires_grad: True\n","Step 14370: Loss: 0.2829, requires_grad: True\n","Step 14380: Loss: 0.0995, requires_grad: True\n","Step 14390: Loss: 0.4900, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-1800\n","Step 14400: Loss: 0.3710, requires_grad: True\n","Step 14400: Gradients computed successfully\n","Step 14410: Loss: 0.2001, requires_grad: True\n","Step 14420: Loss: 0.2181, requires_grad: True\n","Step 14430: Loss: 0.3172, requires_grad: True\n","Step 14440: Loss: 0.1884, requires_grad: True\n","Step 14450: Loss: 0.1677, requires_grad: True\n","Step 14450: Gradients computed successfully\n","Step 14460: Loss: 0.2041, requires_grad: True\n","Step 14470: Loss: 0.3177, requires_grad: True\n","Step 14480: Loss: 0.4605, requires_grad: True\n","Step 14490: Loss: 0.2131, requires_grad: True\n","Step 14500: Loss: 0.1182, requires_grad: True\n","Step 14500: Gradients computed successfully\n","Step 14510: Loss: 0.1094, requires_grad: True\n","Step 14520: Loss: 0.0603, requires_grad: True\n","Step 14530: Loss: 0.2037, requires_grad: True\n","Step 14540: Loss: 0.1336, requires_grad: True\n","Step 14550: Loss: 0.0854, requires_grad: True\n","Step 14550: Gradients computed successfully\n","Step 14560: Loss: 0.3370, requires_grad: True\n","Step 14570: Loss: 0.4098, requires_grad: True\n","Step 14580: Loss: 0.5424, requires_grad: True\n","Step 14590: Loss: 0.1067, requires_grad: True\n","Step 14600: Loss: 0.1482, requires_grad: True\n","Step 14600: Gradients computed successfully\n","Step 14610: Loss: 0.1313, requires_grad: True\n","Step 14620: Loss: 0.4472, requires_grad: True\n","Step 14630: Loss: 0.4186, requires_grad: True\n","Step 14640: Loss: 0.0957, requires_grad: True\n","Step 14650: Loss: 0.3860, requires_grad: True\n","Step 14650: Gradients computed successfully\n","Step 14660: Loss: 0.1746, requires_grad: True\n","Step 14670: Loss: 0.3207, requires_grad: True\n","Step 14680: Loss: 0.2636, requires_grad: True\n","Step 14690: Loss: 0.1768, requires_grad: True\n","Step 14700: Loss: 0.2817, requires_grad: True\n","Step 14700: Gradients computed successfully\n","Step 14710: Loss: 0.1857, requires_grad: True\n","Step 14720: Loss: 0.0858, requires_grad: True\n","Step 14730: Loss: 0.4610, requires_grad: True\n","Step 14740: Loss: 0.1515, requires_grad: True\n","Step 14750: Loss: 0.4451, requires_grad: True\n","Step 14750: Gradients computed successfully\n","Step 14760: Loss: 0.1071, requires_grad: True\n","Step 14770: Loss: 0.1925, requires_grad: True\n","Step 14780: Loss: 0.3195, requires_grad: True\n","Step 14790: Loss: 0.2538, requires_grad: True\n","Step 14800: Loss: 0.1360, requires_grad: True\n","Step 14800: Gradients computed successfully\n","Step 14810: Loss: 0.1571, requires_grad: True\n","Step 14820: Loss: 0.2437, requires_grad: True\n","Step 14830: Loss: 0.1236, requires_grad: True\n","Step 14840: Loss: 0.1251, requires_grad: True\n","Step 14850: Loss: 0.3821, requires_grad: True\n","Step 14850: Gradients computed successfully\n","Step 14860: Loss: 0.4056, requires_grad: True\n","Step 14870: Loss: 0.0987, requires_grad: True\n","Step 14880: Loss: 0.2578, requires_grad: True\n","Step 14890: Loss: 0.2208, requires_grad: True\n","Step 14900: Loss: 0.1587, requires_grad: True\n","Step 14900: Gradients computed successfully\n","Step 14910: Loss: 0.5400, requires_grad: True\n","Step 14920: Loss: 0.1512, requires_grad: True\n","Step 14930: Loss: 0.2517, requires_grad: True\n","Step 14940: Loss: 0.2310, requires_grad: True\n","Step 14950: Loss: 0.1339, requires_grad: True\n","Step 14950: Gradients computed successfully\n","Step 14960: Loss: 0.2451, requires_grad: True\n","Step 14970: Loss: 0.3632, requires_grad: True\n","Step 14980: Loss: 0.1357, requires_grad: True\n","Step 14990: Loss: 0.2274, requires_grad: True\n","Step 15000: Loss: 0.1980, requires_grad: True\n","Step 15000: Gradients computed successfully\n","Step 15010: Loss: 0.2615, requires_grad: True\n","Step 15020: Loss: 0.2768, requires_grad: True\n","Step 15030: Loss: 0.1549, requires_grad: True\n","Step 15040: Loss: 0.2999, requires_grad: True\n","Step 15050: Loss: 0.2700, requires_grad: True\n","Step 15050: Gradients computed successfully\n","Step 15060: Loss: 0.1406, requires_grad: True\n","Step 15070: Loss: 0.2656, requires_grad: True\n","Step 15080: Loss: 0.3813, requires_grad: True\n","Step 15090: Loss: 0.1071, requires_grad: True\n","Step 15100: Loss: 0.1464, requires_grad: True\n","Step 15100: Gradients computed successfully\n","Step 15110: Loss: 0.4247, requires_grad: True\n","Step 15120: Loss: 0.2643, requires_grad: True\n","Step 15130: Loss: 0.1661, requires_grad: True\n","Step 15140: Loss: 0.2186, requires_grad: True\n","Step 15150: Loss: 0.4264, requires_grad: True\n","Step 15150: Gradients computed successfully\n","Step 15160: Loss: 0.4900, requires_grad: True\n","Step 15170: Loss: 0.3944, requires_grad: True\n","Step 15180: Loss: 0.3127, requires_grad: True\n","Step 15190: Loss: 0.1683, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-1900\n","Step 15200: Loss: 0.3181, requires_grad: True\n","Step 15200: Gradients computed successfully\n","Step 15210: Loss: 0.2801, requires_grad: True\n","Step 15220: Loss: 0.4149, requires_grad: True\n","Step 15230: Loss: 0.0689, requires_grad: True\n","Step 15240: Loss: 0.4732, requires_grad: True\n","Step 15250: Loss: 0.3880, requires_grad: True\n","Step 15250: Gradients computed successfully\n","Step 15260: Loss: 0.1154, requires_grad: True\n","Step 15270: Loss: 0.3545, requires_grad: True\n","Step 15280: Loss: 0.0783, requires_grad: True\n","Step 15290: Loss: 0.1043, requires_grad: True\n","Step 15300: Loss: 0.3705, requires_grad: True\n","Step 15300: Gradients computed successfully\n","Step 15310: Loss: 0.2288, requires_grad: True\n","Step 15320: Loss: 0.3966, requires_grad: True\n","Step 15330: Loss: 0.2419, requires_grad: True\n","Step 15340: Loss: 0.1060, requires_grad: True\n","Step 15350: Loss: 0.1752, requires_grad: True\n","Step 15350: Gradients computed successfully\n","Step 15360: Loss: 0.1299, requires_grad: True\n","Step 15370: Loss: 0.1767, requires_grad: True\n","Step 15380: Loss: 0.5003, requires_grad: True\n","Step 15390: Loss: 0.2376, requires_grad: True\n","Step 15400: Loss: 0.1860, requires_grad: True\n","Step 15400: Gradients computed successfully\n","Step 15410: Loss: 0.2913, requires_grad: True\n","Step 15420: Loss: 0.1544, requires_grad: True\n","Step 15430: Loss: 0.2166, requires_grad: True\n","Step 15440: Loss: 0.3002, requires_grad: True\n","Step 15450: Loss: 0.0546, requires_grad: True\n","Step 15450: Gradients computed successfully\n","Step 15460: Loss: 0.1588, requires_grad: True\n","Step 15470: Loss: 0.2645, requires_grad: True\n","Step 15480: Loss: 0.2071, requires_grad: True\n","Step 15490: Loss: 0.4240, requires_grad: True\n","Step 15500: Loss: 0.2304, requires_grad: True\n","Step 15500: Gradients computed successfully\n","Step 15510: Loss: 0.2208, requires_grad: True\n","Step 15520: Loss: 0.1637, requires_grad: True\n","Step 15530: Loss: 0.2116, requires_grad: True\n","Step 15540: Loss: 0.3264, requires_grad: True\n","Step 15550: Loss: 0.2867, requires_grad: True\n","Step 15550: Gradients computed successfully\n","Step 15560: Loss: 0.1349, requires_grad: True\n","Step 15570: Loss: 0.4241, requires_grad: True\n","Step 15580: Loss: 0.1755, requires_grad: True\n","Step 15590: Loss: 0.2185, requires_grad: True\n","Step 15600: Loss: 0.3868, requires_grad: True\n","Step 15600: Gradients computed successfully\n","Step 15610: Loss: 0.5184, requires_grad: True\n","Step 15620: Loss: 0.3848, requires_grad: True\n","Step 15630: Loss: 0.4289, requires_grad: True\n","Step 15640: Loss: 0.2543, requires_grad: True\n","Step 15650: Loss: 0.3563, requires_grad: True\n","Step 15650: Gradients computed successfully\n","Step 15660: Loss: 0.0659, requires_grad: True\n","Step 15670: Loss: 0.1652, requires_grad: True\n","Step 15680: Loss: 0.2630, requires_grad: True\n","Step 15690: Loss: 0.1628, requires_grad: True\n","Step 15700: Loss: 0.2465, requires_grad: True\n","Step 15700: Gradients computed successfully\n","Step 15710: Loss: 0.1590, requires_grad: True\n","Step 15720: Loss: 0.1332, requires_grad: True\n","Step 15730: Loss: 0.1616, requires_grad: True\n","Step 15740: Loss: 0.1836, requires_grad: True\n","Step 15750: Loss: 0.5168, requires_grad: True\n","Step 15750: Gradients computed successfully\n","Step 15760: Loss: 0.2088, requires_grad: True\n","Step 15770: Loss: 0.2467, requires_grad: True\n","Step 15780: Loss: 0.1384, requires_grad: True\n","Step 15790: Loss: 0.2923, requires_grad: True\n","Step 15800: Loss: 0.2303, requires_grad: True\n","Step 15800: Gradients computed successfully\n","Step 15810: Loss: 0.3238, requires_grad: True\n","Step 15820: Loss: 0.1898, requires_grad: True\n","Step 15830: Loss: 0.1797, requires_grad: True\n","Step 15840: Loss: 0.2941, requires_grad: True\n","Step 15850: Loss: 0.1222, requires_grad: True\n","Step 15850: Gradients computed successfully\n","Step 15860: Loss: 0.2546, requires_grad: True\n","Step 15870: Loss: 0.1994, requires_grad: True\n","Step 15880: Loss: 0.5409, requires_grad: True\n","Step 15890: Loss: 0.2279, requires_grad: True\n","Step 15900: Loss: 0.1839, requires_grad: True\n","Step 15900: Gradients computed successfully\n","Step 15910: Loss: 0.1324, requires_grad: True\n","Step 15920: Loss: 0.1481, requires_grad: True\n","Step 15930: Loss: 0.2303, requires_grad: True\n","Step 15940: Loss: 0.2868, requires_grad: True\n","Step 15950: Loss: 0.1709, requires_grad: True\n","Step 15950: Gradients computed successfully\n","Step 15960: Loss: 0.2150, requires_grad: True\n","Step 15970: Loss: 0.1237, requires_grad: True\n","Step 15980: Loss: 0.1510, requires_grad: True\n","Step 15990: Loss: 0.1666, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-2000\n","Step 16000: Loss: 0.2227, requires_grad: True\n","Step 16000: Gradients computed successfully\n","Step 16010: Loss: 0.1872, requires_grad: True\n","Step 16020: Loss: 0.3743, requires_grad: True\n","Step 16030: Loss: 0.3056, requires_grad: True\n","Step 16040: Loss: 0.1590, requires_grad: True\n","Step 16050: Loss: 0.3015, requires_grad: True\n","Step 16050: Gradients computed successfully\n","Step 16060: Loss: 0.2936, requires_grad: True\n","Step 16070: Loss: 0.1304, requires_grad: True\n","Step 16080: Loss: 0.3402, requires_grad: True\n","Step 16090: Loss: 0.3432, requires_grad: True\n","Step 16100: Loss: 0.2142, requires_grad: True\n","Step 16100: Gradients computed successfully\n","Step 16110: Loss: 0.3626, requires_grad: True\n","Step 16120: Loss: 0.2210, requires_grad: True\n","Step 16130: Loss: 0.1874, requires_grad: True\n","Step 16140: Loss: 0.2066, requires_grad: True\n","Step 16150: Loss: 0.1835, requires_grad: True\n","Step 16150: Gradients computed successfully\n","Step 16160: Loss: 0.4732, requires_grad: True\n","Step 16170: Loss: 0.2242, requires_grad: True\n","Step 16180: Loss: 0.1600, requires_grad: True\n","Step 16190: Loss: 0.1666, requires_grad: True\n","Step 16200: Loss: 0.3656, requires_grad: True\n","Step 16200: Gradients computed successfully\n","Step 16210: Loss: 0.3698, requires_grad: True\n","Step 16220: Loss: 0.3089, requires_grad: True\n","Step 16230: Loss: 0.2140, requires_grad: True\n","Step 16240: Loss: 0.4648, requires_grad: True\n","Step 16250: Loss: 0.3412, requires_grad: True\n","Step 16250: Gradients computed successfully\n","Step 16260: Loss: 0.2756, requires_grad: True\n","Step 16270: Loss: 0.3387, requires_grad: True\n","Step 16280: Loss: 0.3775, requires_grad: True\n","Step 16290: Loss: 0.2303, requires_grad: True\n","Step 16300: Loss: 0.1281, requires_grad: True\n","Step 16300: Gradients computed successfully\n","Step 16310: Loss: 0.2295, requires_grad: True\n","Step 16320: Loss: 0.2646, requires_grad: True\n","Step 16330: Loss: 0.1010, requires_grad: True\n","Step 16340: Loss: 0.3376, requires_grad: True\n","Step 16350: Loss: 0.2187, requires_grad: True\n","Step 16350: Gradients computed successfully\n","Step 16360: Loss: 0.1898, requires_grad: True\n","Step 16370: Loss: 0.3648, requires_grad: True\n","Step 16380: Loss: 0.3242, requires_grad: True\n","Step 16390: Loss: 0.1912, requires_grad: True\n","Step 16400: Loss: 0.3163, requires_grad: True\n","Step 16400: Gradients computed successfully\n","Step 16410: Loss: 0.0711, requires_grad: True\n","Step 16420: Loss: 0.1527, requires_grad: True\n","Step 16430: Loss: 0.4463, requires_grad: True\n","Step 16440: Loss: 0.1414, requires_grad: True\n","Step 16450: Loss: 0.1595, requires_grad: True\n","Step 16450: Gradients computed successfully\n","Step 16460: Loss: 0.2761, requires_grad: True\n","Step 16470: Loss: 0.3158, requires_grad: True\n","Step 16480: Loss: 0.4217, requires_grad: True\n","Step 16490: Loss: 0.5867, requires_grad: True\n","Step 16500: Loss: 0.2143, requires_grad: True\n","Step 16500: Gradients computed successfully\n","Step 16510: Loss: 0.5655, requires_grad: True\n","Step 16520: Loss: 0.1296, requires_grad: True\n","Step 16530: Loss: 0.1241, requires_grad: True\n","Step 16540: Loss: 0.1851, requires_grad: True\n","Step 16550: Loss: 0.0983, requires_grad: True\n","Step 16550: Gradients computed successfully\n","Step 16560: Loss: 0.2046, requires_grad: True\n","Step 16570: Loss: 0.2355, requires_grad: True\n","Step 16580: Loss: 0.1594, requires_grad: True\n","Step 16590: Loss: 0.4974, requires_grad: True\n","Step 16600: Loss: 0.1838, requires_grad: True\n","Step 16600: Gradients computed successfully\n","Step 16610: Loss: 0.2232, requires_grad: True\n","Step 16620: Loss: 0.1143, requires_grad: True\n","Step 16630: Loss: 0.0879, requires_grad: True\n","Step 16640: Loss: 0.1868, requires_grad: True\n","Step 16650: Loss: 0.1483, requires_grad: True\n","Step 16650: Gradients computed successfully\n","Step 16660: Loss: 0.3387, requires_grad: True\n","Step 16670: Loss: 0.1761, requires_grad: True\n","Step 16680: Loss: 0.2218, requires_grad: True\n","Step 16690: Loss: 0.2695, requires_grad: True\n","Step 16700: Loss: 0.1251, requires_grad: True\n","Step 16700: Gradients computed successfully\n","Step 16710: Loss: 0.0964, requires_grad: True\n","Step 16720: Loss: 0.1210, requires_grad: True\n","Step 16730: Loss: 0.4246, requires_grad: True\n","Step 16740: Loss: 0.1647, requires_grad: True\n","Step 16750: Loss: 0.4104, requires_grad: True\n","Step 16750: Gradients computed successfully\n","Step 16760: Loss: 0.2008, requires_grad: True\n","Step 16770: Loss: 0.3460, requires_grad: True\n","Step 16780: Loss: 0.1873, requires_grad: True\n","Step 16790: Loss: 0.1955, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-2100\n","Step 16800: Loss: 0.2334, requires_grad: True\n","Step 16800: Gradients computed successfully\n","Step 16810: Loss: 0.1927, requires_grad: True\n","Step 16820: Loss: 0.5403, requires_grad: True\n","Step 16830: Loss: 0.1308, requires_grad: True\n","Step 16840: Loss: 0.3911, requires_grad: True\n","Step 16850: Loss: 0.1117, requires_grad: True\n","Step 16850: Gradients computed successfully\n","Step 16860: Loss: 0.1961, requires_grad: True\n","Step 16870: Loss: 0.1256, requires_grad: True\n","Step 16880: Loss: 0.2245, requires_grad: True\n","Step 16890: Loss: 0.2295, requires_grad: True\n","Step 16900: Loss: 0.2633, requires_grad: True\n","Step 16900: Gradients computed successfully\n","Step 16910: Loss: 0.2416, requires_grad: True\n","Step 16920: Loss: 0.2340, requires_grad: True\n","Step 16930: Loss: 0.1333, requires_grad: True\n","Step 16940: Loss: 0.0662, requires_grad: True\n","Step 16950: Loss: 0.2077, requires_grad: True\n","Step 16950: Gradients computed successfully\n","Step 16960: Loss: 0.2190, requires_grad: True\n","Step 16970: Loss: 0.1330, requires_grad: True\n","Step 16980: Loss: 0.1372, requires_grad: True\n","Step 16990: Loss: 0.2277, requires_grad: True\n","Step 17000: Loss: 0.1262, requires_grad: True\n","Step 17000: Gradients computed successfully\n","Step 17010: Loss: 0.1107, requires_grad: True\n","Step 17020: Loss: 0.0979, requires_grad: True\n","Step 17030: Loss: 0.1591, requires_grad: True\n","Step 17040: Loss: 0.1387, requires_grad: True\n","Step 17050: Loss: 0.1415, requires_grad: True\n","Step 17050: Gradients computed successfully\n","Step 17060: Loss: 0.2689, requires_grad: True\n","Step 17070: Loss: 0.1811, requires_grad: True\n","Step 17080: Loss: 0.0795, requires_grad: True\n","Step 17090: Loss: 0.1555, requires_grad: True\n","Step 17100: Loss: 0.2340, requires_grad: True\n","Step 17100: Gradients computed successfully\n","Step 17110: Loss: 0.1735, requires_grad: True\n","Step 17120: Loss: 0.5428, requires_grad: True\n","Step 17130: Loss: 0.1263, requires_grad: True\n","Step 17140: Loss: 0.2533, requires_grad: True\n","Step 17150: Loss: 0.3840, requires_grad: True\n","Step 17150: Gradients computed successfully\n","Step 17160: Loss: 0.5390, requires_grad: True\n","Step 17170: Loss: 0.2038, requires_grad: True\n","Step 17180: Loss: 0.2004, requires_grad: True\n","Step 17190: Loss: 0.3497, requires_grad: True\n","Step 17200: Loss: 0.4467, requires_grad: True\n","Step 17200: Gradients computed successfully\n","Step 17210: Loss: 0.2712, requires_grad: True\n","Step 17220: Loss: 0.2196, requires_grad: True\n","Step 17230: Loss: 0.1913, requires_grad: True\n","Step 17240: Loss: 0.2962, requires_grad: True\n","Step 17250: Loss: 0.1895, requires_grad: True\n","Step 17250: Gradients computed successfully\n","Step 17260: Loss: 0.2227, requires_grad: True\n","Step 17270: Loss: 0.3099, requires_grad: True\n","Step 17280: Loss: 0.4059, requires_grad: True\n","Step 17290: Loss: 0.1370, requires_grad: True\n","Step 17300: Loss: 0.3307, requires_grad: True\n","Step 17300: Gradients computed successfully\n","Step 17310: Loss: 0.3286, requires_grad: True\n","Step 17320: Loss: 0.3088, requires_grad: True\n","Step 17330: Loss: 0.1815, requires_grad: True\n","Step 17340: Loss: 0.1524, requires_grad: True\n","Step 17350: Loss: 0.2475, requires_grad: True\n","Step 17350: Gradients computed successfully\n","Step 17360: Loss: 0.1328, requires_grad: True\n","Step 17370: Loss: 0.7098, requires_grad: True\n","Step 17380: Loss: 0.3281, requires_grad: True\n","Step 17390: Loss: 0.1569, requires_grad: True\n","Step 17400: Loss: 0.3163, requires_grad: True\n","Step 17400: Gradients computed successfully\n","Step 17410: Loss: 0.0855, requires_grad: True\n","Step 17420: Loss: 0.3264, requires_grad: True\n","Step 17430: Loss: 0.2742, requires_grad: True\n","Step 17440: Loss: 0.1795, requires_grad: True\n","Step 17450: Loss: 0.2915, requires_grad: True\n","Step 17450: Gradients computed successfully\n","Step 17460: Loss: 0.2555, requires_grad: True\n","Step 17470: Loss: 0.2126, requires_grad: True\n","Step 17480: Loss: 0.3837, requires_grad: True\n","Step 17490: Loss: 0.5709, requires_grad: True\n","Step 17500: Loss: 0.3326, requires_grad: True\n","Step 17500: Gradients computed successfully\n","Step 17510: Loss: 0.1514, requires_grad: True\n","Step 17520: Loss: 0.0735, requires_grad: True\n","Step 17530: Loss: 0.4013, requires_grad: True\n","Step 17540: Loss: 0.2114, requires_grad: True\n","Step 17550: Loss: 0.3291, requires_grad: True\n","Step 17550: Gradients computed successfully\n","Step 17560: Loss: 0.1462, requires_grad: True\n","Step 17570: Loss: 0.3667, requires_grad: True\n","Step 17580: Loss: 0.2570, requires_grad: True\n","Step 17590: Loss: 0.1345, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-2200\n","Step 17600: Loss: 0.4234, requires_grad: True\n","Step 17600: Gradients computed successfully\n","Step 17610: Loss: 0.2552, requires_grad: True\n","Step 17620: Loss: 0.2981, requires_grad: True\n","Step 17630: Loss: 0.4231, requires_grad: True\n","Step 17640: Loss: 0.1860, requires_grad: True\n","Step 17650: Loss: 0.2974, requires_grad: True\n","Step 17650: Gradients computed successfully\n","Step 17660: Loss: 0.1579, requires_grad: True\n","Step 17670: Loss: 0.3192, requires_grad: True\n","Step 17680: Loss: 0.2087, requires_grad: True\n","Step 17690: Loss: 0.4332, requires_grad: True\n","Step 17700: Loss: 0.2866, requires_grad: True\n","Step 17700: Gradients computed successfully\n","Step 17710: Loss: 0.2233, requires_grad: True\n","Step 17720: Loss: 0.2574, requires_grad: True\n","Step 17730: Loss: 0.2660, requires_grad: True\n","Step 17740: Loss: 0.2550, requires_grad: True\n","Step 17750: Loss: 0.2027, requires_grad: True\n","Step 17750: Gradients computed successfully\n","Step 17760: Loss: 0.1990, requires_grad: True\n","Step 17770: Loss: 0.2618, requires_grad: True\n","Step 17780: Loss: 0.3838, requires_grad: True\n","Step 17790: Loss: 0.1185, requires_grad: True\n","Step 17800: Loss: 0.1524, requires_grad: True\n","Step 17800: Gradients computed successfully\n","Step 17810: Loss: 0.3374, requires_grad: True\n","Step 17820: Loss: 0.1887, requires_grad: True\n","Step 17830: Loss: 0.1924, requires_grad: True\n","Step 17840: Loss: 0.2453, requires_grad: True\n","Step 17850: Loss: 0.3486, requires_grad: True\n","Step 17850: Gradients computed successfully\n","Step 17860: Loss: 0.2203, requires_grad: True\n","Step 17870: Loss: 0.1766, requires_grad: True\n","Step 17880: Loss: 0.2602, requires_grad: True\n","Step 17890: Loss: 0.3114, requires_grad: True\n","Step 17900: Loss: 0.2521, requires_grad: True\n","Step 17900: Gradients computed successfully\n","Step 17910: Loss: 0.2063, requires_grad: True\n","Step 17920: Loss: 0.1432, requires_grad: True\n","Step 17930: Loss: 0.1501, requires_grad: True\n","Step 17940: Loss: 0.2859, requires_grad: True\n","Step 17950: Loss: 0.2324, requires_grad: True\n","Step 17950: Gradients computed successfully\n","Step 17960: Loss: 0.1569, requires_grad: True\n","Step 17970: Loss: 0.1255, requires_grad: True\n","Step 17980: Loss: 0.1320, requires_grad: True\n","Step 17990: Loss: 0.2460, requires_grad: True\n","Step 18000: Loss: 0.1512, requires_grad: True\n","Step 18000: Gradients computed successfully\n","Step 18010: Loss: 0.1996, requires_grad: True\n","Step 18020: Loss: 0.1899, requires_grad: True\n","Step 18030: Loss: 0.2278, requires_grad: True\n","Step 18040: Loss: 0.1748, requires_grad: True\n","Step 18050: Loss: 0.2394, requires_grad: True\n","Step 18050: Gradients computed successfully\n","Step 18060: Loss: 0.1692, requires_grad: True\n","Step 18070: Loss: 0.3192, requires_grad: True\n","Step 18080: Loss: 0.1382, requires_grad: True\n","Step 18090: Loss: 0.1449, requires_grad: True\n","Step 18100: Loss: 0.2188, requires_grad: True\n","Step 18100: Gradients computed successfully\n","Step 18110: Loss: 0.1560, requires_grad: True\n","Step 18120: Loss: 0.2652, requires_grad: True\n","Step 18130: Loss: 0.3839, requires_grad: True\n","Step 18140: Loss: 0.1939, requires_grad: True\n","Step 18150: Loss: 0.1416, requires_grad: True\n","Step 18150: Gradients computed successfully\n","Step 18160: Loss: 0.1996, requires_grad: True\n","Step 18170: Loss: 0.2283, requires_grad: True\n","Step 18180: Loss: 0.1871, requires_grad: True\n","Step 18190: Loss: 0.2395, requires_grad: True\n","Step 18200: Loss: 0.1076, requires_grad: True\n","Step 18200: Gradients computed successfully\n","Step 18210: Loss: 0.0686, requires_grad: True\n","Step 18220: Loss: 0.4001, requires_grad: True\n","Step 18230: Loss: 0.2613, requires_grad: True\n","Step 18240: Loss: 0.0700, requires_grad: True\n","Step 18250: Loss: 0.3057, requires_grad: True\n","Step 18250: Gradients computed successfully\n","Step 18260: Loss: 0.1410, requires_grad: True\n","Step 18270: Loss: 0.4143, requires_grad: True\n","Step 18280: Loss: 0.2511, requires_grad: True\n","Step 18290: Loss: 0.2846, requires_grad: True\n","Step 18300: Loss: 0.1341, requires_grad: True\n","Step 18300: Gradients computed successfully\n","Step 18310: Loss: 0.2685, requires_grad: True\n","Step 18320: Loss: 0.4142, requires_grad: True\n","Step 18330: Loss: 0.1655, requires_grad: True\n","Step 18340: Loss: 0.1718, requires_grad: True\n","Step 18350: Loss: 0.2738, requires_grad: True\n","Step 18350: Gradients computed successfully\n","Step 18360: Loss: 0.2417, requires_grad: True\n","Step 18370: Loss: 0.1431, requires_grad: True\n","Step 18380: Loss: 0.2318, requires_grad: True\n","Step 18390: Loss: 0.1080, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-2300\n","Step 18400: Loss: 0.1261, requires_grad: True\n","Step 18400: Gradients computed successfully\n","Step 18410: Loss: 0.1538, requires_grad: True\n","Step 18420: Loss: 0.1742, requires_grad: True\n","Step 18430: Loss: 0.1963, requires_grad: True\n","Step 18440: Loss: 0.3006, requires_grad: True\n","Step 18450: Loss: 0.0981, requires_grad: True\n","Step 18450: Gradients computed successfully\n","Step 18460: Loss: 0.1064, requires_grad: True\n","Step 18470: Loss: 0.3888, requires_grad: True\n","Step 18480: Loss: 0.1727, requires_grad: True\n","Step 18490: Loss: 0.2937, requires_grad: True\n","Step 18500: Loss: 0.1060, requires_grad: True\n","Step 18500: Gradients computed successfully\n","Step 18510: Loss: 0.3035, requires_grad: True\n","Step 18520: Loss: 0.0967, requires_grad: True\n","Step 18530: Loss: 0.2084, requires_grad: True\n","Step 18540: Loss: 0.1287, requires_grad: True\n","Step 18550: Loss: 0.3000, requires_grad: True\n","Step 18550: Gradients computed successfully\n","Step 18560: Loss: 0.0863, requires_grad: True\n","Step 18570: Loss: 0.2033, requires_grad: True\n","Step 18580: Loss: 0.3727, requires_grad: True\n","Step 18590: Loss: 0.2117, requires_grad: True\n","Step 18600: Loss: 0.1762, requires_grad: True\n","Step 18600: Gradients computed successfully\n","Step 18610: Loss: 0.1969, requires_grad: True\n","Step 18620: Loss: 0.4701, requires_grad: True\n","Step 18630: Loss: 0.4422, requires_grad: True\n","Step 18640: Loss: 0.0637, requires_grad: True\n","Step 18650: Loss: 0.2902, requires_grad: True\n","Step 18650: Gradients computed successfully\n","Step 18660: Loss: 0.1682, requires_grad: True\n","Step 18670: Loss: 0.4558, requires_grad: True\n","Step 18680: Loss: 0.0956, requires_grad: True\n","Step 18690: Loss: 0.1285, requires_grad: True\n","Step 18700: Loss: 0.2927, requires_grad: True\n","Step 18700: Gradients computed successfully\n","Step 18710: Loss: 0.2485, requires_grad: True\n","Step 18720: Loss: 0.0763, requires_grad: True\n","Step 18730: Loss: 0.1658, requires_grad: True\n","Step 18740: Loss: 0.1934, requires_grad: True\n","Step 18750: Loss: 0.1409, requires_grad: True\n","Step 18750: Gradients computed successfully\n","Step 18760: Loss: 0.1379, requires_grad: True\n","Step 18770: Loss: 0.1079, requires_grad: True\n","Step 18780: Loss: 0.1199, requires_grad: True\n","Step 18790: Loss: 0.1400, requires_grad: True\n","Step 18800: Loss: 0.1623, requires_grad: True\n","Step 18800: Gradients computed successfully\n","Step 18810: Loss: 0.2727, requires_grad: True\n","Step 18820: Loss: 0.2103, requires_grad: True\n","Step 18830: Loss: 0.4003, requires_grad: True\n","Step 18840: Loss: 0.2815, requires_grad: True\n","Step 18850: Loss: 0.2307, requires_grad: True\n","Step 18850: Gradients computed successfully\n","Step 18860: Loss: 0.0744, requires_grad: True\n","Step 18870: Loss: 0.2123, requires_grad: True\n","Step 18880: Loss: 0.3541, requires_grad: True\n","Step 18890: Loss: 0.1326, requires_grad: True\n","Step 18900: Loss: 0.1694, requires_grad: True\n","Step 18900: Gradients computed successfully\n","Step 18910: Loss: 0.1714, requires_grad: True\n","Step 18920: Loss: 0.1249, requires_grad: True\n","Step 18930: Loss: 0.2867, requires_grad: True\n","Step 18940: Loss: 0.1553, requires_grad: True\n","Step 18950: Loss: 0.3161, requires_grad: True\n","Step 18950: Gradients computed successfully\n","Step 18960: Loss: 0.4707, requires_grad: True\n","Step 18970: Loss: 0.2629, requires_grad: True\n","Step 18980: Loss: 0.2588, requires_grad: True\n","Step 18990: Loss: 0.0881, requires_grad: True\n","Step 19000: Loss: 0.2253, requires_grad: True\n","Step 19000: Gradients computed successfully\n","Step 19010: Loss: 0.0939, requires_grad: True\n","Step 19020: Loss: 0.3102, requires_grad: True\n","Step 19030: Loss: 0.2877, requires_grad: True\n","Step 19040: Loss: 0.4306, requires_grad: True\n","Step 19050: Loss: 0.1307, requires_grad: True\n","Step 19050: Gradients computed successfully\n","Step 19060: Loss: 0.2146, requires_grad: True\n","Step 19070: Loss: 0.1087, requires_grad: True\n","Step 19080: Loss: 0.3047, requires_grad: True\n","Step 19090: Loss: 0.1673, requires_grad: True\n","Step 19100: Loss: 0.1373, requires_grad: True\n","Step 19100: Gradients computed successfully\n","Step 19110: Loss: 0.4451, requires_grad: True\n","Step 19120: Loss: 0.2084, requires_grad: True\n","Step 19130: Loss: 0.3195, requires_grad: True\n","Step 19140: Loss: 0.1920, requires_grad: True\n","Step 19150: Loss: 0.0728, requires_grad: True\n","Step 19150: Gradients computed successfully\n","Step 19160: Loss: 0.8327, requires_grad: True\n","Step 19170: Loss: 0.2832, requires_grad: True\n","Step 19180: Loss: 0.2153, requires_grad: True\n","Step 19190: Loss: 0.3693, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-2400\n","Step 19200: Loss: 0.1828, requires_grad: True\n","Step 19200: Gradients computed successfully\n","Step 19210: Loss: 0.2438, requires_grad: True\n","Step 19220: Loss: 0.2588, requires_grad: True\n","Step 19230: Loss: 0.3146, requires_grad: True\n","Step 19240: Loss: 0.4879, requires_grad: True\n","Step 19250: Loss: 0.2258, requires_grad: True\n","Step 19250: Gradients computed successfully\n","Step 19260: Loss: 0.2063, requires_grad: True\n","Step 19270: Loss: 0.0834, requires_grad: True\n","Step 19280: Loss: 0.3119, requires_grad: True\n","Step 19290: Loss: 0.2893, requires_grad: True\n","Step 19300: Loss: 0.3998, requires_grad: True\n","Step 19300: Gradients computed successfully\n","Step 19310: Loss: 0.3363, requires_grad: True\n","Step 19320: Loss: 0.1551, requires_grad: True\n","Step 19330: Loss: 0.2859, requires_grad: True\n","Step 19340: Loss: 0.3081, requires_grad: True\n","Step 19350: Loss: 0.1471, requires_grad: True\n","Step 19350: Gradients computed successfully\n","Step 19360: Loss: 0.1509, requires_grad: True\n","Step 19370: Loss: 0.2257, requires_grad: True\n","Step 19380: Loss: 0.1586, requires_grad: True\n","Step 19390: Loss: 0.2200, requires_grad: True\n","Step 19400: Loss: 0.1275, requires_grad: True\n","Step 19400: Gradients computed successfully\n","Step 19410: Loss: 0.2563, requires_grad: True\n","Step 19420: Loss: 0.3830, requires_grad: True\n","Step 19430: Loss: 0.1730, requires_grad: True\n","Step 19440: Loss: 0.2404, requires_grad: True\n","Step 19450: Loss: 0.1122, requires_grad: True\n","Step 19450: Gradients computed successfully\n","Step 19460: Loss: 0.1541, requires_grad: True\n","Step 19470: Loss: 0.1489, requires_grad: True\n","Step 19480: Loss: 0.3342, requires_grad: True\n","Step 19490: Loss: 0.3285, requires_grad: True\n","Step 19500: Loss: 0.2012, requires_grad: True\n","Step 19500: Gradients computed successfully\n","Step 19510: Loss: 0.2217, requires_grad: True\n","Step 19520: Loss: 0.1916, requires_grad: True\n","Step 19530: Loss: 0.2130, requires_grad: True\n","Step 19540: Loss: 0.4413, requires_grad: True\n","Step 19550: Loss: 0.3920, requires_grad: True\n","Step 19550: Gradients computed successfully\n","Step 19560: Loss: 0.3083, requires_grad: True\n","Step 19570: Loss: 0.1610, requires_grad: True\n","Step 19580: Loss: 0.4072, requires_grad: True\n","Step 19590: Loss: 0.0552, requires_grad: True\n","Step 19600: Loss: 0.2396, requires_grad: True\n","Step 19600: Gradients computed successfully\n","Step 19610: Loss: 0.0961, requires_grad: True\n","Step 19620: Loss: 0.1976, requires_grad: True\n","Step 19630: Loss: 0.1987, requires_grad: True\n","Step 19640: Loss: 0.4046, requires_grad: True\n","Step 19650: Loss: 0.0786, requires_grad: True\n","Step 19650: Gradients computed successfully\n","Step 19660: Loss: 0.1251, requires_grad: True\n","Step 19670: Loss: 0.3217, requires_grad: True\n","Step 19680: Loss: 0.1379, requires_grad: True\n","Step 19690: Loss: 0.2157, requires_grad: True\n","Step 19700: Loss: 0.1702, requires_grad: True\n","Step 19700: Gradients computed successfully\n","Step 19710: Loss: 0.1206, requires_grad: True\n","Step 19720: Loss: 0.2286, requires_grad: True\n","Step 19730: Loss: 0.1347, requires_grad: True\n","Step 19740: Loss: 0.4359, requires_grad: True\n","Step 19750: Loss: 0.2917, requires_grad: True\n","Step 19750: Gradients computed successfully\n","Step 19760: Loss: 0.1513, requires_grad: True\n","Step 19770: Loss: 0.2079, requires_grad: True\n","Step 19780: Loss: 0.1596, requires_grad: True\n","Step 19790: Loss: 0.1433, requires_grad: True\n","Step 19800: Loss: 0.1701, requires_grad: True\n","Step 19800: Gradients computed successfully\n","Step 19810: Loss: 0.2875, requires_grad: True\n","Step 19820: Loss: 0.0877, requires_grad: True\n","Step 19830: Loss: 0.1650, requires_grad: True\n","Step 19840: Loss: 0.2973, requires_grad: True\n","Step 19850: Loss: 0.1230, requires_grad: True\n","Step 19850: Gradients computed successfully\n","Step 19860: Loss: 0.2242, requires_grad: True\n","Step 19870: Loss: 0.1102, requires_grad: True\n","Step 19880: Loss: 0.0864, requires_grad: True\n","Step 19890: Loss: 0.2429, requires_grad: True\n","Step 19900: Loss: 0.1840, requires_grad: True\n","Step 19900: Gradients computed successfully\n","Step 19910: Loss: 0.1513, requires_grad: True\n","Step 19920: Loss: 0.2039, requires_grad: True\n","Step 19930: Loss: 0.2031, requires_grad: True\n","Step 19940: Loss: 0.4143, requires_grad: True\n","Step 19950: Loss: 0.1514, requires_grad: True\n","Step 19950: Gradients computed successfully\n","Step 19960: Loss: 0.1368, requires_grad: True\n","Step 19970: Loss: 0.0853, requires_grad: True\n","Step 19980: Loss: 0.1291, requires_grad: True\n","Step 19990: Loss: 0.3191, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-2500\n","Step 20000: Loss: 0.2387, requires_grad: True\n","Step 20000: Gradients computed successfully\n","Step 20010: Loss: 0.1552, requires_grad: True\n","Step 20020: Loss: 0.1007, requires_grad: True\n","Step 20030: Loss: 0.3159, requires_grad: True\n","Step 20040: Loss: 0.2691, requires_grad: True\n","Step 20050: Loss: 0.1129, requires_grad: True\n","Step 20050: Gradients computed successfully\n","Step 20060: Loss: 0.1363, requires_grad: True\n","Step 20070: Loss: 0.3546, requires_grad: True\n","Step 20080: Loss: 0.1569, requires_grad: True\n","Step 20090: Loss: 0.1518, requires_grad: True\n","Step 20100: Loss: 0.3495, requires_grad: True\n","Step 20100: Gradients computed successfully\n","Step 20110: Loss: 0.2237, requires_grad: True\n","Step 20120: Loss: 0.3001, requires_grad: True\n","Step 20130: Loss: 0.2382, requires_grad: True\n","Step 20140: Loss: 0.3048, requires_grad: True\n","Step 20150: Loss: 0.1572, requires_grad: True\n","Step 20150: Gradients computed successfully\n","Step 20160: Loss: 0.2135, requires_grad: True\n","Step 20170: Loss: 0.1968, requires_grad: True\n","Step 20180: Loss: 0.1083, requires_grad: True\n","Step 20190: Loss: 0.1384, requires_grad: True\n","Step 20200: Loss: 0.1442, requires_grad: True\n","Step 20200: Gradients computed successfully\n","Step 20210: Loss: 0.3460, requires_grad: True\n","Step 20220: Loss: 0.1692, requires_grad: True\n","Step 20230: Loss: 0.2049, requires_grad: True\n","Step 20240: Loss: 0.1727, requires_grad: True\n","Step 20250: Loss: 0.1609, requires_grad: True\n","Step 20250: Gradients computed successfully\n","Step 20260: Loss: 0.2664, requires_grad: True\n","Step 20270: Loss: 0.0919, requires_grad: True\n","Step 20280: Loss: 0.2628, requires_grad: True\n","Step 20290: Loss: 0.2418, requires_grad: True\n","Step 20300: Loss: 0.0529, requires_grad: True\n","Step 20300: Gradients computed successfully\n","Step 20310: Loss: 0.1262, requires_grad: True\n","Step 20320: Loss: 0.0946, requires_grad: True\n","Step 20330: Loss: 0.0870, requires_grad: True\n","Step 20340: Loss: 0.3012, requires_grad: True\n","Step 20350: Loss: 0.1096, requires_grad: True\n","Step 20350: Gradients computed successfully\n","Step 20360: Loss: 0.3203, requires_grad: True\n","Step 20370: Loss: 0.4280, requires_grad: True\n","Step 20380: Loss: 0.2330, requires_grad: True\n","Step 20390: Loss: 0.1007, requires_grad: True\n","Step 20400: Loss: 0.3639, requires_grad: True\n","Step 20400: Gradients computed successfully\n","Step 20410: Loss: 0.2831, requires_grad: True\n","Step 20420: Loss: 0.1659, requires_grad: True\n","Step 20430: Loss: 0.2807, requires_grad: True\n","Step 20440: Loss: 0.1277, requires_grad: True\n","Step 20450: Loss: 0.1242, requires_grad: True\n","Step 20450: Gradients computed successfully\n","Step 20460: Loss: 0.1873, requires_grad: True\n","Step 20470: Loss: 0.1581, requires_grad: True\n","Step 20480: Loss: 0.2584, requires_grad: True\n","Step 20490: Loss: 0.2099, requires_grad: True\n","Step 20500: Loss: 0.2980, requires_grad: True\n","Step 20500: Gradients computed successfully\n","Step 20510: Loss: 0.4430, requires_grad: True\n","Step 20520: Loss: 0.1652, requires_grad: True\n","Step 20530: Loss: 0.2573, requires_grad: True\n","Step 20540: Loss: 0.1985, requires_grad: True\n","Step 20550: Loss: 0.2987, requires_grad: True\n","Step 20550: Gradients computed successfully\n","Step 20560: Loss: 0.0423, requires_grad: True\n","Step 20570: Loss: 0.3358, requires_grad: True\n","Step 20580: Loss: 0.1879, requires_grad: True\n","Step 20590: Loss: 0.1540, requires_grad: True\n","Step 20600: Loss: 0.2327, requires_grad: True\n","Step 20600: Gradients computed successfully\n","Step 20610: Loss: 0.1750, requires_grad: True\n","Step 20620: Loss: 0.1832, requires_grad: True\n","Step 20630: Loss: 0.2243, requires_grad: True\n","Step 20640: Loss: 0.1989, requires_grad: True\n","Step 20650: Loss: 0.3375, requires_grad: True\n","Step 20650: Gradients computed successfully\n","Step 20660: Loss: 0.2023, requires_grad: True\n","Step 20670: Loss: 0.3935, requires_grad: True\n","Step 20680: Loss: 0.1891, requires_grad: True\n","Step 20690: Loss: 0.2003, requires_grad: True\n","Step 20700: Loss: 0.2283, requires_grad: True\n","Step 20700: Gradients computed successfully\n","Step 20710: Loss: 0.1814, requires_grad: True\n","Step 20720: Loss: 0.1557, requires_grad: True\n","Step 20730: Loss: 0.4473, requires_grad: True\n","Step 20740: Loss: 0.2242, requires_grad: True\n","Step 20750: Loss: 0.4711, requires_grad: True\n","Step 20750: Gradients computed successfully\n","Step 20760: Loss: 0.2927, requires_grad: True\n","Step 20770: Loss: 0.1119, requires_grad: True\n","Step 20780: Loss: 0.2196, requires_grad: True\n","Step 20790: Loss: 0.3011, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-2600\n","Step 20800: Loss: 0.1241, requires_grad: True\n","Step 20800: Gradients computed successfully\n","Step 20810: Loss: 0.4961, requires_grad: True\n","Step 20820: Loss: 0.2468, requires_grad: True\n","Step 20830: Loss: 0.4209, requires_grad: True\n","Step 20840: Loss: 0.2891, requires_grad: True\n","Step 20850: Loss: 0.3780, requires_grad: True\n","Step 20850: Gradients computed successfully\n","Step 20860: Loss: 0.2438, requires_grad: True\n","Step 20870: Loss: 0.2656, requires_grad: True\n","Step 20880: Loss: 0.2885, requires_grad: True\n","Step 20890: Loss: 0.2472, requires_grad: True\n","Step 20900: Loss: 0.2593, requires_grad: True\n","Step 20900: Gradients computed successfully\n","Step 20910: Loss: 0.1917, requires_grad: True\n","Step 20920: Loss: 0.1496, requires_grad: True\n","Step 20930: Loss: 0.2081, requires_grad: True\n","Step 20940: Loss: 0.1164, requires_grad: True\n","Step 20950: Loss: 0.3630, requires_grad: True\n","Step 20950: Gradients computed successfully\n","Step 20960: Loss: 0.1877, requires_grad: True\n","Step 20970: Loss: 0.2290, requires_grad: True\n","Step 20980: Loss: 0.1140, requires_grad: True\n","Step 20990: Loss: 0.1342, requires_grad: True\n","Step 21000: Loss: 0.1375, requires_grad: True\n","Step 21000: Gradients computed successfully\n","Step 21010: Loss: 0.0840, requires_grad: True\n","Step 21020: Loss: 0.2732, requires_grad: True\n","Step 21030: Loss: 0.1596, requires_grad: True\n","Step 21040: Loss: 0.1908, requires_grad: True\n","Step 21050: Loss: 0.1148, requires_grad: True\n","Step 21050: Gradients computed successfully\n","Step 21060: Loss: 0.3018, requires_grad: True\n","Step 21070: Loss: 0.2213, requires_grad: True\n","Step 21080: Loss: 0.1003, requires_grad: True\n","Step 21090: Loss: 0.1298, requires_grad: True\n","Step 21100: Loss: 0.5404, requires_grad: True\n","Step 21100: Gradients computed successfully\n","Step 21110: Loss: 0.3596, requires_grad: True\n","Step 21120: Loss: 0.1720, requires_grad: True\n","Step 21130: Loss: 0.2946, requires_grad: True\n","Step 21140: Loss: 0.3528, requires_grad: True\n","Step 21150: Loss: 0.0836, requires_grad: True\n","Step 21150: Gradients computed successfully\n","Step 21160: Loss: 0.1104, requires_grad: True\n","Step 21170: Loss: 0.2923, requires_grad: True\n","Step 21180: Loss: 0.3053, requires_grad: True\n","Step 21190: Loss: 0.1257, requires_grad: True\n","Step 21200: Loss: 0.1351, requires_grad: True\n","Step 21200: Gradients computed successfully\n","Step 21210: Loss: 0.1801, requires_grad: True\n","Step 21220: Loss: 0.2058, requires_grad: True\n","Step 21230: Loss: 0.4480, requires_grad: True\n","Step 21240: Loss: 0.1945, requires_grad: True\n","Step 21250: Loss: 0.2358, requires_grad: True\n","Step 21250: Gradients computed successfully\n","Step 21260: Loss: 0.2289, requires_grad: True\n","Step 21270: Loss: 0.1657, requires_grad: True\n","Step 21280: Loss: 0.2972, requires_grad: True\n","Step 21290: Loss: 0.1898, requires_grad: True\n","Step 21300: Loss: 0.2543, requires_grad: True\n","Step 21300: Gradients computed successfully\n","Step 21310: Loss: 0.0899, requires_grad: True\n","Step 21320: Loss: 0.1773, requires_grad: True\n","Step 21330: Loss: 0.2247, requires_grad: True\n","Step 21340: Loss: 0.1747, requires_grad: True\n","Step 21350: Loss: 0.4044, requires_grad: True\n","Step 21350: Gradients computed successfully\n","Step 21360: Loss: 0.2820, requires_grad: True\n","Step 21370: Loss: 0.4456, requires_grad: True\n","Step 21380: Loss: 0.1506, requires_grad: True\n","Step 21390: Loss: 0.1290, requires_grad: True\n","Step 21400: Loss: 0.2229, requires_grad: True\n","Step 21400: Gradients computed successfully\n","Step 21410: Loss: 0.2207, requires_grad: True\n","Step 21420: Loss: 0.1287, requires_grad: True\n","Step 21430: Loss: 0.1701, requires_grad: True\n","Step 21440: Loss: 0.1781, requires_grad: True\n","Step 21450: Loss: 0.3207, requires_grad: True\n","Step 21450: Gradients computed successfully\n","Step 21460: Loss: 0.1123, requires_grad: True\n","Step 21470: Loss: 0.3127, requires_grad: True\n","Step 21480: Loss: 0.3034, requires_grad: True\n","Step 21490: Loss: 0.1924, requires_grad: True\n","Step 21500: Loss: 0.4975, requires_grad: True\n","Step 21500: Gradients computed successfully\n","Step 21510: Loss: 0.1207, requires_grad: True\n","Step 21520: Loss: 0.1445, requires_grad: True\n","Step 21530: Loss: 0.2401, requires_grad: True\n","Step 21540: Loss: 0.1682, requires_grad: True\n","Step 21550: Loss: 0.2334, requires_grad: True\n","Step 21550: Gradients computed successfully\n","Step 21560: Loss: 0.1855, requires_grad: True\n","Step 21570: Loss: 0.4454, requires_grad: True\n","Step 21580: Loss: 0.1959, requires_grad: True\n","Step 21590: Loss: 0.0866, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-2700\n","Step 21600: Loss: 0.1810, requires_grad: True\n","Step 21600: Gradients computed successfully\n","Step 21610: Loss: 0.0940, requires_grad: True\n","Step 21620: Loss: 0.1155, requires_grad: True\n","Step 21630: Loss: 0.5586, requires_grad: True\n","Step 21640: Loss: 0.1141, requires_grad: True\n","Step 21650: Loss: 0.1536, requires_grad: True\n","Step 21650: Gradients computed successfully\n","Step 21660: Loss: 0.2670, requires_grad: True\n","Step 21670: Loss: 0.1564, requires_grad: True\n","Step 21680: Loss: 0.5386, requires_grad: True\n","Step 21690: Loss: 0.2145, requires_grad: True\n","Step 21700: Loss: 0.1821, requires_grad: True\n","Step 21700: Gradients computed successfully\n","Step 21710: Loss: 0.0745, requires_grad: True\n","Step 21720: Loss: 0.2004, requires_grad: True\n","Step 21730: Loss: 0.2813, requires_grad: True\n","Step 21740: Loss: 0.2995, requires_grad: True\n","Step 21750: Loss: 0.1905, requires_grad: True\n","Step 21750: Gradients computed successfully\n","Step 21760: Loss: 0.2690, requires_grad: True\n","Step 21770: Loss: 0.1423, requires_grad: True\n","Step 21780: Loss: 0.1263, requires_grad: True\n","Step 21790: Loss: 0.1561, requires_grad: True\n","Step 21800: Loss: 0.3121, requires_grad: True\n","Step 21800: Gradients computed successfully\n","Step 21810: Loss: 0.4187, requires_grad: True\n","Step 21820: Loss: 0.4408, requires_grad: True\n","Step 21830: Loss: 0.1706, requires_grad: True\n","Step 21840: Loss: 0.2706, requires_grad: True\n","Step 21850: Loss: 0.0488, requires_grad: True\n","Step 21850: Gradients computed successfully\n","Step 21860: Loss: 0.1769, requires_grad: True\n","Step 21870: Loss: 0.2264, requires_grad: True\n","Step 21880: Loss: 0.4409, requires_grad: True\n","Step 21890: Loss: 0.1164, requires_grad: True\n","Step 21900: Loss: 0.1193, requires_grad: True\n","Step 21900: Gradients computed successfully\n","Step 21910: Loss: 0.1091, requires_grad: True\n","Step 21920: Loss: 0.2607, requires_grad: True\n","Step 21930: Loss: 0.2384, requires_grad: True\n","Step 21940: Loss: 0.0937, requires_grad: True\n","Step 21950: Loss: 0.2417, requires_grad: True\n","Step 21950: Gradients computed successfully\n","Step 21960: Loss: 0.1778, requires_grad: True\n","Step 21970: Loss: 0.1429, requires_grad: True\n","Step 21980: Loss: 0.7167, requires_grad: True\n","Step 21990: Loss: 0.2257, requires_grad: True\n","Step 22000: Loss: 0.1677, requires_grad: True\n","Step 22000: Gradients computed successfully\n","Step 22010: Loss: 0.2380, requires_grad: True\n","Step 22020: Loss: 0.1371, requires_grad: True\n","Step 22030: Loss: 0.2170, requires_grad: True\n","Step 22040: Loss: 0.3101, requires_grad: True\n","Step 22050: Loss: 0.1341, requires_grad: True\n","Step 22050: Gradients computed successfully\n","Step 22060: Loss: 0.1164, requires_grad: True\n","Step 22070: Loss: 0.1274, requires_grad: True\n","Step 22080: Loss: 0.1483, requires_grad: True\n","Step 22090: Loss: 0.1805, requires_grad: True\n","Step 22100: Loss: 0.0704, requires_grad: True\n","Step 22100: Gradients computed successfully\n","Step 22110: Loss: 0.1698, requires_grad: True\n","Step 22120: Loss: 0.1593, requires_grad: True\n","Step 22130: Loss: 0.4696, requires_grad: True\n","Step 22140: Loss: 0.2616, requires_grad: True\n","Step 22150: Loss: 0.5203, requires_grad: True\n","Step 22150: Gradients computed successfully\n","Step 22160: Loss: 0.1626, requires_grad: True\n","Step 22170: Loss: 0.1484, requires_grad: True\n","Step 22180: Loss: 0.4430, requires_grad: True\n","Step 22190: Loss: 0.2553, requires_grad: True\n","Step 22200: Loss: 0.4012, requires_grad: True\n","Step 22200: Gradients computed successfully\n","Step 22210: Loss: 0.2541, requires_grad: True\n","Step 22220: Loss: 0.1053, requires_grad: True\n","Step 22230: Loss: 0.6092, requires_grad: True\n","Step 22240: Loss: 0.0893, requires_grad: True\n","Step 22250: Loss: 0.1863, requires_grad: True\n","Step 22250: Gradients computed successfully\n","Step 22260: Loss: 0.2884, requires_grad: True\n","Step 22270: Loss: 0.4358, requires_grad: True\n","Step 22280: Loss: 0.3367, requires_grad: True\n","Step 22290: Loss: 0.2205, requires_grad: True\n","Step 22300: Loss: 0.3346, requires_grad: True\n","Step 22300: Gradients computed successfully\n","Step 22310: Loss: 0.4350, requires_grad: True\n","Step 22320: Loss: 0.2269, requires_grad: True\n","Step 22330: Loss: 0.1478, requires_grad: True\n","Step 22340: Loss: 0.2152, requires_grad: True\n","Step 22350: Loss: 0.1696, requires_grad: True\n","Step 22350: Gradients computed successfully\n","Step 22360: Loss: 0.0999, requires_grad: True\n","Step 22370: Loss: 0.3730, requires_grad: True\n","Step 22380: Loss: 0.3453, requires_grad: True\n","Step 22390: Loss: 0.1101, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-2800\n","Step 22400: Loss: 0.1199, requires_grad: True\n","Step 22400: Gradients computed successfully\n","Step 22410: Loss: 0.3896, requires_grad: True\n","Step 22420: Loss: 0.1925, requires_grad: True\n","Step 22430: Loss: 0.1073, requires_grad: True\n","Step 22440: Loss: 0.1514, requires_grad: True\n","Step 22450: Loss: 0.0882, requires_grad: True\n","Step 22450: Gradients computed successfully\n","Step 22460: Loss: 0.2243, requires_grad: True\n","Step 22470: Loss: 0.3891, requires_grad: True\n","Step 22480: Loss: 0.1538, requires_grad: True\n","Step 22490: Loss: 0.3478, requires_grad: True\n","Step 22500: Loss: 0.5618, requires_grad: True\n","Step 22500: Gradients computed successfully\n","Step 22510: Loss: 0.1588, requires_grad: True\n","Step 22520: Loss: 0.2049, requires_grad: True\n","Step 22530: Loss: 0.1097, requires_grad: True\n","Step 22540: Loss: 0.0874, requires_grad: True\n","Step 22550: Loss: 0.1654, requires_grad: True\n","Step 22550: Gradients computed successfully\n","Step 22560: Loss: 0.1483, requires_grad: True\n","Step 22570: Loss: 0.3959, requires_grad: True\n","Step 22580: Loss: 0.2825, requires_grad: True\n","Step 22590: Loss: 0.1921, requires_grad: True\n","Step 22600: Loss: 0.3456, requires_grad: True\n","Step 22600: Gradients computed successfully\n","Step 22610: Loss: 0.4387, requires_grad: True\n","Step 22620: Loss: 0.3071, requires_grad: True\n","Step 22630: Loss: 0.1645, requires_grad: True\n","Step 22640: Loss: 0.3835, requires_grad: True\n","Step 22650: Loss: 0.1512, requires_grad: True\n","Step 22650: Gradients computed successfully\n","Step 22660: Loss: 0.3126, requires_grad: True\n","Step 22670: Loss: 0.1910, requires_grad: True\n","Step 22680: Loss: 0.5421, requires_grad: True\n","Step 22690: Loss: 0.3645, requires_grad: True\n","Step 22700: Loss: 0.3991, requires_grad: True\n","Step 22700: Gradients computed successfully\n","Step 22710: Loss: 0.2435, requires_grad: True\n","Step 22720: Loss: 0.2091, requires_grad: True\n","Step 22730: Loss: 0.1718, requires_grad: True\n","Step 22740: Loss: 0.2092, requires_grad: True\n","Step 22750: Loss: 0.2942, requires_grad: True\n","Step 22750: Gradients computed successfully\n","Step 22760: Loss: 0.1827, requires_grad: True\n","Step 22770: Loss: 0.2124, requires_grad: True\n","Step 22780: Loss: 0.1477, requires_grad: True\n","Step 22790: Loss: 0.1340, requires_grad: True\n","Step 22800: Loss: 0.1358, requires_grad: True\n","Step 22800: Gradients computed successfully\n","Step 22810: Loss: 0.1246, requires_grad: True\n","Step 22820: Loss: 0.3253, requires_grad: True\n","Step 22830: Loss: 0.3330, requires_grad: True\n","Step 22840: Loss: 0.1895, requires_grad: True\n","Step 22850: Loss: 0.1388, requires_grad: True\n","Step 22850: Gradients computed successfully\n","Step 22860: Loss: 0.2262, requires_grad: True\n","Step 22870: Loss: 0.1532, requires_grad: True\n","Step 22880: Loss: 0.2914, requires_grad: True\n","Step 22890: Loss: 0.4200, requires_grad: True\n","Step 22900: Loss: 0.1663, requires_grad: True\n","Step 22900: Gradients computed successfully\n","Step 22910: Loss: 0.1542, requires_grad: True\n","Step 22920: Loss: 0.4133, requires_grad: True\n","Step 22930: Loss: 0.1723, requires_grad: True\n","Step 22940: Loss: 0.2900, requires_grad: True\n","Step 22950: Loss: 0.1969, requires_grad: True\n","Step 22950: Gradients computed successfully\n","Step 22960: Loss: 0.5885, requires_grad: True\n","Step 22970: Loss: 0.3383, requires_grad: True\n","Step 22980: Loss: 0.3609, requires_grad: True\n","Step 22990: Loss: 0.7061, requires_grad: True\n","Step 23000: Loss: 0.2946, requires_grad: True\n","Step 23000: Gradients computed successfully\n","Step 23010: Loss: 0.1893, requires_grad: True\n","Step 23020: Loss: 0.1576, requires_grad: True\n","Step 23030: Loss: 0.1270, requires_grad: True\n","Step 23040: Loss: 0.1222, requires_grad: True\n","Step 23050: Loss: 0.2525, requires_grad: True\n","Step 23050: Gradients computed successfully\n","Step 23060: Loss: 0.1808, requires_grad: True\n","Step 23070: Loss: 0.1780, requires_grad: True\n","Step 23080: Loss: 0.0625, requires_grad: True\n","Step 23090: Loss: 0.5191, requires_grad: True\n","Step 23100: Loss: 0.1296, requires_grad: True\n","Step 23100: Gradients computed successfully\n","Step 23110: Loss: 0.0884, requires_grad: True\n","Step 23120: Loss: 0.3706, requires_grad: True\n","Step 23130: Loss: 0.1792, requires_grad: True\n","Step 23140: Loss: 0.4677, requires_grad: True\n","Step 23150: Loss: 0.1537, requires_grad: True\n","Step 23150: Gradients computed successfully\n","Step 23160: Loss: 0.4171, requires_grad: True\n","Step 23170: Loss: 0.1651, requires_grad: True\n","Step 23180: Loss: 0.1278, requires_grad: True\n","Step 23190: Loss: 0.2555, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-2900\n","Step 23200: Loss: 0.1568, requires_grad: True\n","Step 23200: Gradients computed successfully\n","Step 23210: Loss: 0.1632, requires_grad: True\n","Step 23220: Loss: 0.0661, requires_grad: True\n","Step 23230: Loss: 0.3515, requires_grad: True\n","Step 23240: Loss: 0.2674, requires_grad: True\n","Step 23250: Loss: 0.5175, requires_grad: True\n","Step 23250: Gradients computed successfully\n","Step 23260: Loss: 0.1422, requires_grad: True\n","Step 23270: Loss: 0.1201, requires_grad: True\n","Step 23280: Loss: 0.3081, requires_grad: True\n","Step 23290: Loss: 0.2378, requires_grad: True\n","Step 23300: Loss: 0.1873, requires_grad: True\n","Step 23300: Gradients computed successfully\n","Step 23310: Loss: 0.1927, requires_grad: True\n","Step 23320: Loss: 0.1357, requires_grad: True\n","Step 23330: Loss: 0.1550, requires_grad: True\n","Step 23340: Loss: 0.3123, requires_grad: True\n","Step 23350: Loss: 0.1031, requires_grad: True\n","Step 23350: Gradients computed successfully\n","Step 23360: Loss: 0.3596, requires_grad: True\n","Step 23370: Loss: 0.4773, requires_grad: True\n","Step 23380: Loss: 0.2972, requires_grad: True\n","Step 23390: Loss: 0.2150, requires_grad: True\n","Step 23400: Loss: 0.2385, requires_grad: True\n","Step 23400: Gradients computed successfully\n","Step 23410: Loss: 0.2701, requires_grad: True\n","Step 23420: Loss: 0.4288, requires_grad: True\n","Step 23430: Loss: 0.4329, requires_grad: True\n","Step 23440: Loss: 0.0837, requires_grad: True\n","Step 23450: Loss: 0.2675, requires_grad: True\n","Step 23450: Gradients computed successfully\n","Step 23460: Loss: 0.3655, requires_grad: True\n","Step 23470: Loss: 0.3082, requires_grad: True\n","Step 23480: Loss: 0.2863, requires_grad: True\n","Step 23490: Loss: 0.1363, requires_grad: True\n","Step 23500: Loss: 0.1854, requires_grad: True\n","Step 23500: Gradients computed successfully\n","Step 23510: Loss: 0.4204, requires_grad: True\n","Step 23520: Loss: 0.2933, requires_grad: True\n","Step 23530: Loss: 0.3900, requires_grad: True\n","Step 23540: Loss: 0.2674, requires_grad: True\n","Step 23550: Loss: 0.0920, requires_grad: True\n","Step 23550: Gradients computed successfully\n","Step 23560: Loss: 0.1507, requires_grad: True\n","Step 23570: Loss: 0.2199, requires_grad: True\n","Step 23580: Loss: 0.2654, requires_grad: True\n","Step 23590: Loss: 0.2502, requires_grad: True\n","Step 23600: Loss: 0.1814, requires_grad: True\n","Step 23600: Gradients computed successfully\n","Step 23610: Loss: 0.1147, requires_grad: True\n","Step 23620: Loss: 0.0403, requires_grad: True\n","Step 23630: Loss: 0.1016, requires_grad: True\n","Step 23640: Loss: 0.1947, requires_grad: True\n","Step 23650: Loss: 0.2076, requires_grad: True\n","Step 23650: Gradients computed successfully\n","Step 23660: Loss: 0.2161, requires_grad: True\n","Step 23670: Loss: 0.2719, requires_grad: True\n","Step 23680: Loss: 0.4459, requires_grad: True\n","Step 23690: Loss: 0.1935, requires_grad: True\n","Step 23700: Loss: 0.1831, requires_grad: True\n","Step 23700: Gradients computed successfully\n","Step 23710: Loss: 0.3078, requires_grad: True\n","Step 23720: Loss: 0.0844, requires_grad: True\n","Step 23730: Loss: 0.2683, requires_grad: True\n","Step 23740: Loss: 0.0489, requires_grad: True\n","Step 23750: Loss: 0.2036, requires_grad: True\n","Step 23750: Gradients computed successfully\n","Step 23760: Loss: 0.1185, requires_grad: True\n","Step 23770: Loss: 0.1060, requires_grad: True\n","Step 23780: Loss: 0.1129, requires_grad: True\n","Step 23790: Loss: 0.4253, requires_grad: True\n","Step 23800: Loss: 0.1243, requires_grad: True\n","Step 23800: Gradients computed successfully\n","Step 23810: Loss: 0.1297, requires_grad: True\n","Step 23820: Loss: 0.2073, requires_grad: True\n","Step 23830: Loss: 0.1992, requires_grad: True\n","Step 23840: Loss: 0.0549, requires_grad: True\n","Step 23850: Loss: 0.2024, requires_grad: True\n","Step 23850: Gradients computed successfully\n","Step 23860: Loss: 0.2195, requires_grad: True\n","Step 23870: Loss: 0.1576, requires_grad: True\n","Step 23880: Loss: 0.2889, requires_grad: True\n","Step 23890: Loss: 0.1283, requires_grad: True\n","Step 23900: Loss: 0.3939, requires_grad: True\n","Step 23900: Gradients computed successfully\n","Step 23910: Loss: 0.1682, requires_grad: True\n","Step 23920: Loss: 0.2451, requires_grad: True\n","Step 23930: Loss: 0.0554, requires_grad: True\n","Step 23940: Loss: 0.1613, requires_grad: True\n","Step 23950: Loss: 0.2290, requires_grad: True\n","Step 23950: Gradients computed successfully\n","Step 23960: Loss: 0.2812, requires_grad: True\n","Step 23970: Loss: 0.4097, requires_grad: True\n","Step 23980: Loss: 0.2738, requires_grad: True\n","Step 23990: Loss: 0.2601, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-3000\n","Step 24000: Loss: 0.4429, requires_grad: True\n","Step 24000: Gradients computed successfully\n","Step 24010: Loss: 0.3551, requires_grad: True\n","Step 24020: Loss: 0.1404, requires_grad: True\n","Step 24030: Loss: 0.5904, requires_grad: True\n","Step 24040: Loss: 0.1536, requires_grad: True\n","Step 24050: Loss: 0.6816, requires_grad: True\n","Step 24050: Gradients computed successfully\n","Step 24060: Loss: 0.2824, requires_grad: True\n","Step 24070: Loss: 0.1662, requires_grad: True\n","Step 24080: Loss: 0.2843, requires_grad: True\n","Step 24090: Loss: 0.2537, requires_grad: True\n","Step 24100: Loss: 0.2317, requires_grad: True\n","Step 24100: Gradients computed successfully\n","Step 24110: Loss: 0.2697, requires_grad: True\n","Step 24120: Loss: 0.5639, requires_grad: True\n","Step 24130: Loss: 0.0959, requires_grad: True\n","Step 24140: Loss: 0.1847, requires_grad: True\n","Step 24150: Loss: 0.1541, requires_grad: True\n","Step 24150: Gradients computed successfully\n","Step 24160: Loss: 0.3391, requires_grad: True\n","Step 24170: Loss: 0.3588, requires_grad: True\n","Step 24180: Loss: 0.2256, requires_grad: True\n","Step 24190: Loss: 0.2609, requires_grad: True\n","Step 24200: Loss: 0.2709, requires_grad: True\n","Step 24200: Gradients computed successfully\n","Step 24210: Loss: 0.1556, requires_grad: True\n","Step 24220: Loss: 0.4732, requires_grad: True\n","Step 24230: Loss: 0.5685, requires_grad: True\n","Step 24240: Loss: 0.2680, requires_grad: True\n","Step 24250: Loss: 0.2097, requires_grad: True\n","Step 24250: Gradients computed successfully\n","Step 24260: Loss: 0.1211, requires_grad: True\n","Step 24270: Loss: 0.2857, requires_grad: True\n","Step 24280: Loss: 0.2639, requires_grad: True\n","Step 24290: Loss: 0.1692, requires_grad: True\n","Step 24300: Loss: 0.2514, requires_grad: True\n","Step 24300: Gradients computed successfully\n","Step 24310: Loss: 0.1982, requires_grad: True\n","Step 24320: Loss: 0.2216, requires_grad: True\n","Step 24330: Loss: 0.4175, requires_grad: True\n","Step 24340: Loss: 0.1543, requires_grad: True\n","Step 24350: Loss: 0.2172, requires_grad: True\n","Step 24350: Gradients computed successfully\n","Step 24360: Loss: 0.2254, requires_grad: True\n","Step 24370: Loss: 0.2845, requires_grad: True\n","Step 24380: Loss: 0.3144, requires_grad: True\n","Step 24390: Loss: 0.1222, requires_grad: True\n","Step 24400: Loss: 0.2085, requires_grad: True\n","Step 24400: Gradients computed successfully\n","Step 24410: Loss: 0.1571, requires_grad: True\n","Step 24420: Loss: 0.1495, requires_grad: True\n","Step 24430: Loss: 0.2229, requires_grad: True\n","Step 24440: Loss: 0.1813, requires_grad: True\n","Step 24450: Loss: 0.3205, requires_grad: True\n","Step 24450: Gradients computed successfully\n","Step 24460: Loss: 0.2201, requires_grad: True\n","Step 24470: Loss: 0.5591, requires_grad: True\n","Step 24480: Loss: 0.1322, requires_grad: True\n","Step 24490: Loss: 0.2481, requires_grad: True\n","Step 24500: Loss: 0.1447, requires_grad: True\n","Step 24500: Gradients computed successfully\n","Step 24510: Loss: 0.2798, requires_grad: True\n","Step 24520: Loss: 0.1809, requires_grad: True\n","Step 24530: Loss: 0.1568, requires_grad: True\n","Step 24540: Loss: 0.4263, requires_grad: True\n","Step 24550: Loss: 0.0849, requires_grad: True\n","Step 24550: Gradients computed successfully\n","Step 24560: Loss: 0.0564, requires_grad: True\n","Step 24570: Loss: 0.3788, requires_grad: True\n","Step 24580: Loss: 0.1208, requires_grad: True\n","Step 24590: Loss: 0.3957, requires_grad: True\n","Step 24600: Loss: 0.2810, requires_grad: True\n","Step 24600: Gradients computed successfully\n","Step 24610: Loss: 0.2856, requires_grad: True\n","Step 24620: Loss: 0.1113, requires_grad: True\n","Step 24630: Loss: 0.3012, requires_grad: True\n","Step 24640: Loss: 0.1966, requires_grad: True\n","Step 24650: Loss: 0.1977, requires_grad: True\n","Step 24650: Gradients computed successfully\n","Step 24660: Loss: 0.1563, requires_grad: True\n","Step 24670: Loss: 0.2026, requires_grad: True\n","Step 24680: Loss: 0.1988, requires_grad: True\n","Step 24690: Loss: 0.1147, requires_grad: True\n","Step 24700: Loss: 0.1683, requires_grad: True\n","Step 24700: Gradients computed successfully\n","Step 24710: Loss: 0.4088, requires_grad: True\n","Step 24720: Loss: 0.0987, requires_grad: True\n","Step 24730: Loss: 0.2485, requires_grad: True\n","Step 24740: Loss: 0.1759, requires_grad: True\n","Step 24750: Loss: 0.3795, requires_grad: True\n","Step 24750: Gradients computed successfully\n","Step 24760: Loss: 0.1672, requires_grad: True\n","Step 24770: Loss: 0.1737, requires_grad: True\n","Step 24780: Loss: 0.3080, requires_grad: True\n","Step 24790: Loss: 0.0977, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-3100\n","Step 24800: Loss: 0.4732, requires_grad: True\n","Step 24800: Gradients computed successfully\n","Step 24810: Loss: 0.1692, requires_grad: True\n","Step 24820: Loss: 0.1695, requires_grad: True\n","Step 24830: Loss: 0.4494, requires_grad: True\n","Step 24840: Loss: 0.5156, requires_grad: True\n","Step 24850: Loss: 0.2295, requires_grad: True\n","Step 24850: Gradients computed successfully\n","Step 24860: Loss: 0.2599, requires_grad: True\n","Step 24870: Loss: 0.2199, requires_grad: True\n","Step 24880: Loss: 0.1887, requires_grad: True\n","Step 24890: Loss: 0.3782, requires_grad: True\n","Step 24900: Loss: 0.1333, requires_grad: True\n","Step 24900: Gradients computed successfully\n","Step 24910: Loss: 0.1636, requires_grad: True\n","Step 24920: Loss: 0.2823, requires_grad: True\n","Step 24930: Loss: 0.1159, requires_grad: True\n","Step 24940: Loss: 0.2100, requires_grad: True\n","Step 24950: Loss: 0.5407, requires_grad: True\n","Step 24950: Gradients computed successfully\n","Step 24960: Loss: 0.3914, requires_grad: True\n","Step 24970: Loss: 0.2210, requires_grad: True\n","Step 24980: Loss: 0.2452, requires_grad: True\n","Step 24990: Loss: 0.3087, requires_grad: True\n","Step 25000: Loss: 0.2160, requires_grad: True\n","Step 25000: Gradients computed successfully\n","Step 25010: Loss: 0.2101, requires_grad: True\n","Step 25020: Loss: 0.2107, requires_grad: True\n","Step 25030: Loss: 0.1829, requires_grad: True\n","Step 25040: Loss: 0.2003, requires_grad: True\n","Step 25050: Loss: 0.6168, requires_grad: True\n","Step 25050: Gradients computed successfully\n","Step 25060: Loss: 0.1408, requires_grad: True\n","Step 25070: Loss: 0.1401, requires_grad: True\n","Step 25080: Loss: 0.2543, requires_grad: True\n","Step 25090: Loss: 0.1721, requires_grad: True\n","Step 25100: Loss: 0.1160, requires_grad: True\n","Step 25100: Gradients computed successfully\n","Step 25110: Loss: 0.2351, requires_grad: True\n","Step 25120: Loss: 0.1393, requires_grad: True\n","Step 25130: Loss: 0.4586, requires_grad: True\n","Step 25140: Loss: 0.2456, requires_grad: True\n","Step 25150: Loss: 0.2715, requires_grad: True\n","Step 25150: Gradients computed successfully\n","Step 25160: Loss: 0.1287, requires_grad: True\n","Step 25170: Loss: 0.1660, requires_grad: True\n","Step 25180: Loss: 0.2377, requires_grad: True\n","Step 25190: Loss: 0.1651, requires_grad: True\n","Step 25200: Loss: 0.3123, requires_grad: True\n","Step 25200: Gradients computed successfully\n","Step 25210: Loss: 0.0989, requires_grad: True\n","Step 25220: Loss: 0.0773, requires_grad: True\n","Step 25230: Loss: 0.3394, requires_grad: True\n","Step 25240: Loss: 0.0531, requires_grad: True\n","Step 25250: Loss: 0.1966, requires_grad: True\n","Step 25250: Gradients computed successfully\n","Step 25260: Loss: 0.3808, requires_grad: True\n","Step 25270: Loss: 0.3484, requires_grad: True\n","Step 25280: Loss: 0.1608, requires_grad: True\n","Step 25290: Loss: 0.4197, requires_grad: True\n","Step 25300: Loss: 0.0996, requires_grad: True\n","Step 25300: Gradients computed successfully\n","Step 25310: Loss: 0.3750, requires_grad: True\n","Step 25320: Loss: 0.2082, requires_grad: True\n","Step 25330: Loss: 0.5562, requires_grad: True\n","Step 25340: Loss: 0.3196, requires_grad: True\n","Step 25350: Loss: 0.2747, requires_grad: True\n","Step 25350: Gradients computed successfully\n","Step 25360: Loss: 0.0986, requires_grad: True\n","Step 25370: Loss: 0.2499, requires_grad: True\n","Step 25380: Loss: 0.0707, requires_grad: True\n","Step 25390: Loss: 0.0833, requires_grad: True\n","Step 25400: Loss: 0.1757, requires_grad: True\n","Step 25400: Gradients computed successfully\n","Step 25410: Loss: 0.4055, requires_grad: True\n","Step 25420: Loss: 0.1751, requires_grad: True\n","Step 25430: Loss: 0.2208, requires_grad: True\n","Step 25440: Loss: 0.3476, requires_grad: True\n","Step 25450: Loss: 0.2861, requires_grad: True\n","Step 25450: Gradients computed successfully\n","Step 25460: Loss: 0.1439, requires_grad: True\n","Step 25470: Loss: 0.2586, requires_grad: True\n","Step 25480: Loss: 0.4179, requires_grad: True\n","Step 25490: Loss: 0.2782, requires_grad: True\n","Step 25500: Loss: 0.1031, requires_grad: True\n","Step 25500: Gradients computed successfully\n","Step 25510: Loss: 0.4229, requires_grad: True\n","Step 25520: Loss: 0.3011, requires_grad: True\n","Step 25530: Loss: 0.1935, requires_grad: True\n","Step 25540: Loss: 0.2286, requires_grad: True\n","Step 25550: Loss: 0.3204, requires_grad: True\n","Step 25550: Gradients computed successfully\n","Step 25560: Loss: 0.5014, requires_grad: True\n","Step 25570: Loss: 0.3232, requires_grad: True\n","Step 25580: Loss: 0.3005, requires_grad: True\n","Step 25590: Loss: 0.3260, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-3200\n","Step 25600: Loss: 0.2350, requires_grad: True\n","Step 25600: Gradients computed successfully\n","Step 25610: Loss: 0.0894, requires_grad: True\n","Step 25620: Loss: 0.1470, requires_grad: True\n","Step 25630: Loss: 0.1473, requires_grad: True\n","Step 25640: Loss: 0.2184, requires_grad: True\n","Step 25650: Loss: 0.2160, requires_grad: True\n","Step 25650: Gradients computed successfully\n","Step 25660: Loss: 0.2004, requires_grad: True\n","Step 25670: Loss: 0.1547, requires_grad: True\n","Step 25680: Loss: 0.4839, requires_grad: True\n","Step 25690: Loss: 0.3999, requires_grad: True\n","Step 25700: Loss: 0.2510, requires_grad: True\n","Step 25700: Gradients computed successfully\n","Step 25710: Loss: 0.5939, requires_grad: True\n","Step 25720: Loss: 0.1547, requires_grad: True\n","Step 25730: Loss: 0.2241, requires_grad: True\n","Step 25740: Loss: 0.2584, requires_grad: True\n","Step 25750: Loss: 0.3658, requires_grad: True\n","Step 25750: Gradients computed successfully\n","Step 25760: Loss: 0.4352, requires_grad: True\n","Step 25770: Loss: 0.1353, requires_grad: True\n","Step 25780: Loss: 0.5714, requires_grad: True\n","Step 25790: Loss: 0.1819, requires_grad: True\n","Step 25800: Loss: 0.1229, requires_grad: True\n","Step 25800: Gradients computed successfully\n","Step 25810: Loss: 0.2114, requires_grad: True\n","Step 25820: Loss: 0.1530, requires_grad: True\n","Step 25830: Loss: 0.1857, requires_grad: True\n","Step 25840: Loss: 0.2546, requires_grad: True\n","Step 25850: Loss: 0.1564, requires_grad: True\n","Step 25850: Gradients computed successfully\n","Step 25860: Loss: 0.2973, requires_grad: True\n","Step 25870: Loss: 0.2612, requires_grad: True\n","Step 25880: Loss: 0.2141, requires_grad: True\n","Step 25890: Loss: 0.1308, requires_grad: True\n","Step 25900: Loss: 0.2166, requires_grad: True\n","Step 25900: Gradients computed successfully\n","Step 25910: Loss: 0.1596, requires_grad: True\n","Step 25920: Loss: 0.1781, requires_grad: True\n","Step 25930: Loss: 0.3349, requires_grad: True\n","Step 25940: Loss: 0.4031, requires_grad: True\n","Step 25950: Loss: 0.1893, requires_grad: True\n","Step 25950: Gradients computed successfully\n","Step 25960: Loss: 0.1655, requires_grad: True\n","Step 25970: Loss: 0.1382, requires_grad: True\n","Step 25980: Loss: 0.1517, requires_grad: True\n","Step 25990: Loss: 0.1207, requires_grad: True\n","Step 26000: Loss: 0.4083, requires_grad: True\n","Step 26000: Gradients computed successfully\n","Step 26010: Loss: 0.1398, requires_grad: True\n","Step 26020: Loss: 0.1862, requires_grad: True\n","Step 26030: Loss: 0.2822, requires_grad: True\n","Step 26040: Loss: 0.2259, requires_grad: True\n","Step 26050: Loss: 0.4355, requires_grad: True\n","Step 26050: Gradients computed successfully\n","Step 26060: Loss: 0.1479, requires_grad: True\n","Step 26070: Loss: 0.2562, requires_grad: True\n","Step 26080: Loss: 0.1009, requires_grad: True\n","Step 26090: Loss: 0.0957, requires_grad: True\n","Step 26100: Loss: 0.2140, requires_grad: True\n","Step 26100: Gradients computed successfully\n","Step 26110: Loss: 0.1838, requires_grad: True\n","Step 26120: Loss: 0.1490, requires_grad: True\n","Step 26130: Loss: 0.2295, requires_grad: True\n","Step 26140: Loss: 0.4540, requires_grad: True\n","Step 26150: Loss: 0.2470, requires_grad: True\n","Step 26150: Gradients computed successfully\n","Step 26160: Loss: 0.3160, requires_grad: True\n","Step 26170: Loss: 0.0519, requires_grad: True\n","Step 26180: Loss: 0.0715, requires_grad: True\n","Step 26190: Loss: 0.2514, requires_grad: True\n","Step 26200: Loss: 0.1300, requires_grad: True\n","Step 26200: Gradients computed successfully\n","Step 26210: Loss: 0.3638, requires_grad: True\n","Step 26220: Loss: 0.1706, requires_grad: True\n","Step 26230: Loss: 0.2481, requires_grad: True\n","Step 26240: Loss: 0.1669, requires_grad: True\n","Step 26250: Loss: 0.1600, requires_grad: True\n","Step 26250: Gradients computed successfully\n","Step 26260: Loss: 0.1571, requires_grad: True\n","Step 26270: Loss: 0.0775, requires_grad: True\n","Step 26280: Loss: 0.2229, requires_grad: True\n","Step 26290: Loss: 0.1242, requires_grad: True\n","Step 26300: Loss: 0.1490, requires_grad: True\n","Step 26300: Gradients computed successfully\n","Step 26310: Loss: 0.1483, requires_grad: True\n","Step 26320: Loss: 0.2828, requires_grad: True\n","Step 26330: Loss: 0.2413, requires_grad: True\n","Step 26340: Loss: 0.5929, requires_grad: True\n","Step 26350: Loss: 0.1269, requires_grad: True\n","Step 26350: Gradients computed successfully\n","Step 26360: Loss: 0.2795, requires_grad: True\n","Step 26370: Loss: 0.1053, requires_grad: True\n","Step 26380: Loss: 0.0943, requires_grad: True\n","Step 26390: Loss: 0.1209, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-3300\n","Step 26400: Loss: 0.1086, requires_grad: True\n","Step 26400: Gradients computed successfully\n","Step 26410: Loss: 0.1350, requires_grad: True\n","Step 26420: Loss: 0.2398, requires_grad: True\n","Step 26430: Loss: 0.1115, requires_grad: True\n","Step 26440: Loss: 0.0936, requires_grad: True\n","Step 26450: Loss: 0.2592, requires_grad: True\n","Step 26450: Gradients computed successfully\n","Step 26460: Loss: 0.1312, requires_grad: True\n","Step 26470: Loss: 0.4327, requires_grad: True\n","Step 26480: Loss: 0.3890, requires_grad: True\n","Step 26490: Loss: 0.2030, requires_grad: True\n","Step 26500: Loss: 0.2214, requires_grad: True\n","Step 26500: Gradients computed successfully\n","Step 26510: Loss: 0.0859, requires_grad: True\n","Step 26520: Loss: 0.2312, requires_grad: True\n","Step 26530: Loss: 0.2056, requires_grad: True\n","Step 26540: Loss: 0.1348, requires_grad: True\n","Step 26550: Loss: 0.0809, requires_grad: True\n","Step 26550: Gradients computed successfully\n","Step 26560: Loss: 0.0512, requires_grad: True\n","Step 26570: Loss: 0.2268, requires_grad: True\n","Step 26580: Loss: 0.2021, requires_grad: True\n","Step 26590: Loss: 0.1925, requires_grad: True\n","Step 26600: Loss: 0.3375, requires_grad: True\n","Step 26600: Gradients computed successfully\n","Step 26610: Loss: 0.3638, requires_grad: True\n","Step 26620: Loss: 0.1941, requires_grad: True\n","Step 26630: Loss: 0.2009, requires_grad: True\n","Step 26640: Loss: 0.1538, requires_grad: True\n","Step 26650: Loss: 0.0530, requires_grad: True\n","Step 26650: Gradients computed successfully\n","Step 26660: Loss: 0.0856, requires_grad: True\n","Step 26670: Loss: 0.1858, requires_grad: True\n","Step 26680: Loss: 0.0680, requires_grad: True\n","Step 26690: Loss: 0.0667, requires_grad: True\n","Step 26700: Loss: 0.3146, requires_grad: True\n","Step 26700: Gradients computed successfully\n","Step 26710: Loss: 0.3250, requires_grad: True\n","Step 26720: Loss: 0.3446, requires_grad: True\n","Step 26730: Loss: 0.1609, requires_grad: True\n","Step 26740: Loss: 0.4052, requires_grad: True\n","Step 26750: Loss: 0.3772, requires_grad: True\n","Step 26750: Gradients computed successfully\n","Step 26760: Loss: 0.1551, requires_grad: True\n","Step 26770: Loss: 0.1030, requires_grad: True\n","Step 26780: Loss: 0.2260, requires_grad: True\n","Step 26790: Loss: 0.1647, requires_grad: True\n","Step 26800: Loss: 0.1877, requires_grad: True\n","Step 26800: Gradients computed successfully\n","Step 26810: Loss: 0.2460, requires_grad: True\n","Step 26820: Loss: 0.1408, requires_grad: True\n","Step 26830: Loss: 0.1803, requires_grad: True\n","Step 26840: Loss: 0.1168, requires_grad: True\n","Step 26850: Loss: 0.2648, requires_grad: True\n","Step 26850: Gradients computed successfully\n","Step 26860: Loss: 0.1309, requires_grad: True\n","Step 26870: Loss: 0.0776, requires_grad: True\n","Step 26880: Loss: 0.3416, requires_grad: True\n","Step 26890: Loss: 0.4920, requires_grad: True\n","Step 26900: Loss: 0.3268, requires_grad: True\n","Step 26900: Gradients computed successfully\n","Step 26910: Loss: 0.2701, requires_grad: True\n","Step 26920: Loss: 0.2373, requires_grad: True\n","Step 26930: Loss: 0.2447, requires_grad: True\n","Step 26940: Loss: 0.1261, requires_grad: True\n","Step 26950: Loss: 0.2216, requires_grad: True\n","Step 26950: Gradients computed successfully\n","Step 26960: Loss: 0.2078, requires_grad: True\n","Step 26970: Loss: 0.1924, requires_grad: True\n","Step 26980: Loss: 0.4514, requires_grad: True\n","Step 26990: Loss: 0.2452, requires_grad: True\n","Step 27000: Loss: 0.3866, requires_grad: True\n","Step 27000: Gradients computed successfully\n","Step 27010: Loss: 0.1274, requires_grad: True\n","Step 27020: Loss: 0.1898, requires_grad: True\n","Step 27030: Loss: 0.2402, requires_grad: True\n","Step 27040: Loss: 0.5656, requires_grad: True\n","Step 27050: Loss: 0.0719, requires_grad: True\n","Step 27050: Gradients computed successfully\n","Step 27060: Loss: 0.4581, requires_grad: True\n","Step 27070: Loss: 0.2425, requires_grad: True\n","Step 27080: Loss: 0.1088, requires_grad: True\n","Step 27090: Loss: 0.2421, requires_grad: True\n","Step 27100: Loss: 0.3207, requires_grad: True\n","Step 27100: Gradients computed successfully\n","Step 27110: Loss: 0.3256, requires_grad: True\n","Step 27120: Loss: 0.3531, requires_grad: True\n","Step 27130: Loss: 0.1427, requires_grad: True\n","Step 27140: Loss: 0.0856, requires_grad: True\n","Step 27150: Loss: 0.1530, requires_grad: True\n","Step 27150: Gradients computed successfully\n","Step 27160: Loss: 0.0900, requires_grad: True\n","Step 27170: Loss: 0.5385, requires_grad: True\n","Step 27180: Loss: 0.2230, requires_grad: True\n","Step 27190: Loss: 0.0911, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-3400\n","Step 27200: Loss: 0.2416, requires_grad: True\n","Step 27200: Gradients computed successfully\n","Step 27210: Loss: 0.4234, requires_grad: True\n","Step 27220: Loss: 0.1910, requires_grad: True\n","Step 27230: Loss: 0.3961, requires_grad: True\n","Step 27240: Loss: 0.4590, requires_grad: True\n","Step 27250: Loss: 0.2394, requires_grad: True\n","Step 27250: Gradients computed successfully\n","Step 27260: Loss: 0.0914, requires_grad: True\n","Step 27270: Loss: 0.0871, requires_grad: True\n","Step 27280: Loss: 0.1899, requires_grad: True\n","Step 27290: Loss: 0.0743, requires_grad: True\n","Step 27300: Loss: 0.2333, requires_grad: True\n","Step 27300: Gradients computed successfully\n","Step 27310: Loss: 0.2525, requires_grad: True\n","Step 27320: Loss: 0.1641, requires_grad: True\n","Step 27330: Loss: 0.1781, requires_grad: True\n","Step 27340: Loss: 0.3736, requires_grad: True\n","Step 27350: Loss: 0.2379, requires_grad: True\n","Step 27350: Gradients computed successfully\n","Step 27360: Loss: 0.0868, requires_grad: True\n","Step 27370: Loss: 0.2620, requires_grad: True\n","Step 27380: Loss: 0.2788, requires_grad: True\n","Step 27390: Loss: 0.1904, requires_grad: True\n","Step 27400: Loss: 0.2010, requires_grad: True\n","Step 27400: Gradients computed successfully\n","Step 27410: Loss: 0.0966, requires_grad: True\n","Step 27420: Loss: 0.2125, requires_grad: True\n","Step 27430: Loss: 0.0670, requires_grad: True\n","Step 27440: Loss: 0.3435, requires_grad: True\n","Step 27450: Loss: 0.1522, requires_grad: True\n","Step 27450: Gradients computed successfully\n","Step 27460: Loss: 0.1963, requires_grad: True\n","Step 27470: Loss: 0.1301, requires_grad: True\n","Step 27480: Loss: 0.3023, requires_grad: True\n","Step 27490: Loss: 0.3907, requires_grad: True\n","Step 27500: Loss: 0.2374, requires_grad: True\n","Step 27500: Gradients computed successfully\n","Step 27510: Loss: 0.2224, requires_grad: True\n","Step 27520: Loss: 0.1531, requires_grad: True\n","Step 27530: Loss: 0.1900, requires_grad: True\n","Step 27540: Loss: 0.3515, requires_grad: True\n","Step 27550: Loss: 0.1583, requires_grad: True\n","Step 27550: Gradients computed successfully\n","Step 27560: Loss: 0.1233, requires_grad: True\n","Step 27570: Loss: 0.4164, requires_grad: True\n","Step 27580: Loss: 0.1249, requires_grad: True\n","Step 27590: Loss: 0.4029, requires_grad: True\n","Step 27600: Loss: 0.0558, requires_grad: True\n","Step 27600: Gradients computed successfully\n","Step 27610: Loss: 0.1574, requires_grad: True\n","Step 27620: Loss: 0.1397, requires_grad: True\n","Step 27630: Loss: 0.2367, requires_grad: True\n","Step 27640: Loss: 0.2001, requires_grad: True\n","Step 27650: Loss: 0.2528, requires_grad: True\n","Step 27650: Gradients computed successfully\n","Step 27660: Loss: 0.1905, requires_grad: True\n","Step 27670: Loss: 0.3199, requires_grad: True\n","Step 27680: Loss: 0.0632, requires_grad: True\n","Step 27690: Loss: 0.2501, requires_grad: True\n","Step 27700: Loss: 0.3674, requires_grad: True\n","Step 27700: Gradients computed successfully\n","Step 27710: Loss: 0.2083, requires_grad: True\n","Step 27720: Loss: 0.2191, requires_grad: True\n","Step 27730: Loss: 0.4422, requires_grad: True\n","Step 27740: Loss: 0.1277, requires_grad: True\n","Step 27750: Loss: 0.2313, requires_grad: True\n","Step 27750: Gradients computed successfully\n","Step 27760: Loss: 0.0556, requires_grad: True\n","Step 27770: Loss: 0.3051, requires_grad: True\n","Step 27780: Loss: 0.1277, requires_grad: True\n","Step 27790: Loss: 0.1236, requires_grad: True\n","Step 27800: Loss: 0.2016, requires_grad: True\n","Step 27800: Gradients computed successfully\n","Step 27810: Loss: 0.4988, requires_grad: True\n","Step 27820: Loss: 0.1863, requires_grad: True\n","Step 27830: Loss: 0.2577, requires_grad: True\n","Step 27840: Loss: 0.1275, requires_grad: True\n","Step 27850: Loss: 0.1335, requires_grad: True\n","Step 27850: Gradients computed successfully\n","Step 27860: Loss: 0.1790, requires_grad: True\n","Step 27870: Loss: 0.2106, requires_grad: True\n","Step 27880: Loss: 0.3037, requires_grad: True\n","Step 27890: Loss: 0.4630, requires_grad: True\n","Step 27900: Loss: 0.0917, requires_grad: True\n","Step 27900: Gradients computed successfully\n","Step 27910: Loss: 0.0887, requires_grad: True\n","Step 27920: Loss: 0.1212, requires_grad: True\n","Step 27930: Loss: 0.1282, requires_grad: True\n","Step 27940: Loss: 0.1893, requires_grad: True\n","Step 27950: Loss: 0.5022, requires_grad: True\n","Step 27950: Gradients computed successfully\n","Step 27960: Loss: 0.3705, requires_grad: True\n","Step 27970: Loss: 0.0874, requires_grad: True\n","Step 27980: Loss: 0.4622, requires_grad: True\n","Step 27990: Loss: 0.2221, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-3500\n","Step 28000: Loss: 0.1997, requires_grad: True\n","Step 28000: Gradients computed successfully\n","Step 28010: Loss: 0.2744, requires_grad: True\n","Step 28020: Loss: 0.7328, requires_grad: True\n","Step 28030: Loss: 0.1469, requires_grad: True\n","Step 28040: Loss: 0.1754, requires_grad: True\n","Step 28050: Loss: 0.1166, requires_grad: True\n","Step 28050: Gradients computed successfully\n","Step 28060: Loss: 0.2746, requires_grad: True\n","Step 28070: Loss: 0.1989, requires_grad: True\n","Step 28080: Loss: 0.4444, requires_grad: True\n","Step 28090: Loss: 0.0886, requires_grad: True\n","Step 28100: Loss: 0.1659, requires_grad: True\n","Step 28100: Gradients computed successfully\n","Step 28110: Loss: 0.1816, requires_grad: True\n","Step 28120: Loss: 0.0973, requires_grad: True\n","Step 28130: Loss: 0.2037, requires_grad: True\n","Step 28140: Loss: 0.1092, requires_grad: True\n","Step 28150: Loss: 0.0830, requires_grad: True\n","Step 28150: Gradients computed successfully\n","Step 28160: Loss: 0.2633, requires_grad: True\n","Step 28170: Loss: 0.1774, requires_grad: True\n","Step 28180: Loss: 0.1823, requires_grad: True\n","Step 28190: Loss: 0.1190, requires_grad: True\n","Step 28200: Loss: 0.2587, requires_grad: True\n","Step 28200: Gradients computed successfully\n","Step 28210: Loss: 0.0837, requires_grad: True\n","Step 28220: Loss: 0.1961, requires_grad: True\n","Step 28230: Loss: 0.1618, requires_grad: True\n","Step 28240: Loss: 0.3480, requires_grad: True\n","Step 28250: Loss: 0.1447, requires_grad: True\n","Step 28250: Gradients computed successfully\n","Step 28260: Loss: 0.2939, requires_grad: True\n","Step 28270: Loss: 0.1165, requires_grad: True\n","Step 28280: Loss: 0.2267, requires_grad: True\n","Step 28290: Loss: 0.2798, requires_grad: True\n","Step 28300: Loss: 0.4569, requires_grad: True\n","Step 28300: Gradients computed successfully\n","Step 28310: Loss: 0.1836, requires_grad: True\n","Step 28320: Loss: 0.2006, requires_grad: True\n","Step 28330: Loss: 0.2067, requires_grad: True\n","Step 28340: Loss: 0.2870, requires_grad: True\n","Step 28350: Loss: 0.2432, requires_grad: True\n","Step 28350: Gradients computed successfully\n","Step 28360: Loss: 0.1712, requires_grad: True\n","Step 28370: Loss: 0.2055, requires_grad: True\n","Step 28380: Loss: 0.1490, requires_grad: True\n","Step 28390: Loss: 0.1594, requires_grad: True\n","Step 28400: Loss: 0.2856, requires_grad: True\n","Step 28400: Gradients computed successfully\n","Step 28410: Loss: 0.2128, requires_grad: True\n","Step 28420: Loss: 0.0994, requires_grad: True\n","Step 28430: Loss: 0.2069, requires_grad: True\n","Step 28440: Loss: 0.0508, requires_grad: True\n","Step 28450: Loss: 0.2480, requires_grad: True\n","Step 28450: Gradients computed successfully\n","Step 28460: Loss: 0.0800, requires_grad: True\n","Step 28470: Loss: 0.1031, requires_grad: True\n","Step 28480: Loss: 0.1913, requires_grad: True\n","Step 28490: Loss: 0.1372, requires_grad: True\n","Step 28500: Loss: 0.1637, requires_grad: True\n","Step 28500: Gradients computed successfully\n","Step 28510: Loss: 0.2621, requires_grad: True\n","Step 28520: Loss: 0.1183, requires_grad: True\n","Step 28530: Loss: 0.2731, requires_grad: True\n","Step 28540: Loss: 0.3752, requires_grad: True\n","Step 28550: Loss: 0.0706, requires_grad: True\n","Step 28550: Gradients computed successfully\n","Step 28560: Loss: 0.0959, requires_grad: True\n","Step 28570: Loss: 0.1231, requires_grad: True\n","Step 28580: Loss: 0.3381, requires_grad: True\n","Step 28590: Loss: 0.1760, requires_grad: True\n","Step 28600: Loss: 0.4118, requires_grad: True\n","Step 28600: Gradients computed successfully\n","Step 28610: Loss: 0.1944, requires_grad: True\n","Step 28620: Loss: 0.2460, requires_grad: True\n","Step 28630: Loss: 0.3409, requires_grad: True\n","Step 28640: Loss: 0.2457, requires_grad: True\n","Step 28650: Loss: 0.3078, requires_grad: True\n","Step 28650: Gradients computed successfully\n","Step 28660: Loss: 0.3178, requires_grad: True\n","Step 28670: Loss: 0.3249, requires_grad: True\n","Step 28680: Loss: 0.2110, requires_grad: True\n","Step 28690: Loss: 0.2424, requires_grad: True\n","Step 28700: Loss: 0.3911, requires_grad: True\n","Step 28700: Gradients computed successfully\n","Step 28710: Loss: 0.3345, requires_grad: True\n","Step 28720: Loss: 0.2773, requires_grad: True\n","Step 28730: Loss: 0.1497, requires_grad: True\n","Step 28740: Loss: 0.4126, requires_grad: True\n","Step 28750: Loss: 0.2134, requires_grad: True\n","Step 28750: Gradients computed successfully\n","Step 28760: Loss: 0.1337, requires_grad: True\n","Step 28770: Loss: 0.1252, requires_grad: True\n","Step 28780: Loss: 0.2979, requires_grad: True\n","Step 28790: Loss: 0.2566, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-3600\n","Step 28800: Loss: 0.0952, requires_grad: True\n","Step 28800: Gradients computed successfully\n","Step 28810: Loss: 0.2392, requires_grad: True\n","Step 28820: Loss: 0.3555, requires_grad: True\n","Step 28830: Loss: 0.1564, requires_grad: True\n","Step 28840: Loss: 0.2320, requires_grad: True\n","Step 28850: Loss: 0.1162, requires_grad: True\n","Step 28850: Gradients computed successfully\n","Step 28860: Loss: 0.3536, requires_grad: True\n","Step 28870: Loss: 0.3055, requires_grad: True\n","Step 28880: Loss: 0.2027, requires_grad: True\n","Step 28890: Loss: 0.0987, requires_grad: True\n","Step 28900: Loss: 0.1687, requires_grad: True\n","Step 28900: Gradients computed successfully\n","Step 28910: Loss: 0.1531, requires_grad: True\n","Step 28920: Loss: 0.2078, requires_grad: True\n","Step 28930: Loss: 0.2011, requires_grad: True\n","Step 28940: Loss: 0.2202, requires_grad: True\n","Step 28950: Loss: 0.2733, requires_grad: True\n","Step 28950: Gradients computed successfully\n","Step 28960: Loss: 0.3168, requires_grad: True\n","Step 28970: Loss: 0.3073, requires_grad: True\n","Step 28980: Loss: 0.2324, requires_grad: True\n","Step 28990: Loss: 0.0876, requires_grad: True\n","Step 29000: Loss: 0.2272, requires_grad: True\n","Step 29000: Gradients computed successfully\n","Step 29010: Loss: 0.0866, requires_grad: True\n","Step 29020: Loss: 0.2790, requires_grad: True\n","Step 29030: Loss: 0.6614, requires_grad: True\n","Step 29040: Loss: 0.1441, requires_grad: True\n","Step 29050: Loss: 0.3338, requires_grad: True\n","Step 29050: Gradients computed successfully\n","Step 29060: Loss: 0.2080, requires_grad: True\n","Step 29070: Loss: 0.1243, requires_grad: True\n","Step 29080: Loss: 0.1527, requires_grad: True\n","Step 29090: Loss: 0.1860, requires_grad: True\n","Step 29100: Loss: 0.0957, requires_grad: True\n","Step 29100: Gradients computed successfully\n","Step 29110: Loss: 0.1476, requires_grad: True\n","Step 29120: Loss: 0.0895, requires_grad: True\n","Step 29130: Loss: 0.2185, requires_grad: True\n","Step 29140: Loss: 0.0708, requires_grad: True\n","Step 29150: Loss: 0.1274, requires_grad: True\n","Step 29150: Gradients computed successfully\n","Step 29160: Loss: 0.1152, requires_grad: True\n","Step 29170: Loss: 0.2123, requires_grad: True\n","Step 29180: Loss: 0.1115, requires_grad: True\n","Step 29190: Loss: 0.1499, requires_grad: True\n","Step 29200: Loss: 0.2266, requires_grad: True\n","Step 29200: Gradients computed successfully\n","Step 29210: Loss: 0.0696, requires_grad: True\n","Step 29220: Loss: 0.1101, requires_grad: True\n","Step 29230: Loss: 0.1259, requires_grad: True\n","Step 29240: Loss: 0.2299, requires_grad: True\n","Step 29250: Loss: 0.1829, requires_grad: True\n","Step 29250: Gradients computed successfully\n","Step 29260: Loss: 0.1384, requires_grad: True\n","Step 29270: Loss: 0.1609, requires_grad: True\n","Step 29280: Loss: 0.0766, requires_grad: True\n","Step 29290: Loss: 0.1702, requires_grad: True\n","Step 29300: Loss: 0.1891, requires_grad: True\n","Step 29300: Gradients computed successfully\n","Step 29310: Loss: 0.1903, requires_grad: True\n","Step 29320: Loss: 0.1478, requires_grad: True\n","Step 29330: Loss: 0.2495, requires_grad: True\n","Step 29340: Loss: 0.0679, requires_grad: True\n","Step 29350: Loss: 0.5887, requires_grad: True\n","Step 29350: Gradients computed successfully\n","Step 29360: Loss: 0.2725, requires_grad: True\n","Step 29370: Loss: 0.2394, requires_grad: True\n","Step 29380: Loss: 0.1875, requires_grad: True\n","Step 29390: Loss: 0.1558, requires_grad: True\n","Step 29400: Loss: 0.3711, requires_grad: True\n","Step 29400: Gradients computed successfully\n","Step 29410: Loss: 0.1886, requires_grad: True\n","Step 29420: Loss: 0.2619, requires_grad: True\n","Step 29430: Loss: 0.2817, requires_grad: True\n","Step 29440: Loss: 0.2653, requires_grad: True\n","Step 29450: Loss: 0.0459, requires_grad: True\n","Step 29450: Gradients computed successfully\n","Step 29460: Loss: 0.2766, requires_grad: True\n","Step 29470: Loss: 0.2063, requires_grad: True\n","Step 29480: Loss: 0.4046, requires_grad: True\n","Step 29490: Loss: 0.6439, requires_grad: True\n","Step 29500: Loss: 0.1475, requires_grad: True\n","Step 29500: Gradients computed successfully\n","Step 29510: Loss: 0.3738, requires_grad: True\n","Step 29520: Loss: 0.1424, requires_grad: True\n","Step 29530: Loss: 0.1295, requires_grad: True\n","Step 29540: Loss: 0.1229, requires_grad: True\n","Step 29550: Loss: 0.2167, requires_grad: True\n","Step 29550: Gradients computed successfully\n","Step 29560: Loss: 0.2030, requires_grad: True\n","Step 29570: Loss: 0.3723, requires_grad: True\n","Step 29580: Loss: 0.0524, requires_grad: True\n","Step 29590: Loss: 0.8531, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-3700\n","Step 29600: Loss: 0.1518, requires_grad: True\n","Step 29600: Gradients computed successfully\n","Step 29610: Loss: 0.3000, requires_grad: True\n","Step 29620: Loss: 0.1356, requires_grad: True\n","Step 29630: Loss: 0.0789, requires_grad: True\n","Step 29640: Loss: 0.1520, requires_grad: True\n","Step 29650: Loss: 0.1470, requires_grad: True\n","Step 29650: Gradients computed successfully\n","Step 29660: Loss: 0.0665, requires_grad: True\n","Step 29670: Loss: 0.1010, requires_grad: True\n","Step 29680: Loss: 0.1059, requires_grad: True\n","Step 29690: Loss: 0.1229, requires_grad: True\n","Step 29700: Loss: 0.7269, requires_grad: True\n","Step 29700: Gradients computed successfully\n","Step 29710: Loss: 0.2118, requires_grad: True\n","Step 29720: Loss: 0.1514, requires_grad: True\n","Step 29730: Loss: 0.1374, requires_grad: True\n","Step 29740: Loss: 0.0674, requires_grad: True\n","Step 29750: Loss: 0.1510, requires_grad: True\n","Step 29750: Gradients computed successfully\n","Step 29760: Loss: 0.2479, requires_grad: True\n","Step 29770: Loss: 0.2032, requires_grad: True\n","Step 29780: Loss: 0.2679, requires_grad: True\n","Step 29790: Loss: 0.1942, requires_grad: True\n","Step 29800: Loss: 0.3387, requires_grad: True\n","Step 29800: Gradients computed successfully\n","Step 29810: Loss: 0.1840, requires_grad: True\n","Step 29820: Loss: 0.1842, requires_grad: True\n","Step 29830: Loss: 0.2273, requires_grad: True\n","Step 29840: Loss: 0.1666, requires_grad: True\n","Step 29850: Loss: 0.3409, requires_grad: True\n","Step 29850: Gradients computed successfully\n","Step 29860: Loss: 0.0690, requires_grad: True\n","Step 29870: Loss: 0.1437, requires_grad: True\n","Step 29880: Loss: 0.2888, requires_grad: True\n","Step 29890: Loss: 0.1471, requires_grad: True\n","Step 29900: Loss: 0.2358, requires_grad: True\n","Step 29900: Gradients computed successfully\n","Step 29910: Loss: 0.2227, requires_grad: True\n","Step 29920: Loss: 0.2756, requires_grad: True\n","Step 29930: Loss: 0.2692, requires_grad: True\n","Step 29940: Loss: 0.4913, requires_grad: True\n","Step 29950: Loss: 0.2880, requires_grad: True\n","Step 29950: Gradients computed successfully\n","Step 29960: Loss: 0.6104, requires_grad: True\n","Step 29970: Loss: 0.0782, requires_grad: True\n","Step 29980: Loss: 0.1085, requires_grad: True\n","Step 29990: Loss: 0.0791, requires_grad: True\n","Step 30000: Loss: 0.1112, requires_grad: True\n","Step 30000: Gradients computed successfully\n","Step 30010: Loss: 0.2933, requires_grad: True\n","Step 30020: Loss: 0.1849, requires_grad: True\n","Step 30030: Loss: 0.1157, requires_grad: True\n","Step 30040: Loss: 0.4975, requires_grad: True\n","Step 30050: Loss: 0.2134, requires_grad: True\n","Step 30050: Gradients computed successfully\n","Step 30060: Loss: 0.2737, requires_grad: True\n","Step 30070: Loss: 0.1306, requires_grad: True\n","Step 30080: Loss: 0.0800, requires_grad: True\n","Step 30090: Loss: 0.1733, requires_grad: True\n","Step 30100: Loss: 0.2286, requires_grad: True\n","Step 30100: Gradients computed successfully\n","Step 30110: Loss: 0.3741, requires_grad: True\n","Step 30120: Loss: 0.0902, requires_grad: True\n","Step 30130: Loss: 0.2971, requires_grad: True\n","Step 30140: Loss: 0.3374, requires_grad: True\n","Step 30150: Loss: 0.4597, requires_grad: True\n","Step 30150: Gradients computed successfully\n","Step 30160: Loss: 0.2114, requires_grad: True\n","Step 30170: Loss: 0.4162, requires_grad: True\n","Step 30180: Loss: 0.2652, requires_grad: True\n","Step 30190: Loss: 0.0743, requires_grad: True\n","Step 30200: Loss: 0.1911, requires_grad: True\n","Step 30200: Gradients computed successfully\n","Step 30210: Loss: 0.1265, requires_grad: True\n","Step 30220: Loss: 0.1758, requires_grad: True\n","Step 30230: Loss: 0.2918, requires_grad: True\n","Step 30240: Loss: 0.1897, requires_grad: True\n","Step 30250: Loss: 0.1105, requires_grad: True\n","Step 30250: Gradients computed successfully\n","Step 30260: Loss: 0.1806, requires_grad: True\n","Step 30270: Loss: 0.2100, requires_grad: True\n","Step 30280: Loss: 0.1672, requires_grad: True\n","Step 30290: Loss: 0.1977, requires_grad: True\n","Step 30300: Loss: 0.4216, requires_grad: True\n","Step 30300: Gradients computed successfully\n","Step 30310: Loss: 0.2618, requires_grad: True\n","Step 30320: Loss: 0.2291, requires_grad: True\n","Step 30330: Loss: 0.3066, requires_grad: True\n","Step 30340: Loss: 0.1347, requires_grad: True\n","Step 30350: Loss: 0.3429, requires_grad: True\n","Step 30350: Gradients computed successfully\n","Step 30360: Loss: 0.2790, requires_grad: True\n","Step 30370: Loss: 0.5247, requires_grad: True\n","Step 30380: Loss: 0.2601, requires_grad: True\n","Step 30390: Loss: 0.1264, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-3800\n","Step 30400: Loss: 0.1752, requires_grad: True\n","Step 30400: Gradients computed successfully\n","Step 30410: Loss: 0.1580, requires_grad: True\n","Step 30420: Loss: 0.2783, requires_grad: True\n","Step 30430: Loss: 0.0989, requires_grad: True\n","Step 30440: Loss: 0.1626, requires_grad: True\n","Step 30450: Loss: 0.1799, requires_grad: True\n","Step 30450: Gradients computed successfully\n","Step 30460: Loss: 0.1020, requires_grad: True\n","Step 30470: Loss: 0.2219, requires_grad: True\n","Step 30480: Loss: 0.1282, requires_grad: True\n","Step 30490: Loss: 0.2153, requires_grad: True\n","Step 30500: Loss: 0.2960, requires_grad: True\n","Step 30500: Gradients computed successfully\n","Step 30510: Loss: 0.2112, requires_grad: True\n","Step 30520: Loss: 0.1279, requires_grad: True\n","Step 30530: Loss: 0.1246, requires_grad: True\n","Step 30540: Loss: 0.2158, requires_grad: True\n","Step 30550: Loss: 0.1741, requires_grad: True\n","Step 30550: Gradients computed successfully\n","Step 30560: Loss: 0.7304, requires_grad: True\n","Step 30570: Loss: 0.2302, requires_grad: True\n","Step 30580: Loss: 0.1935, requires_grad: True\n","Step 30590: Loss: 0.2600, requires_grad: True\n","Step 30600: Loss: 0.1737, requires_grad: True\n","Step 30600: Gradients computed successfully\n","Step 30610: Loss: 0.5072, requires_grad: True\n","Step 30620: Loss: 0.2196, requires_grad: True\n","Step 30630: Loss: 0.2358, requires_grad: True\n","Step 30640: Loss: 0.1897, requires_grad: True\n","Step 30650: Loss: 0.3054, requires_grad: True\n","Step 30650: Gradients computed successfully\n","Step 30660: Loss: 0.3627, requires_grad: True\n","Step 30670: Loss: 0.1119, requires_grad: True\n","Step 30680: Loss: 0.1716, requires_grad: True\n","Step 30690: Loss: 0.3026, requires_grad: True\n","Step 30700: Loss: 0.1808, requires_grad: True\n","Step 30700: Gradients computed successfully\n","Step 30710: Loss: 0.2798, requires_grad: True\n","Step 30720: Loss: 0.2645, requires_grad: True\n","Step 30730: Loss: 0.2871, requires_grad: True\n","Step 30740: Loss: 0.1454, requires_grad: True\n","Step 30750: Loss: 0.2436, requires_grad: True\n","Step 30750: Gradients computed successfully\n","Step 30760: Loss: 0.1717, requires_grad: True\n","Step 30770: Loss: 0.2080, requires_grad: True\n","Step 30780: Loss: 0.2604, requires_grad: True\n","Step 30790: Loss: 0.1282, requires_grad: True\n","Step 30800: Loss: 0.1416, requires_grad: True\n","Step 30800: Gradients computed successfully\n","Step 30810: Loss: 0.2296, requires_grad: True\n","Step 30820: Loss: 0.1138, requires_grad: True\n","Step 30830: Loss: 0.2797, requires_grad: True\n","Step 30840: Loss: 0.3933, requires_grad: True\n","Step 30850: Loss: 0.0802, requires_grad: True\n","Step 30850: Gradients computed successfully\n","Step 30860: Loss: 0.2467, requires_grad: True\n","Step 30870: Loss: 0.2392, requires_grad: True\n","Step 30880: Loss: 0.2022, requires_grad: True\n","Step 30890: Loss: 0.2981, requires_grad: True\n","Step 30900: Loss: 0.1770, requires_grad: True\n","Step 30900: Gradients computed successfully\n","Step 30910: Loss: 0.2365, requires_grad: True\n","Step 30920: Loss: 0.1886, requires_grad: True\n","Step 30930: Loss: 0.2127, requires_grad: True\n","Step 30940: Loss: 0.0758, requires_grad: True\n","Step 30950: Loss: 0.3080, requires_grad: True\n","Step 30950: Gradients computed successfully\n","Step 30960: Loss: 0.0832, requires_grad: True\n","Step 30970: Loss: 0.2352, requires_grad: True\n","Step 30980: Loss: 0.1380, requires_grad: True\n","Step 30990: Loss: 0.3167, requires_grad: True\n","Step 31000: Loss: 0.4055, requires_grad: True\n","Step 31000: Gradients computed successfully\n","Step 31010: Loss: 0.2881, requires_grad: True\n","Step 31020: Loss: 0.2379, requires_grad: True\n","Step 31030: Loss: 0.2510, requires_grad: True\n","Step 31040: Loss: 0.2828, requires_grad: True\n","Step 31050: Loss: 0.2271, requires_grad: True\n","Step 31050: Gradients computed successfully\n","Step 31060: Loss: 0.2290, requires_grad: True\n","Step 31070: Loss: 0.2230, requires_grad: True\n","Step 31080: Loss: 0.2237, requires_grad: True\n","Step 31090: Loss: 0.2252, requires_grad: True\n","Step 31100: Loss: 0.1928, requires_grad: True\n","Step 31100: Gradients computed successfully\n","Step 31110: Loss: 0.2089, requires_grad: True\n","Step 31120: Loss: 0.5741, requires_grad: True\n","Step 31130: Loss: 0.2038, requires_grad: True\n","Step 31140: Loss: 0.1724, requires_grad: True\n","Step 31150: Loss: 0.1716, requires_grad: True\n","Step 31150: Gradients computed successfully\n","Step 31160: Loss: 0.2663, requires_grad: True\n","Step 31170: Loss: 0.1645, requires_grad: True\n","Step 31180: Loss: 0.0509, requires_grad: True\n","Step 31190: Loss: 0.2158, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-3900\n","Step 31200: Loss: 0.1525, requires_grad: True\n","Step 31200: Gradients computed successfully\n","Step 31210: Loss: 0.1371, requires_grad: True\n","Step 31220: Loss: 0.1399, requires_grad: True\n","Step 31230: Loss: 0.1599, requires_grad: True\n","Step 31240: Loss: 0.1137, requires_grad: True\n","Step 31250: Loss: 0.1789, requires_grad: True\n","Step 31250: Gradients computed successfully\n","Step 31260: Loss: 0.1265, requires_grad: True\n","Step 31270: Loss: 0.2968, requires_grad: True\n","Step 31280: Loss: 0.5271, requires_grad: True\n","Step 31290: Loss: 0.2221, requires_grad: True\n","Step 31300: Loss: 0.1100, requires_grad: True\n","Step 31300: Gradients computed successfully\n","Step 31310: Loss: 0.2824, requires_grad: True\n","Step 31320: Loss: 0.2371, requires_grad: True\n","Step 31330: Loss: 0.3348, requires_grad: True\n","Step 31340: Loss: 0.1886, requires_grad: True\n","Step 31350: Loss: 0.1122, requires_grad: True\n","Step 31350: Gradients computed successfully\n","Step 31360: Loss: 0.0792, requires_grad: True\n","Step 31370: Loss: 0.1959, requires_grad: True\n","Step 31380: Loss: 0.1657, requires_grad: True\n","Step 31390: Loss: 0.1579, requires_grad: True\n","Step 31400: Loss: 0.1919, requires_grad: True\n","Step 31400: Gradients computed successfully\n","Step 31410: Loss: 0.2206, requires_grad: True\n","Step 31420: Loss: 0.3526, requires_grad: True\n","Step 31430: Loss: 0.4050, requires_grad: True\n","Step 31440: Loss: 0.0635, requires_grad: True\n","Step 31450: Loss: 0.5249, requires_grad: True\n","Step 31450: Gradients computed successfully\n","Step 31460: Loss: 0.2571, requires_grad: True\n","Step 31470: Loss: 0.1617, requires_grad: True\n","Step 31480: Loss: 0.2351, requires_grad: True\n","Step 31490: Loss: 0.1986, requires_grad: True\n","Step 31500: Loss: 0.3678, requires_grad: True\n","Step 31500: Gradients computed successfully\n","Step 31510: Loss: 0.1436, requires_grad: True\n","Step 31520: Loss: 0.1667, requires_grad: True\n","Step 31530: Loss: 0.4038, requires_grad: True\n","Step 31540: Loss: 0.4043, requires_grad: True\n","Step 31550: Loss: 0.3081, requires_grad: True\n","Step 31550: Gradients computed successfully\n","Step 31560: Loss: 0.3295, requires_grad: True\n","Step 31570: Loss: 0.2348, requires_grad: True\n","Step 31580: Loss: 0.4326, requires_grad: True\n","Step 31590: Loss: 0.1601, requires_grad: True\n","Step 31600: Loss: 0.1669, requires_grad: True\n","Step 31600: Gradients computed successfully\n","Step 31610: Loss: 0.3178, requires_grad: True\n","Step 31620: Loss: 0.2742, requires_grad: True\n","Step 31630: Loss: 0.0617, requires_grad: True\n","Step 31640: Loss: 0.0943, requires_grad: True\n","Step 31650: Loss: 0.1706, requires_grad: True\n","Step 31650: Gradients computed successfully\n","Step 31660: Loss: 0.3597, requires_grad: True\n","Step 31670: Loss: 0.2609, requires_grad: True\n","Step 31680: Loss: 0.1768, requires_grad: True\n","Step 31690: Loss: 0.2178, requires_grad: True\n","Step 31700: Loss: 0.1546, requires_grad: True\n","Step 31700: Gradients computed successfully\n","Step 31710: Loss: 0.2309, requires_grad: True\n","Step 31720: Loss: 0.2093, requires_grad: True\n","Step 31730: Loss: 0.1616, requires_grad: True\n","Step 31740: Loss: 0.1368, requires_grad: True\n","Step 31750: Loss: 0.2100, requires_grad: True\n","Step 31750: Gradients computed successfully\n","Step 31760: Loss: 0.4509, requires_grad: True\n","Step 31770: Loss: 0.1255, requires_grad: True\n","Step 31780: Loss: 0.2445, requires_grad: True\n","Step 31790: Loss: 0.1894, requires_grad: True\n","Step 31800: Loss: 0.1208, requires_grad: True\n","Step 31800: Gradients computed successfully\n","Step 31810: Loss: 0.2532, requires_grad: True\n","Step 31820: Loss: 0.1991, requires_grad: True\n","Step 31830: Loss: 0.3009, requires_grad: True\n","Step 31840: Loss: 0.3436, requires_grad: True\n","Step 31850: Loss: 0.1240, requires_grad: True\n","Step 31850: Gradients computed successfully\n","Step 31860: Loss: 0.1026, requires_grad: True\n","Step 31870: Loss: 0.1043, requires_grad: True\n","Step 31880: Loss: 0.4888, requires_grad: True\n","Step 31890: Loss: 0.0729, requires_grad: True\n","Step 31900: Loss: 0.2775, requires_grad: True\n","Step 31900: Gradients computed successfully\n","Step 31910: Loss: 0.1231, requires_grad: True\n","Step 31920: Loss: 0.2510, requires_grad: True\n","Step 31930: Loss: 0.4885, requires_grad: True\n","Step 31940: Loss: 0.3232, requires_grad: True\n","Step 31950: Loss: 0.2902, requires_grad: True\n","Step 31950: Gradients computed successfully\n","Step 31960: Loss: 0.1401, requires_grad: True\n","Step 31970: Loss: 0.0996, requires_grad: True\n","Step 31980: Loss: 0.1573, requires_grad: True\n","Step 31990: Loss: 0.4392, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-4000\n","Step 32000: Loss: 0.2500, requires_grad: True\n","Step 32000: Gradients computed successfully\n","Step 32010: Loss: 0.1218, requires_grad: True\n","Step 32020: Loss: 0.4642, requires_grad: True\n","Step 32030: Loss: 0.2125, requires_grad: True\n","Step 32040: Loss: 0.1079, requires_grad: True\n","Step 32050: Loss: 0.1820, requires_grad: True\n","Step 32050: Gradients computed successfully\n","Step 32060: Loss: 0.1972, requires_grad: True\n","Step 32070: Loss: 0.0816, requires_grad: True\n","Step 32080: Loss: 0.2382, requires_grad: True\n","Step 32090: Loss: 0.2464, requires_grad: True\n","Step 32100: Loss: 0.2059, requires_grad: True\n","Step 32100: Gradients computed successfully\n","Step 32110: Loss: 0.0772, requires_grad: True\n","Step 32120: Loss: 0.3997, requires_grad: True\n","Step 32130: Loss: 0.4093, requires_grad: True\n","Step 32140: Loss: 0.0703, requires_grad: True\n","Step 32150: Loss: 0.3902, requires_grad: True\n","Step 32150: Gradients computed successfully\n","Step 32160: Loss: 0.1068, requires_grad: True\n","Step 32170: Loss: 0.4425, requires_grad: True\n","Step 32180: Loss: 0.0710, requires_grad: True\n","Step 32190: Loss: 0.1820, requires_grad: True\n","Step 32200: Loss: 0.1935, requires_grad: True\n","Step 32200: Gradients computed successfully\n","Step 32210: Loss: 0.1948, requires_grad: True\n","Step 32220: Loss: 0.3238, requires_grad: True\n","Step 32230: Loss: 0.1381, requires_grad: True\n","Step 32240: Loss: 0.2671, requires_grad: True\n","Step 32250: Loss: 0.2586, requires_grad: True\n","Step 32250: Gradients computed successfully\n","Step 32260: Loss: 0.0750, requires_grad: True\n","Step 32270: Loss: 0.1103, requires_grad: True\n","Step 32280: Loss: 0.1020, requires_grad: True\n","Step 32290: Loss: 0.4337, requires_grad: True\n","Step 32300: Loss: 0.1182, requires_grad: True\n","Step 32300: Gradients computed successfully\n","Step 32310: Loss: 0.5081, requires_grad: True\n","Step 32320: Loss: 0.1641, requires_grad: True\n","Step 32330: Loss: 0.2248, requires_grad: True\n","Step 32340: Loss: 0.3913, requires_grad: True\n","Step 32350: Loss: 0.1400, requires_grad: True\n","Step 32350: Gradients computed successfully\n","Step 32360: Loss: 0.1944, requires_grad: True\n","Step 32370: Loss: 0.0985, requires_grad: True\n","Step 32380: Loss: 0.4484, requires_grad: True\n","Step 32390: Loss: 0.5777, requires_grad: True\n","Step 32400: Loss: 0.1397, requires_grad: True\n","Step 32400: Gradients computed successfully\n","Step 32410: Loss: 0.1562, requires_grad: True\n","Step 32420: Loss: 0.3342, requires_grad: True\n","Step 32430: Loss: 0.2413, requires_grad: True\n","Step 32440: Loss: 0.1788, requires_grad: True\n","Step 32450: Loss: 0.2514, requires_grad: True\n","Step 32450: Gradients computed successfully\n","Step 32460: Loss: 0.1830, requires_grad: True\n","Step 32470: Loss: 0.1162, requires_grad: True\n","Step 32480: Loss: 0.4299, requires_grad: True\n","Step 32490: Loss: 0.1199, requires_grad: True\n","Step 32500: Loss: 0.3242, requires_grad: True\n","Step 32500: Gradients computed successfully\n","Step 32510: Loss: 0.2750, requires_grad: True\n","Step 32520: Loss: 0.0716, requires_grad: True\n","Step 32530: Loss: 0.3141, requires_grad: True\n","Step 32540: Loss: 0.2135, requires_grad: True\n","Step 32550: Loss: 0.4316, requires_grad: True\n","Step 32550: Gradients computed successfully\n","Step 32560: Loss: 0.2127, requires_grad: True\n","Step 32570: Loss: 0.3247, requires_grad: True\n","Step 32580: Loss: 0.2235, requires_grad: True\n","Step 32590: Loss: 0.1013, requires_grad: True\n","Step 32600: Loss: 0.3065, requires_grad: True\n","Step 32600: Gradients computed successfully\n","Step 32610: Loss: 0.0909, requires_grad: True\n","Step 32620: Loss: 0.1297, requires_grad: True\n","Step 32630: Loss: 0.2694, requires_grad: True\n","Step 32640: Loss: 0.2231, requires_grad: True\n","Step 32650: Loss: 0.1553, requires_grad: True\n","Step 32650: Gradients computed successfully\n","Step 32660: Loss: 0.2952, requires_grad: True\n","Step 32670: Loss: 0.1517, requires_grad: True\n","Step 32680: Loss: 0.2758, requires_grad: True\n","Step 32690: Loss: 0.1549, requires_grad: True\n","Step 32700: Loss: 0.5134, requires_grad: True\n","Step 32700: Gradients computed successfully\n","Step 32710: Loss: 0.5100, requires_grad: True\n","Step 32720: Loss: 0.3030, requires_grad: True\n","Step 32730: Loss: 0.1813, requires_grad: True\n","Step 32740: Loss: 0.1497, requires_grad: True\n","Step 32750: Loss: 0.0950, requires_grad: True\n","Step 32750: Gradients computed successfully\n","Step 32760: Loss: 0.2551, requires_grad: True\n","Step 32770: Loss: 0.1980, requires_grad: True\n","Step 32780: Loss: 0.3311, requires_grad: True\n","Step 32790: Loss: 0.2573, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-4100\n","Step 32800: Loss: 0.4197, requires_grad: True\n","Step 32800: Gradients computed successfully\n","Step 32810: Loss: 0.0755, requires_grad: True\n","Step 32820: Loss: 0.4146, requires_grad: True\n","Step 32830: Loss: 0.2810, requires_grad: True\n","Step 32840: Loss: 0.1538, requires_grad: True\n","Step 32850: Loss: 0.2246, requires_grad: True\n","Step 32850: Gradients computed successfully\n","Step 32860: Loss: 0.4124, requires_grad: True\n","Step 32870: Loss: 0.1492, requires_grad: True\n","Step 32880: Loss: 0.0612, requires_grad: True\n","Step 32890: Loss: 0.1172, requires_grad: True\n","Step 32900: Loss: 0.3375, requires_grad: True\n","Step 32900: Gradients computed successfully\n","Step 32910: Loss: 0.1803, requires_grad: True\n","Step 32920: Loss: 0.4245, requires_grad: True\n","Step 32930: Loss: 0.1184, requires_grad: True\n","Step 32940: Loss: 0.2873, requires_grad: True\n","Step 32950: Loss: 0.2619, requires_grad: True\n","Step 32950: Gradients computed successfully\n","Step 32960: Loss: 0.4529, requires_grad: True\n","Step 32970: Loss: 0.1234, requires_grad: True\n","Step 32980: Loss: 0.2647, requires_grad: True\n","Step 32990: Loss: 0.2557, requires_grad: True\n","Step 33000: Loss: 0.0938, requires_grad: True\n","Step 33000: Gradients computed successfully\n","Step 33010: Loss: 0.0885, requires_grad: True\n","Step 33020: Loss: 0.3621, requires_grad: True\n","Step 33030: Loss: 0.3060, requires_grad: True\n","Step 33040: Loss: 0.1356, requires_grad: True\n","Step 33050: Loss: 0.4246, requires_grad: True\n","Step 33050: Gradients computed successfully\n","Step 33060: Loss: 0.1154, requires_grad: True\n","Step 33070: Loss: 0.1528, requires_grad: True\n","Step 33080: Loss: 0.1258, requires_grad: True\n","Step 33090: Loss: 0.1760, requires_grad: True\n","Step 33100: Loss: 0.1485, requires_grad: True\n","Step 33100: Gradients computed successfully\n","Step 33110: Loss: 0.2551, requires_grad: True\n","Step 33120: Loss: 0.3666, requires_grad: True\n","Step 33130: Loss: 0.1748, requires_grad: True\n","Step 33140: Loss: 0.3838, requires_grad: True\n","Step 33150: Loss: 0.2114, requires_grad: True\n","Step 33150: Gradients computed successfully\n","Step 33160: Loss: 0.1762, requires_grad: True\n","Step 33170: Loss: 0.1610, requires_grad: True\n","Step 33180: Loss: 0.2923, requires_grad: True\n","Step 33190: Loss: 0.2504, requires_grad: True\n","Step 33200: Loss: 0.2996, requires_grad: True\n","Step 33200: Gradients computed successfully\n","Step 33210: Loss: 0.1233, requires_grad: True\n","Step 33220: Loss: 0.1977, requires_grad: True\n","Step 33230: Loss: 0.2865, requires_grad: True\n","Step 33240: Loss: 0.3086, requires_grad: True\n","Step 33250: Loss: 0.1707, requires_grad: True\n","Step 33250: Gradients computed successfully\n","Step 33260: Loss: 0.1006, requires_grad: True\n","Step 33270: Loss: 0.3057, requires_grad: True\n","Step 33280: Loss: 0.1432, requires_grad: True\n","Step 33290: Loss: 0.2319, requires_grad: True\n","Step 33300: Loss: 0.1363, requires_grad: True\n","Step 33300: Gradients computed successfully\n","Step 33310: Loss: 0.1750, requires_grad: True\n","Step 33320: Loss: 0.2304, requires_grad: True\n","Step 33330: Loss: 0.0854, requires_grad: True\n","Step 33340: Loss: 0.1982, requires_grad: True\n","Step 33350: Loss: 0.2394, requires_grad: True\n","Step 33350: Gradients computed successfully\n","Step 33360: Loss: 0.3132, requires_grad: True\n","Step 33370: Loss: 0.2631, requires_grad: True\n","Step 33380: Loss: 0.3822, requires_grad: True\n","Step 33390: Loss: 0.3322, requires_grad: True\n","Step 33400: Loss: 0.1961, requires_grad: True\n","Step 33400: Gradients computed successfully\n","Step 33410: Loss: 0.0808, requires_grad: True\n","Step 33420: Loss: 0.1638, requires_grad: True\n","Step 33430: Loss: 0.3493, requires_grad: True\n","Step 33440: Loss: 0.0738, requires_grad: True\n","Step 33450: Loss: 0.0502, requires_grad: True\n","Step 33450: Gradients computed successfully\n","Step 33460: Loss: 0.1286, requires_grad: True\n","Step 33470: Loss: 0.2761, requires_grad: True\n","Step 33480: Loss: 0.3079, requires_grad: True\n","Step 33490: Loss: 0.2668, requires_grad: True\n","Step 33500: Loss: 0.1436, requires_grad: True\n","Step 33500: Gradients computed successfully\n","Step 33510: Loss: 0.5440, requires_grad: True\n","Step 33520: Loss: 0.3415, requires_grad: True\n","Step 33530: Loss: 0.2875, requires_grad: True\n","Step 33540: Loss: 0.1175, requires_grad: True\n","Step 33550: Loss: 0.3559, requires_grad: True\n","Step 33550: Gradients computed successfully\n","Step 33560: Loss: 0.1961, requires_grad: True\n","Step 33570: Loss: 0.2546, requires_grad: True\n","Step 33580: Loss: 0.2166, requires_grad: True\n","Step 33590: Loss: 0.1425, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-4200\n","Step 33600: Loss: 0.1948, requires_grad: True\n","Step 33600: Gradients computed successfully\n","Step 33610: Loss: 0.2317, requires_grad: True\n","Step 33620: Loss: 0.5305, requires_grad: True\n","Step 33630: Loss: 0.3856, requires_grad: True\n","Step 33640: Loss: 0.1465, requires_grad: True\n","Step 33650: Loss: 0.1503, requires_grad: True\n","Step 33650: Gradients computed successfully\n","Step 33660: Loss: 0.2143, requires_grad: True\n","Step 33670: Loss: 0.1103, requires_grad: True\n","Step 33680: Loss: 0.0424, requires_grad: True\n","Step 33690: Loss: 0.1681, requires_grad: True\n","Step 33700: Loss: 0.2963, requires_grad: True\n","Step 33700: Gradients computed successfully\n","Step 33710: Loss: 0.1207, requires_grad: True\n","Step 33720: Loss: 0.1923, requires_grad: True\n","Step 33730: Loss: 0.3085, requires_grad: True\n","Step 33740: Loss: 0.1840, requires_grad: True\n","Step 33750: Loss: 0.1694, requires_grad: True\n","Step 33750: Gradients computed successfully\n","Step 33760: Loss: 0.1711, requires_grad: True\n","Step 33770: Loss: 0.1735, requires_grad: True\n","Step 33780: Loss: 0.0883, requires_grad: True\n","Step 33790: Loss: 0.2342, requires_grad: True\n","Step 33800: Loss: 0.2397, requires_grad: True\n","Step 33800: Gradients computed successfully\n","Step 33810: Loss: 0.1450, requires_grad: True\n","Step 33820: Loss: 0.2848, requires_grad: True\n","Step 33830: Loss: 0.2838, requires_grad: True\n","Step 33840: Loss: 0.2133, requires_grad: True\n","Step 33850: Loss: 0.3575, requires_grad: True\n","Step 33850: Gradients computed successfully\n","Step 33860: Loss: 0.1369, requires_grad: True\n","Step 33870: Loss: 0.1892, requires_grad: True\n","Step 33880: Loss: 0.1143, requires_grad: True\n","Step 33890: Loss: 0.3480, requires_grad: True\n","Step 33900: Loss: 0.0960, requires_grad: True\n","Step 33900: Gradients computed successfully\n","Step 33910: Loss: 0.1429, requires_grad: True\n","Step 33920: Loss: 0.1789, requires_grad: True\n","Step 33930: Loss: 0.1009, requires_grad: True\n","Step 33940: Loss: 0.0826, requires_grad: True\n","Step 33950: Loss: 0.1508, requires_grad: True\n","Step 33950: Gradients computed successfully\n","Step 33960: Loss: 0.2541, requires_grad: True\n","Step 33970: Loss: 0.2309, requires_grad: True\n","Step 33980: Loss: 0.1653, requires_grad: True\n","Step 33990: Loss: 0.1944, requires_grad: True\n","Step 34000: Loss: 0.3375, requires_grad: True\n","Step 34000: Gradients computed successfully\n","Step 34010: Loss: 0.1053, requires_grad: True\n","Step 34020: Loss: 0.1350, requires_grad: True\n","Step 34030: Loss: 0.1658, requires_grad: True\n","Step 34040: Loss: 0.2632, requires_grad: True\n","Step 34050: Loss: 0.1527, requires_grad: True\n","Step 34050: Gradients computed successfully\n","Step 34060: Loss: 0.1967, requires_grad: True\n","Step 34070: Loss: 0.3499, requires_grad: True\n","Step 34080: Loss: 0.0907, requires_grad: True\n","Step 34090: Loss: 0.3344, requires_grad: True\n","Step 34100: Loss: 0.2539, requires_grad: True\n","Step 34100: Gradients computed successfully\n","Step 34110: Loss: 0.0997, requires_grad: True\n","Step 34120: Loss: 0.3142, requires_grad: True\n","Step 34130: Loss: 0.1101, requires_grad: True\n","Step 34140: Loss: 0.1883, requires_grad: True\n","Step 34150: Loss: 0.2776, requires_grad: True\n","Step 34150: Gradients computed successfully\n","Step 34160: Loss: 0.1052, requires_grad: True\n","Step 34170: Loss: 0.1001, requires_grad: True\n","Step 34180: Loss: 0.4647, requires_grad: True\n","Step 34190: Loss: 0.4884, requires_grad: True\n","Step 34200: Loss: 0.3209, requires_grad: True\n","Step 34200: Gradients computed successfully\n","Step 34210: Loss: 0.2141, requires_grad: True\n","Step 34220: Loss: 0.1952, requires_grad: True\n","Step 34230: Loss: 0.2343, requires_grad: True\n","Step 34240: Loss: 0.1712, requires_grad: True\n","Step 34250: Loss: 0.2661, requires_grad: True\n","Step 34250: Gradients computed successfully\n","Step 34260: Loss: 0.3417, requires_grad: True\n","Step 34270: Loss: 0.1883, requires_grad: True\n","Step 34280: Loss: 0.0706, requires_grad: True\n","Step 34290: Loss: 0.0527, requires_grad: True\n","Step 34300: Loss: 0.1589, requires_grad: True\n","Step 34300: Gradients computed successfully\n","Step 34310: Loss: 0.1967, requires_grad: True\n","Step 34320: Loss: 0.2613, requires_grad: True\n","Step 34330: Loss: 0.1647, requires_grad: True\n","Step 34340: Loss: 0.2022, requires_grad: True\n","Step 34350: Loss: 0.0793, requires_grad: True\n","Step 34350: Gradients computed successfully\n","Step 34360: Loss: 0.1289, requires_grad: True\n","Step 34370: Loss: 0.2171, requires_grad: True\n","Step 34380: Loss: 0.1388, requires_grad: True\n","Step 34390: Loss: 0.2701, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-4300\n","Step 34400: Loss: 0.3368, requires_grad: True\n","Step 34400: Gradients computed successfully\n","Step 34410: Loss: 0.2212, requires_grad: True\n","Step 34420: Loss: 0.1946, requires_grad: True\n","Step 34430: Loss: 0.2392, requires_grad: True\n","Step 34440: Loss: 0.0555, requires_grad: True\n","Step 34450: Loss: 0.1610, requires_grad: True\n","Step 34450: Gradients computed successfully\n","Step 34460: Loss: 0.3611, requires_grad: True\n","Step 34470: Loss: 0.2869, requires_grad: True\n","Step 34480: Loss: 0.1250, requires_grad: True\n","Step 34490: Loss: 0.2941, requires_grad: True\n","Step 34500: Loss: 0.1251, requires_grad: True\n","Step 34500: Gradients computed successfully\n","Step 34510: Loss: 0.1890, requires_grad: True\n","Step 34520: Loss: 0.1248, requires_grad: True\n","Step 34530: Loss: 0.2251, requires_grad: True\n","Step 34540: Loss: 0.1541, requires_grad: True\n","Step 34550: Loss: 0.1604, requires_grad: True\n","Step 34550: Gradients computed successfully\n","Step 34560: Loss: 0.1473, requires_grad: True\n","Step 34570: Loss: 0.1932, requires_grad: True\n","Step 34580: Loss: 0.1967, requires_grad: True\n","Step 34590: Loss: 0.1501, requires_grad: True\n","Step 34600: Loss: 0.2984, requires_grad: True\n","Step 34600: Gradients computed successfully\n","Step 34610: Loss: 0.2793, requires_grad: True\n","Step 34620: Loss: 0.1528, requires_grad: True\n","Step 34630: Loss: 0.2902, requires_grad: True\n","Step 34640: Loss: 0.1469, requires_grad: True\n","Step 34650: Loss: 0.2627, requires_grad: True\n","Step 34650: Gradients computed successfully\n","Step 34660: Loss: 0.5399, requires_grad: True\n","Step 34670: Loss: 0.3267, requires_grad: True\n","Step 34680: Loss: 0.0608, requires_grad: True\n","Step 34690: Loss: 0.1726, requires_grad: True\n","Step 34700: Loss: 0.3633, requires_grad: True\n","Step 34700: Gradients computed successfully\n","Step 34710: Loss: 0.3365, requires_grad: True\n","Step 34720: Loss: 0.1867, requires_grad: True\n","Step 34730: Loss: 0.2632, requires_grad: True\n","Step 34740: Loss: 0.4709, requires_grad: True\n","Step 34750: Loss: 0.2202, requires_grad: True\n","Step 34750: Gradients computed successfully\n","Step 34760: Loss: 0.1343, requires_grad: True\n","Step 34770: Loss: 0.3063, requires_grad: True\n","Step 34780: Loss: 0.3332, requires_grad: True\n","Step 34790: Loss: 0.0996, requires_grad: True\n","Step 34800: Loss: 0.1983, requires_grad: True\n","Step 34800: Gradients computed successfully\n","Step 34810: Loss: 0.2097, requires_grad: True\n","Step 34820: Loss: 0.1372, requires_grad: True\n","Step 34830: Loss: 0.1512, requires_grad: True\n","Step 34840: Loss: 0.1114, requires_grad: True\n","Step 34850: Loss: 0.1231, requires_grad: True\n","Step 34850: Gradients computed successfully\n","Step 34860: Loss: 0.1706, requires_grad: True\n","Step 34870: Loss: 0.1382, requires_grad: True\n","Step 34880: Loss: 0.1579, requires_grad: True\n","Step 34890: Loss: 0.1622, requires_grad: True\n","Step 34900: Loss: 0.1807, requires_grad: True\n","Step 34900: Gradients computed successfully\n","Step 34910: Loss: 0.1232, requires_grad: True\n","Step 34920: Loss: 0.2045, requires_grad: True\n","Step 34930: Loss: 0.4618, requires_grad: True\n","Step 34940: Loss: 0.2371, requires_grad: True\n","Step 34950: Loss: 0.3269, requires_grad: True\n","Step 34950: Gradients computed successfully\n","Step 34960: Loss: 0.0380, requires_grad: True\n","Step 34970: Loss: 0.2644, requires_grad: True\n","Step 34980: Loss: 0.0936, requires_grad: True\n","Step 34990: Loss: 0.3837, requires_grad: True\n","Step 35000: Loss: 0.3268, requires_grad: True\n","Step 35000: Gradients computed successfully\n","Step 35010: Loss: 0.0731, requires_grad: True\n","Step 35020: Loss: 0.4251, requires_grad: True\n","Step 35030: Loss: 0.1357, requires_grad: True\n","Step 35040: Loss: 0.2387, requires_grad: True\n","Step 35050: Loss: 0.1492, requires_grad: True\n","Step 35050: Gradients computed successfully\n","Step 35060: Loss: 0.2147, requires_grad: True\n","Step 35070: Loss: 0.2559, requires_grad: True\n","Step 35080: Loss: 0.2699, requires_grad: True\n","Step 35090: Loss: 0.1099, requires_grad: True\n","Step 35100: Loss: 0.2288, requires_grad: True\n","Step 35100: Gradients computed successfully\n","Step 35110: Loss: 0.3247, requires_grad: True\n","Step 35120: Loss: 0.2450, requires_grad: True\n","Step 35130: Loss: 0.3316, requires_grad: True\n","Step 35140: Loss: 0.0835, requires_grad: True\n","Epoch 1 completed. Average loss: 0.2655\n","Saved model at end of epoch 1 to Checkpoints/mistral_complex_sql_training_4090/epoch-1\n","\n","Epoch 2/2\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f303be7a5e1a48a39a8f1e1d7299bb6c","version_major":2,"version_minor":0},"text/plain":["Epoch 2:   0%|          | 0/35150 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Step 0: Loss: 0.2351, requires_grad: True\n","Step 0: Gradients computed successfully\n","Step 10: Loss: 0.4300, requires_grad: True\n","Step 20: Loss: 0.2056, requires_grad: True\n","Step 30: Loss: 0.1964, requires_grad: True\n","Step 40: Loss: 0.1094, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-4400\n","Step 50: Loss: 0.1670, requires_grad: True\n","Step 50: Gradients computed successfully\n","Step 60: Loss: 0.3024, requires_grad: True\n","Step 70: Loss: 0.3094, requires_grad: True\n","Step 80: Loss: 0.1763, requires_grad: True\n","Step 90: Loss: 0.2580, requires_grad: True\n","Step 100: Loss: 0.2136, requires_grad: True\n","Step 100: Gradients computed successfully\n","Step 110: Loss: 0.1623, requires_grad: True\n","Step 120: Loss: 0.2967, requires_grad: True\n","Step 130: Loss: 0.3950, requires_grad: True\n","Step 140: Loss: 0.3261, requires_grad: True\n","Step 150: Loss: 0.1117, requires_grad: True\n","Step 150: Gradients computed successfully\n","Step 160: Loss: 0.2084, requires_grad: True\n","Step 170: Loss: 0.3866, requires_grad: True\n","Step 180: Loss: 0.2627, requires_grad: True\n","Step 190: Loss: 0.3220, requires_grad: True\n","Step 200: Loss: 0.0501, requires_grad: True\n","Step 200: Gradients computed successfully\n","Step 210: Loss: 0.3780, requires_grad: True\n","Step 220: Loss: 0.1783, requires_grad: True\n","Step 230: Loss: 0.2893, requires_grad: True\n","Step 240: Loss: 0.0663, requires_grad: True\n","Step 250: Loss: 0.2536, requires_grad: True\n","Step 250: Gradients computed successfully\n","Step 260: Loss: 0.1676, requires_grad: True\n","Step 270: Loss: 0.1960, requires_grad: True\n","Step 280: Loss: 0.1875, requires_grad: True\n","Step 290: Loss: 0.1656, requires_grad: True\n","Step 300: Loss: 0.3860, requires_grad: True\n","Step 300: Gradients computed successfully\n","Step 310: Loss: 0.1644, requires_grad: True\n","Step 320: Loss: 0.1359, requires_grad: True\n","Step 330: Loss: 0.2472, requires_grad: True\n","Step 340: Loss: 0.2019, requires_grad: True\n","Step 350: Loss: 0.1221, requires_grad: True\n","Step 350: Gradients computed successfully\n","Step 360: Loss: 0.4160, requires_grad: True\n","Step 370: Loss: 0.1160, requires_grad: True\n","Step 380: Loss: 0.1986, requires_grad: True\n","Step 390: Loss: 0.3903, requires_grad: True\n","Step 400: Loss: 0.3312, requires_grad: True\n","Step 400: Gradients computed successfully\n","Step 410: Loss: 0.3999, requires_grad: True\n","Step 420: Loss: 0.2772, requires_grad: True\n","Step 430: Loss: 0.2847, requires_grad: True\n","Step 440: Loss: 0.5204, requires_grad: True\n","Step 450: Loss: 0.2005, requires_grad: True\n","Step 450: Gradients computed successfully\n","Step 460: Loss: 0.1615, requires_grad: True\n","Step 470: Loss: 0.1519, requires_grad: True\n","Step 480: Loss: 0.1657, requires_grad: True\n","Step 490: Loss: 0.1360, requires_grad: True\n","Step 500: Loss: 0.1596, requires_grad: True\n","Step 500: Gradients computed successfully\n","Step 510: Loss: 0.1285, requires_grad: True\n","Step 520: Loss: 0.1103, requires_grad: True\n","Step 530: Loss: 0.0674, requires_grad: True\n","Step 540: Loss: 0.2640, requires_grad: True\n","Step 550: Loss: 0.1269, requires_grad: True\n","Step 550: Gradients computed successfully\n","Step 560: Loss: 0.1289, requires_grad: True\n","Step 570: Loss: 0.2543, requires_grad: True\n","Step 580: Loss: 0.2551, requires_grad: True\n","Step 590: Loss: 0.1989, requires_grad: True\n","Step 600: Loss: 0.1471, requires_grad: True\n","Step 600: Gradients computed successfully\n","Step 610: Loss: 0.2721, requires_grad: True\n","Step 620: Loss: 0.2333, requires_grad: True\n","Step 630: Loss: 0.1502, requires_grad: True\n","Step 640: Loss: 0.0937, requires_grad: True\n","Step 650: Loss: 0.1674, requires_grad: True\n","Step 650: Gradients computed successfully\n","Step 660: Loss: 0.1374, requires_grad: True\n","Step 670: Loss: 0.1647, requires_grad: True\n","Step 680: Loss: 0.1886, requires_grad: True\n","Step 690: Loss: 0.4996, requires_grad: True\n","Step 700: Loss: 0.0766, requires_grad: True\n","Step 700: Gradients computed successfully\n","Step 710: Loss: 0.1120, requires_grad: True\n","Step 720: Loss: 0.3002, requires_grad: True\n","Step 730: Loss: 0.5016, requires_grad: True\n","Step 740: Loss: 0.2819, requires_grad: True\n","Step 750: Loss: 0.1899, requires_grad: True\n","Step 750: Gradients computed successfully\n","Step 760: Loss: 0.1209, requires_grad: True\n","Step 770: Loss: 0.1987, requires_grad: True\n","Step 780: Loss: 0.0626, requires_grad: True\n","Step 790: Loss: 0.1518, requires_grad: True\n","Step 800: Loss: 0.3292, requires_grad: True\n","Step 800: Gradients computed successfully\n","Step 810: Loss: 0.1623, requires_grad: True\n","Step 820: Loss: 0.3662, requires_grad: True\n","Step 830: Loss: 0.1436, requires_grad: True\n","Step 840: Loss: 0.1949, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-4500\n","Step 850: Loss: 0.2291, requires_grad: True\n","Step 850: Gradients computed successfully\n","Step 860: Loss: 0.2083, requires_grad: True\n","Step 870: Loss: 0.0857, requires_grad: True\n","Step 880: Loss: 0.1194, requires_grad: True\n","Step 890: Loss: 0.4327, requires_grad: True\n","Step 900: Loss: 0.1269, requires_grad: True\n","Step 900: Gradients computed successfully\n","Step 910: Loss: 0.2903, requires_grad: True\n","Step 920: Loss: 0.0646, requires_grad: True\n","Step 930: Loss: 0.2427, requires_grad: True\n","Step 940: Loss: 0.0611, requires_grad: True\n","Step 950: Loss: 0.2286, requires_grad: True\n","Step 950: Gradients computed successfully\n","Step 960: Loss: 0.1677, requires_grad: True\n","Step 970: Loss: 0.1057, requires_grad: True\n","Step 980: Loss: 0.2690, requires_grad: True\n","Step 990: Loss: 0.2105, requires_grad: True\n","Step 1000: Loss: 0.1948, requires_grad: True\n","Step 1000: Gradients computed successfully\n","Step 1010: Loss: 0.1042, requires_grad: True\n","Step 1020: Loss: 0.1836, requires_grad: True\n","Step 1030: Loss: 0.1678, requires_grad: True\n","Step 1040: Loss: 0.1095, requires_grad: True\n","Step 1050: Loss: 0.2365, requires_grad: True\n","Step 1050: Gradients computed successfully\n","Step 1060: Loss: 0.2708, requires_grad: True\n","Step 1070: Loss: 0.1209, requires_grad: True\n","Step 1080: Loss: 0.1350, requires_grad: True\n","Step 1090: Loss: 0.1517, requires_grad: True\n","Step 1100: Loss: 0.5161, requires_grad: True\n","Step 1100: Gradients computed successfully\n","Step 1110: Loss: 0.0606, requires_grad: True\n","Step 1120: Loss: 0.3468, requires_grad: True\n","Step 1130: Loss: 0.1515, requires_grad: True\n","Step 1140: Loss: 0.2262, requires_grad: True\n","Step 1150: Loss: 0.0979, requires_grad: True\n","Step 1150: Gradients computed successfully\n","Step 1160: Loss: 0.4377, requires_grad: True\n","Step 1170: Loss: 0.1538, requires_grad: True\n","Step 1180: Loss: 0.1589, requires_grad: True\n","Step 1190: Loss: 0.3080, requires_grad: True\n","Step 1200: Loss: 0.0940, requires_grad: True\n","Step 1200: Gradients computed successfully\n","Step 1210: Loss: 0.1957, requires_grad: True\n","Step 1220: Loss: 0.3759, requires_grad: True\n","Step 1230: Loss: 0.1559, requires_grad: True\n","Step 1240: Loss: 0.0956, requires_grad: True\n","Step 1250: Loss: 0.1757, requires_grad: True\n","Step 1250: Gradients computed successfully\n","Step 1260: Loss: 0.0705, requires_grad: True\n","Step 1270: Loss: 0.1419, requires_grad: True\n","Step 1280: Loss: 0.2915, requires_grad: True\n","Step 1290: Loss: 0.2094, requires_grad: True\n","Step 1300: Loss: 0.6921, requires_grad: True\n","Step 1300: Gradients computed successfully\n","Step 1310: Loss: 0.2510, requires_grad: True\n","Step 1320: Loss: 0.1519, requires_grad: True\n","Step 1330: Loss: 0.1445, requires_grad: True\n","Step 1340: Loss: 0.2064, requires_grad: True\n","Step 1350: Loss: 0.1483, requires_grad: True\n","Step 1350: Gradients computed successfully\n","Step 1360: Loss: 0.2534, requires_grad: True\n","Step 1370: Loss: 0.4492, requires_grad: True\n","Step 1380: Loss: 0.5392, requires_grad: True\n","Step 1390: Loss: 0.3112, requires_grad: True\n","Step 1400: Loss: 0.3782, requires_grad: True\n","Step 1400: Gradients computed successfully\n","Step 1410: Loss: 0.4655, requires_grad: True\n","Step 1420: Loss: 0.1600, requires_grad: True\n","Step 1430: Loss: 0.2061, requires_grad: True\n","Step 1440: Loss: 0.1993, requires_grad: True\n","Step 1450: Loss: 0.2953, requires_grad: True\n","Step 1450: Gradients computed successfully\n","Step 1460: Loss: 0.2002, requires_grad: True\n","Step 1470: Loss: 0.2654, requires_grad: True\n","Step 1480: Loss: 0.0791, requires_grad: True\n","Step 1490: Loss: 0.2180, requires_grad: True\n","Step 1500: Loss: 0.3709, requires_grad: True\n","Step 1500: Gradients computed successfully\n","Step 1510: Loss: 0.1989, requires_grad: True\n","Step 1520: Loss: 0.0859, requires_grad: True\n","Step 1530: Loss: 0.5466, requires_grad: True\n","Step 1540: Loss: 0.2160, requires_grad: True\n","Step 1550: Loss: 0.1077, requires_grad: True\n","Step 1550: Gradients computed successfully\n","Step 1560: Loss: 0.1427, requires_grad: True\n","Step 1570: Loss: 0.3555, requires_grad: True\n","Step 1580: Loss: 0.3056, requires_grad: True\n","Step 1590: Loss: 0.5119, requires_grad: True\n","Step 1600: Loss: 0.1917, requires_grad: True\n","Step 1600: Gradients computed successfully\n","Step 1610: Loss: 0.1555, requires_grad: True\n","Step 1620: Loss: 0.6114, requires_grad: True\n","Step 1630: Loss: 0.1391, requires_grad: True\n","Step 1640: Loss: 0.4042, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-4600\n","Step 1650: Loss: 0.0737, requires_grad: True\n","Step 1650: Gradients computed successfully\n","Step 1660: Loss: 0.1564, requires_grad: True\n","Step 1670: Loss: 0.1916, requires_grad: True\n","Step 1680: Loss: 0.2753, requires_grad: True\n","Step 1690: Loss: 0.1259, requires_grad: True\n","Step 1700: Loss: 0.2160, requires_grad: True\n","Step 1700: Gradients computed successfully\n","Step 1710: Loss: 0.3130, requires_grad: True\n","Step 1720: Loss: 0.2175, requires_grad: True\n","Step 1730: Loss: 0.1557, requires_grad: True\n","Step 1740: Loss: 0.1572, requires_grad: True\n","Step 1750: Loss: 0.2462, requires_grad: True\n","Step 1750: Gradients computed successfully\n","Step 1760: Loss: 0.3129, requires_grad: True\n","Step 1770: Loss: 0.2125, requires_grad: True\n","Step 1780: Loss: 0.2873, requires_grad: True\n","Step 1790: Loss: 0.1275, requires_grad: True\n","Step 1800: Loss: 0.8826, requires_grad: True\n","Step 1800: Gradients computed successfully\n","Step 1810: Loss: 0.1565, requires_grad: True\n","Step 1820: Loss: 0.3563, requires_grad: True\n","Step 1830: Loss: 0.1204, requires_grad: True\n","Step 1840: Loss: 0.5151, requires_grad: True\n","Step 1850: Loss: 0.2162, requires_grad: True\n","Step 1850: Gradients computed successfully\n","Step 1860: Loss: 0.1497, requires_grad: True\n","Step 1870: Loss: 0.3051, requires_grad: True\n","Step 1880: Loss: 0.1531, requires_grad: True\n","Step 1890: Loss: 0.2274, requires_grad: True\n","Step 1900: Loss: 0.3003, requires_grad: True\n","Step 1900: Gradients computed successfully\n","Step 1910: Loss: 0.4137, requires_grad: True\n","Step 1920: Loss: 0.3923, requires_grad: True\n","Step 1930: Loss: 0.1039, requires_grad: True\n","Step 1940: Loss: 0.2775, requires_grad: True\n","Step 1950: Loss: 0.1934, requires_grad: True\n","Step 1950: Gradients computed successfully\n","Step 1960: Loss: 0.1528, requires_grad: True\n","Step 1970: Loss: 0.1151, requires_grad: True\n","Step 1980: Loss: 0.2749, requires_grad: True\n","Step 1990: Loss: 0.3381, requires_grad: True\n","Step 2000: Loss: 0.4587, requires_grad: True\n","Step 2000: Gradients computed successfully\n","Step 2010: Loss: 0.1315, requires_grad: True\n","Step 2020: Loss: 0.1844, requires_grad: True\n","Step 2030: Loss: 0.5343, requires_grad: True\n","Step 2040: Loss: 0.0606, requires_grad: True\n","Step 2050: Loss: 0.1202, requires_grad: True\n","Step 2050: Gradients computed successfully\n","Step 2060: Loss: 0.1011, requires_grad: True\n","Step 2070: Loss: 0.5204, requires_grad: True\n","Step 2080: Loss: 0.1258, requires_grad: True\n","Step 2090: Loss: 0.0903, requires_grad: True\n","Step 2100: Loss: 0.2333, requires_grad: True\n","Step 2100: Gradients computed successfully\n","Step 2110: Loss: 0.2936, requires_grad: True\n","Step 2120: Loss: 0.2647, requires_grad: True\n","Step 2130: Loss: 0.2650, requires_grad: True\n","Step 2140: Loss: 0.2810, requires_grad: True\n","Step 2150: Loss: 0.2098, requires_grad: True\n","Step 2150: Gradients computed successfully\n","Step 2160: Loss: 0.3666, requires_grad: True\n","Step 2170: Loss: 0.3149, requires_grad: True\n","Step 2180: Loss: 0.5037, requires_grad: True\n","Step 2190: Loss: 0.2931, requires_grad: True\n","Step 2200: Loss: 0.2292, requires_grad: True\n","Step 2200: Gradients computed successfully\n","Step 2210: Loss: 0.1390, requires_grad: True\n","Step 2220: Loss: 0.3889, requires_grad: True\n","Step 2230: Loss: 0.3075, requires_grad: True\n","Step 2240: Loss: 0.1145, requires_grad: True\n","Step 2250: Loss: 0.0619, requires_grad: True\n","Step 2250: Gradients computed successfully\n","Step 2260: Loss: 0.1687, requires_grad: True\n","Step 2270: Loss: 0.1813, requires_grad: True\n","Step 2280: Loss: 0.2814, requires_grad: True\n","Step 2290: Loss: 0.3528, requires_grad: True\n","Step 2300: Loss: 0.1891, requires_grad: True\n","Step 2300: Gradients computed successfully\n","Step 2310: Loss: 0.1037, requires_grad: True\n","Step 2320: Loss: 0.2736, requires_grad: True\n","Step 2330: Loss: 0.1804, requires_grad: True\n","Step 2340: Loss: 0.2766, requires_grad: True\n","Step 2350: Loss: 0.3014, requires_grad: True\n","Step 2350: Gradients computed successfully\n","Step 2360: Loss: 0.3353, requires_grad: True\n","Step 2370: Loss: 0.1313, requires_grad: True\n","Step 2380: Loss: 0.1791, requires_grad: True\n","Step 2390: Loss: 0.1461, requires_grad: True\n","Step 2400: Loss: 0.1119, requires_grad: True\n","Step 2400: Gradients computed successfully\n","Step 2410: Loss: 0.2383, requires_grad: True\n","Step 2420: Loss: 0.2035, requires_grad: True\n","Step 2430: Loss: 0.1250, requires_grad: True\n","Step 2440: Loss: 0.2593, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-4700\n","Step 2450: Loss: 0.2482, requires_grad: True\n","Step 2450: Gradients computed successfully\n","Step 2460: Loss: 0.1271, requires_grad: True\n","Step 2470: Loss: 0.2569, requires_grad: True\n","Step 2480: Loss: 0.2971, requires_grad: True\n","Step 2490: Loss: 0.1496, requires_grad: True\n","Step 2500: Loss: 0.3925, requires_grad: True\n","Step 2500: Gradients computed successfully\n","Step 2510: Loss: 0.1909, requires_grad: True\n","Step 2520: Loss: 0.0668, requires_grad: True\n","Step 2530: Loss: 0.0967, requires_grad: True\n","Step 2540: Loss: 0.2100, requires_grad: True\n","Step 2550: Loss: 0.2694, requires_grad: True\n","Step 2550: Gradients computed successfully\n","Step 2560: Loss: 0.1772, requires_grad: True\n","Step 2570: Loss: 0.3178, requires_grad: True\n","Step 2580: Loss: 0.1127, requires_grad: True\n","Step 2590: Loss: 0.5677, requires_grad: True\n","Step 2600: Loss: 0.1587, requires_grad: True\n","Step 2600: Gradients computed successfully\n","Step 2610: Loss: 0.1340, requires_grad: True\n","Step 2620: Loss: 0.1553, requires_grad: True\n","Step 2630: Loss: 0.1609, requires_grad: True\n","Step 2640: Loss: 0.1361, requires_grad: True\n","Step 2650: Loss: 0.1288, requires_grad: True\n","Step 2650: Gradients computed successfully\n","Step 2660: Loss: 0.2039, requires_grad: True\n","Step 2670: Loss: 0.1049, requires_grad: True\n","Step 2680: Loss: 0.2442, requires_grad: True\n","Step 2690: Loss: 0.1897, requires_grad: True\n","Step 2700: Loss: 0.1442, requires_grad: True\n","Step 2700: Gradients computed successfully\n","Step 2710: Loss: 0.3493, requires_grad: True\n","Step 2720: Loss: 0.0698, requires_grad: True\n","Step 2730: Loss: 0.3287, requires_grad: True\n","Step 2740: Loss: 0.1310, requires_grad: True\n","Step 2750: Loss: 0.2246, requires_grad: True\n","Step 2750: Gradients computed successfully\n","Step 2760: Loss: 0.1898, requires_grad: True\n","Step 2770: Loss: 0.4893, requires_grad: True\n","Step 2780: Loss: 0.3559, requires_grad: True\n","Step 2790: Loss: 0.5139, requires_grad: True\n","Step 2800: Loss: 0.1306, requires_grad: True\n","Step 2800: Gradients computed successfully\n","Step 2810: Loss: 0.1927, requires_grad: True\n","Step 2820: Loss: 0.0740, requires_grad: True\n","Step 2830: Loss: 0.4131, requires_grad: True\n","Step 2840: Loss: 0.2636, requires_grad: True\n","Step 2850: Loss: 0.1778, requires_grad: True\n","Step 2850: Gradients computed successfully\n","Step 2860: Loss: 0.1186, requires_grad: True\n","Step 2870: Loss: 0.2125, requires_grad: True\n","Step 2880: Loss: 0.2219, requires_grad: True\n","Step 2890: Loss: 0.0669, requires_grad: True\n","Step 2900: Loss: 0.1598, requires_grad: True\n","Step 2900: Gradients computed successfully\n","Step 2910: Loss: 0.2398, requires_grad: True\n","Step 2920: Loss: 0.1455, requires_grad: True\n","Step 2930: Loss: 0.5170, requires_grad: True\n","Step 2940: Loss: 0.1289, requires_grad: True\n","Step 2950: Loss: 0.2962, requires_grad: True\n","Step 2950: Gradients computed successfully\n","Step 2960: Loss: 0.1849, requires_grad: True\n","Step 2970: Loss: 0.2340, requires_grad: True\n","Step 2980: Loss: 0.2217, requires_grad: True\n","Step 2990: Loss: 0.1992, requires_grad: True\n","Step 3000: Loss: 0.2106, requires_grad: True\n","Step 3000: Gradients computed successfully\n","Step 3010: Loss: 0.3067, requires_grad: True\n","Step 3020: Loss: 0.1508, requires_grad: True\n","Step 3030: Loss: 0.2525, requires_grad: True\n","Step 3040: Loss: 0.3988, requires_grad: True\n","Step 3050: Loss: 0.1357, requires_grad: True\n","Step 3050: Gradients computed successfully\n","Step 3060: Loss: 0.0970, requires_grad: True\n","Step 3070: Loss: 0.3141, requires_grad: True\n","Step 3080: Loss: 0.1568, requires_grad: True\n","Step 3090: Loss: 0.4180, requires_grad: True\n","Step 3100: Loss: 0.1479, requires_grad: True\n","Step 3100: Gradients computed successfully\n","Step 3110: Loss: 0.2677, requires_grad: True\n","Step 3120: Loss: 0.1289, requires_grad: True\n","Step 3130: Loss: 0.1065, requires_grad: True\n","Step 3140: Loss: 0.0976, requires_grad: True\n","Step 3150: Loss: 0.4752, requires_grad: True\n","Step 3150: Gradients computed successfully\n","Step 3160: Loss: 0.4684, requires_grad: True\n","Step 3170: Loss: 0.3690, requires_grad: True\n","Step 3180: Loss: 0.2086, requires_grad: True\n","Step 3190: Loss: 0.1041, requires_grad: True\n","Step 3200: Loss: 0.2100, requires_grad: True\n","Step 3200: Gradients computed successfully\n","Step 3210: Loss: 0.2998, requires_grad: True\n","Step 3220: Loss: 0.1050, requires_grad: True\n","Step 3230: Loss: 0.4441, requires_grad: True\n","Step 3240: Loss: 0.2609, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-4800\n","Step 3250: Loss: 0.1849, requires_grad: True\n","Step 3250: Gradients computed successfully\n","Step 3260: Loss: 0.2913, requires_grad: True\n","Step 3270: Loss: 0.2576, requires_grad: True\n","Step 3280: Loss: 0.1715, requires_grad: True\n","Step 3290: Loss: 0.2498, requires_grad: True\n","Step 3300: Loss: 0.4185, requires_grad: True\n","Step 3300: Gradients computed successfully\n","Step 3310: Loss: 0.1859, requires_grad: True\n","Step 3320: Loss: 0.2906, requires_grad: True\n","Step 3330: Loss: 0.0714, requires_grad: True\n","Step 3340: Loss: 0.3782, requires_grad: True\n","Step 3350: Loss: 0.3062, requires_grad: True\n","Step 3350: Gradients computed successfully\n","Step 3360: Loss: 0.2330, requires_grad: True\n","Step 3370: Loss: 0.2756, requires_grad: True\n","Step 3380: Loss: 0.1902, requires_grad: True\n","Step 3390: Loss: 0.1701, requires_grad: True\n","Step 3400: Loss: 0.2532, requires_grad: True\n","Step 3400: Gradients computed successfully\n","Step 3410: Loss: 0.4323, requires_grad: True\n","Step 3420: Loss: 0.5289, requires_grad: True\n","Step 3430: Loss: 0.1048, requires_grad: True\n","Step 3440: Loss: 0.1623, requires_grad: True\n","Step 3450: Loss: 0.2387, requires_grad: True\n","Step 3450: Gradients computed successfully\n","Step 3460: Loss: 0.1690, requires_grad: True\n","Step 3470: Loss: 0.4748, requires_grad: True\n","Step 3480: Loss: 0.5685, requires_grad: True\n","Step 3490: Loss: 0.2138, requires_grad: True\n","Step 3500: Loss: 0.1376, requires_grad: True\n","Step 3500: Gradients computed successfully\n","Step 3510: Loss: 0.1550, requires_grad: True\n","Step 3520: Loss: 0.1873, requires_grad: True\n","Step 3530: Loss: 0.1858, requires_grad: True\n","Step 3540: Loss: 0.2766, requires_grad: True\n","Step 3550: Loss: 0.1499, requires_grad: True\n","Step 3550: Gradients computed successfully\n","Step 3560: Loss: 0.1986, requires_grad: True\n","Step 3570: Loss: 0.4215, requires_grad: True\n","Step 3580: Loss: 0.1583, requires_grad: True\n","Step 3590: Loss: 0.2337, requires_grad: True\n","Step 3600: Loss: 0.1890, requires_grad: True\n","Step 3600: Gradients computed successfully\n","Step 3610: Loss: 0.2143, requires_grad: True\n","Step 3620: Loss: 0.3398, requires_grad: True\n","Step 3630: Loss: 0.2924, requires_grad: True\n","Step 3640: Loss: 0.0890, requires_grad: True\n","Step 3650: Loss: 0.0657, requires_grad: True\n","Step 3650: Gradients computed successfully\n","Step 3660: Loss: 0.2862, requires_grad: True\n","Step 3670: Loss: 0.0918, requires_grad: True\n","Step 3680: Loss: 0.1771, requires_grad: True\n","Step 3690: Loss: 0.1005, requires_grad: True\n","Step 3700: Loss: 0.3328, requires_grad: True\n","Step 3700: Gradients computed successfully\n","Step 3710: Loss: 0.1184, requires_grad: True\n","Step 3720: Loss: 0.2563, requires_grad: True\n","Step 3730: Loss: 0.4062, requires_grad: True\n","Step 3740: Loss: 0.1260, requires_grad: True\n","Step 3750: Loss: 0.1063, requires_grad: True\n","Step 3750: Gradients computed successfully\n","Step 3760: Loss: 0.1090, requires_grad: True\n","Step 3770: Loss: 0.1189, requires_grad: True\n","Step 3780: Loss: 0.4053, requires_grad: True\n","Step 3790: Loss: 0.1758, requires_grad: True\n","Step 3800: Loss: 0.1509, requires_grad: True\n","Step 3800: Gradients computed successfully\n","Step 3810: Loss: 0.1406, requires_grad: True\n","Step 3820: Loss: 0.2563, requires_grad: True\n","Step 3830: Loss: 0.1914, requires_grad: True\n","Step 3840: Loss: 0.1911, requires_grad: True\n","Step 3850: Loss: 0.1437, requires_grad: True\n","Step 3850: Gradients computed successfully\n","Step 3860: Loss: 0.4037, requires_grad: True\n","Step 3870: Loss: 0.1249, requires_grad: True\n","Step 3880: Loss: 0.1900, requires_grad: True\n","Step 3890: Loss: 0.1428, requires_grad: True\n","Step 3900: Loss: 0.0558, requires_grad: True\n","Step 3900: Gradients computed successfully\n","Step 3910: Loss: 0.0985, requires_grad: True\n","Step 3920: Loss: 0.2346, requires_grad: True\n","Step 3930: Loss: 0.6054, requires_grad: True\n","Step 3940: Loss: 0.2574, requires_grad: True\n","Step 3950: Loss: 0.1740, requires_grad: True\n","Step 3950: Gradients computed successfully\n","Step 3960: Loss: 0.1062, requires_grad: True\n","Step 3970: Loss: 0.1373, requires_grad: True\n","Step 3980: Loss: 0.1498, requires_grad: True\n","Step 3990: Loss: 0.1530, requires_grad: True\n","Step 4000: Loss: 0.1117, requires_grad: True\n","Step 4000: Gradients computed successfully\n","Step 4010: Loss: 0.0697, requires_grad: True\n","Step 4020: Loss: 0.1650, requires_grad: True\n","Step 4030: Loss: 0.2438, requires_grad: True\n","Step 4040: Loss: 0.1643, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-4900\n","Step 4050: Loss: 0.1641, requires_grad: True\n","Step 4050: Gradients computed successfully\n","Step 4060: Loss: 0.3928, requires_grad: True\n","Step 4070: Loss: 0.1860, requires_grad: True\n","Step 4080: Loss: 0.2549, requires_grad: True\n","Step 4090: Loss: 0.1551, requires_grad: True\n","Step 4100: Loss: 0.1109, requires_grad: True\n","Step 4100: Gradients computed successfully\n","Step 4110: Loss: 0.3480, requires_grad: True\n","Step 4120: Loss: 0.1113, requires_grad: True\n","Step 4130: Loss: 0.2138, requires_grad: True\n","Step 4140: Loss: 0.1576, requires_grad: True\n","Step 4150: Loss: 0.1433, requires_grad: True\n","Step 4150: Gradients computed successfully\n","Step 4160: Loss: 0.0821, requires_grad: True\n","Step 4170: Loss: 0.1370, requires_grad: True\n","Step 4180: Loss: 0.2337, requires_grad: True\n","Step 4190: Loss: 0.4077, requires_grad: True\n","Step 4200: Loss: 0.2620, requires_grad: True\n","Step 4200: Gradients computed successfully\n","Step 4210: Loss: 0.1655, requires_grad: True\n","Step 4220: Loss: 0.1200, requires_grad: True\n","Step 4230: Loss: 0.5157, requires_grad: True\n","Step 4240: Loss: 0.1899, requires_grad: True\n","Step 4250: Loss: 0.4298, requires_grad: True\n","Step 4250: Gradients computed successfully\n","Step 4260: Loss: 0.1063, requires_grad: True\n","Step 4270: Loss: 0.1478, requires_grad: True\n","Step 4280: Loss: 0.2167, requires_grad: True\n","Step 4290: Loss: 0.0724, requires_grad: True\n","Step 4300: Loss: 0.0908, requires_grad: True\n","Step 4300: Gradients computed successfully\n","Step 4310: Loss: 0.2710, requires_grad: True\n","Step 4320: Loss: 0.1997, requires_grad: True\n","Step 4330: Loss: 0.1459, requires_grad: True\n","Step 4340: Loss: 0.5104, requires_grad: True\n","Step 4350: Loss: 0.2580, requires_grad: True\n","Step 4350: Gradients computed successfully\n","Step 4360: Loss: 0.2474, requires_grad: True\n","Step 4370: Loss: 0.0529, requires_grad: True\n","Step 4380: Loss: 0.0827, requires_grad: True\n","Step 4390: Loss: 0.0607, requires_grad: True\n","Step 4400: Loss: 0.3201, requires_grad: True\n","Step 4400: Gradients computed successfully\n","Step 4410: Loss: 0.2666, requires_grad: True\n","Step 4420: Loss: 0.1138, requires_grad: True\n","Step 4430: Loss: 0.0893, requires_grad: True\n","Step 4440: Loss: 0.2459, requires_grad: True\n","Step 4450: Loss: 0.0904, requires_grad: True\n","Step 4450: Gradients computed successfully\n","Step 4460: Loss: 0.1678, requires_grad: True\n","Step 4470: Loss: 0.2188, requires_grad: True\n","Step 4480: Loss: 0.3491, requires_grad: True\n","Step 4490: Loss: 0.2013, requires_grad: True\n","Step 4500: Loss: 0.0496, requires_grad: True\n","Step 4500: Gradients computed successfully\n","Step 4510: Loss: 0.1144, requires_grad: True\n","Step 4520: Loss: 0.1292, requires_grad: True\n","Step 4530: Loss: 0.1970, requires_grad: True\n","Step 4540: Loss: 0.2937, requires_grad: True\n","Step 4550: Loss: 0.2325, requires_grad: True\n","Step 4550: Gradients computed successfully\n","Step 4560: Loss: 0.1799, requires_grad: True\n","Step 4570: Loss: 0.3267, requires_grad: True\n","Step 4580: Loss: 0.4038, requires_grad: True\n","Step 4590: Loss: 0.3105, requires_grad: True\n","Step 4600: Loss: 0.0884, requires_grad: True\n","Step 4600: Gradients computed successfully\n","Step 4610: Loss: 0.1679, requires_grad: True\n","Step 4620: Loss: 0.1395, requires_grad: True\n","Step 4630: Loss: 0.2200, requires_grad: True\n","Step 4640: Loss: 0.1257, requires_grad: True\n","Step 4650: Loss: 0.4258, requires_grad: True\n","Step 4650: Gradients computed successfully\n","Step 4660: Loss: 0.3038, requires_grad: True\n","Step 4670: Loss: 0.2539, requires_grad: True\n","Step 4680: Loss: 0.1559, requires_grad: True\n","Step 4690: Loss: 0.2030, requires_grad: True\n","Step 4700: Loss: 0.0829, requires_grad: True\n","Step 4700: Gradients computed successfully\n","Step 4710: Loss: 0.4005, requires_grad: True\n","Step 4720: Loss: 0.1241, requires_grad: True\n","Step 4730: Loss: 0.2831, requires_grad: True\n","Step 4740: Loss: 0.3160, requires_grad: True\n","Step 4750: Loss: 0.1444, requires_grad: True\n","Step 4750: Gradients computed successfully\n","Step 4760: Loss: 0.0988, requires_grad: True\n","Step 4770: Loss: 0.1223, requires_grad: True\n","Step 4780: Loss: 0.1813, requires_grad: True\n","Step 4790: Loss: 0.2184, requires_grad: True\n","Step 4800: Loss: 0.1424, requires_grad: True\n","Step 4800: Gradients computed successfully\n","Step 4810: Loss: 0.1527, requires_grad: True\n","Step 4820: Loss: 0.3241, requires_grad: True\n","Step 4830: Loss: 0.1346, requires_grad: True\n","Step 4840: Loss: 0.3149, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-5000\n","Step 4850: Loss: 0.1266, requires_grad: True\n","Step 4850: Gradients computed successfully\n","Step 4860: Loss: 0.2956, requires_grad: True\n","Step 4870: Loss: 0.0661, requires_grad: True\n","Step 4880: Loss: 0.1486, requires_grad: True\n","Step 4890: Loss: 0.1238, requires_grad: True\n","Step 4900: Loss: 0.1438, requires_grad: True\n","Step 4900: Gradients computed successfully\n","Step 4910: Loss: 0.1262, requires_grad: True\n","Step 4920: Loss: 0.0858, requires_grad: True\n","Step 4930: Loss: 0.3274, requires_grad: True\n","Step 4940: Loss: 0.1326, requires_grad: True\n","Step 4950: Loss: 0.2220, requires_grad: True\n","Step 4950: Gradients computed successfully\n","Step 4960: Loss: 0.1897, requires_grad: True\n","Step 4970: Loss: 0.0727, requires_grad: True\n","Step 4980: Loss: 0.3316, requires_grad: True\n","Step 4990: Loss: 0.1590, requires_grad: True\n","Step 5000: Loss: 0.0609, requires_grad: True\n","Step 5000: Gradients computed successfully\n","Step 5010: Loss: 0.3019, requires_grad: True\n","Step 5020: Loss: 0.4493, requires_grad: True\n","Step 5030: Loss: 0.1517, requires_grad: True\n","Step 5040: Loss: 0.2053, requires_grad: True\n","Step 5050: Loss: 0.3192, requires_grad: True\n","Step 5050: Gradients computed successfully\n","Step 5060: Loss: 0.3446, requires_grad: True\n","Step 5070: Loss: 0.2481, requires_grad: True\n","Step 5080: Loss: 0.1960, requires_grad: True\n","Step 5090: Loss: 0.3022, requires_grad: True\n","Step 5100: Loss: 0.1698, requires_grad: True\n","Step 5100: Gradients computed successfully\n","Step 5110: Loss: 0.2449, requires_grad: True\n","Step 5120: Loss: 0.1901, requires_grad: True\n","Step 5130: Loss: 0.0818, requires_grad: True\n","Step 5140: Loss: 0.2607, requires_grad: True\n","Step 5150: Loss: 0.0999, requires_grad: True\n","Step 5150: Gradients computed successfully\n","Step 5160: Loss: 0.1045, requires_grad: True\n","Step 5170: Loss: 0.1543, requires_grad: True\n","Step 5180: Loss: 0.2752, requires_grad: True\n","Step 5190: Loss: 0.2348, requires_grad: True\n","Step 5200: Loss: 0.2402, requires_grad: True\n","Step 5200: Gradients computed successfully\n","Step 5210: Loss: 0.1362, requires_grad: True\n","Step 5220: Loss: 0.1945, requires_grad: True\n","Step 5230: Loss: 0.2783, requires_grad: True\n","Step 5240: Loss: 0.2186, requires_grad: True\n","Step 5250: Loss: 0.2576, requires_grad: True\n","Step 5250: Gradients computed successfully\n","Step 5260: Loss: 0.2829, requires_grad: True\n","Step 5270: Loss: 0.0769, requires_grad: True\n","Step 5280: Loss: 0.1663, requires_grad: True\n","Step 5290: Loss: 0.3194, requires_grad: True\n","Step 5300: Loss: 0.1235, requires_grad: True\n","Step 5300: Gradients computed successfully\n","Step 5310: Loss: 0.2165, requires_grad: True\n","Step 5320: Loss: 0.1886, requires_grad: True\n","Step 5330: Loss: 0.1079, requires_grad: True\n","Step 5340: Loss: 0.3354, requires_grad: True\n","Step 5350: Loss: 0.4547, requires_grad: True\n","Step 5350: Gradients computed successfully\n","Step 5360: Loss: 0.2625, requires_grad: True\n","Step 5370: Loss: 0.3545, requires_grad: True\n","Step 5380: Loss: 0.3139, requires_grad: True\n","Step 5390: Loss: 0.1840, requires_grad: True\n","Step 5400: Loss: 0.1247, requires_grad: True\n","Step 5400: Gradients computed successfully\n","Step 5410: Loss: 0.3543, requires_grad: True\n","Step 5420: Loss: 0.2653, requires_grad: True\n","Step 5430: Loss: 0.2079, requires_grad: True\n","Step 5440: Loss: 0.3058, requires_grad: True\n","Step 5450: Loss: 0.2323, requires_grad: True\n","Step 5450: Gradients computed successfully\n","Step 5460: Loss: 0.1912, requires_grad: True\n","Step 5470: Loss: 0.1842, requires_grad: True\n","Step 5480: Loss: 0.4070, requires_grad: True\n","Step 5490: Loss: 0.1163, requires_grad: True\n","Step 5500: Loss: 0.4135, requires_grad: True\n","Step 5500: Gradients computed successfully\n","Step 5510: Loss: 0.0723, requires_grad: True\n","Step 5520: Loss: 0.3180, requires_grad: True\n","Step 5530: Loss: 0.0962, requires_grad: True\n","Step 5540: Loss: 0.2299, requires_grad: True\n","Step 5550: Loss: 0.2731, requires_grad: True\n","Step 5550: Gradients computed successfully\n","Step 5560: Loss: 0.0882, requires_grad: True\n","Step 5570: Loss: 0.0758, requires_grad: True\n","Step 5580: Loss: 0.1778, requires_grad: True\n","Step 5590: Loss: 0.2174, requires_grad: True\n","Step 5600: Loss: 0.0896, requires_grad: True\n","Step 5600: Gradients computed successfully\n","Step 5610: Loss: 0.1697, requires_grad: True\n","Step 5620: Loss: 0.1445, requires_grad: True\n","Step 5630: Loss: 0.1692, requires_grad: True\n","Step 5640: Loss: 0.2235, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-5100\n","Step 5650: Loss: 0.1789, requires_grad: True\n","Step 5650: Gradients computed successfully\n","Step 5660: Loss: 0.2297, requires_grad: True\n","Step 5670: Loss: 0.3080, requires_grad: True\n","Step 5680: Loss: 0.2698, requires_grad: True\n","Step 5690: Loss: 0.2522, requires_grad: True\n","Step 5700: Loss: 0.2899, requires_grad: True\n","Step 5700: Gradients computed successfully\n","Step 5710: Loss: 0.1444, requires_grad: True\n","Step 5720: Loss: 0.2178, requires_grad: True\n","Step 5730: Loss: 0.2487, requires_grad: True\n","Step 5740: Loss: 0.1030, requires_grad: True\n","Step 5750: Loss: 0.3660, requires_grad: True\n","Step 5750: Gradients computed successfully\n","Step 5760: Loss: 0.1659, requires_grad: True\n","Step 5770: Loss: 0.3810, requires_grad: True\n","Step 5780: Loss: 0.2023, requires_grad: True\n","Step 5790: Loss: 0.2063, requires_grad: True\n","Step 5800: Loss: 0.2605, requires_grad: True\n","Step 5800: Gradients computed successfully\n","Step 5810: Loss: 0.1091, requires_grad: True\n","Step 5820: Loss: 0.2408, requires_grad: True\n","Step 5830: Loss: 0.2313, requires_grad: True\n","Step 5840: Loss: 0.1977, requires_grad: True\n","Step 5850: Loss: 0.4272, requires_grad: True\n","Step 5850: Gradients computed successfully\n","Step 5860: Loss: 0.2290, requires_grad: True\n","Step 5870: Loss: 0.3493, requires_grad: True\n","Step 5880: Loss: 0.1076, requires_grad: True\n","Step 5890: Loss: 0.0683, requires_grad: True\n","Step 5900: Loss: 0.1254, requires_grad: True\n","Step 5900: Gradients computed successfully\n","Step 5910: Loss: 0.2880, requires_grad: True\n","Step 5920: Loss: 0.2263, requires_grad: True\n","Step 5930: Loss: 0.1071, requires_grad: True\n","Step 5940: Loss: 0.3096, requires_grad: True\n","Step 5950: Loss: 0.1305, requires_grad: True\n","Step 5950: Gradients computed successfully\n","Step 5960: Loss: 0.8032, requires_grad: True\n","Step 5970: Loss: 0.2242, requires_grad: True\n","Step 5980: Loss: 0.0637, requires_grad: True\n","Step 5990: Loss: 0.2470, requires_grad: True\n","Step 6000: Loss: 0.2052, requires_grad: True\n","Step 6000: Gradients computed successfully\n","Step 6010: Loss: 0.1110, requires_grad: True\n","Step 6020: Loss: 0.2039, requires_grad: True\n","Step 6030: Loss: 0.0477, requires_grad: True\n","Step 6040: Loss: 0.1622, requires_grad: True\n","Step 6050: Loss: 0.1859, requires_grad: True\n","Step 6050: Gradients computed successfully\n","Step 6060: Loss: 0.3890, requires_grad: True\n","Step 6070: Loss: 0.3572, requires_grad: True\n","Step 6080: Loss: 0.5186, requires_grad: True\n","Step 6090: Loss: 0.2201, requires_grad: True\n","Step 6100: Loss: 0.1439, requires_grad: True\n","Step 6100: Gradients computed successfully\n","Step 6110: Loss: 0.1153, requires_grad: True\n","Step 6120: Loss: 0.0955, requires_grad: True\n","Step 6130: Loss: 0.4158, requires_grad: True\n","Step 6140: Loss: 0.1754, requires_grad: True\n","Step 6150: Loss: 0.2641, requires_grad: True\n","Step 6150: Gradients computed successfully\n","Step 6160: Loss: 0.2576, requires_grad: True\n","Step 6170: Loss: 0.1086, requires_grad: True\n","Step 6180: Loss: 0.1478, requires_grad: True\n","Step 6190: Loss: 0.1624, requires_grad: True\n","Step 6200: Loss: 0.3403, requires_grad: True\n","Step 6200: Gradients computed successfully\n","Step 6210: Loss: 0.1970, requires_grad: True\n","Step 6220: Loss: 0.2043, requires_grad: True\n","Step 6230: Loss: 0.1601, requires_grad: True\n","Step 6240: Loss: 0.1533, requires_grad: True\n","Step 6250: Loss: 0.1089, requires_grad: True\n","Step 6250: Gradients computed successfully\n","Step 6260: Loss: 0.1975, requires_grad: True\n","Step 6270: Loss: 0.1679, requires_grad: True\n","Step 6280: Loss: 0.4252, requires_grad: True\n","Step 6290: Loss: 0.0761, requires_grad: True\n","Step 6300: Loss: 0.0796, requires_grad: True\n","Step 6300: Gradients computed successfully\n","Step 6310: Loss: 0.1911, requires_grad: True\n","Step 6320: Loss: 0.1953, requires_grad: True\n","Step 6330: Loss: 0.1266, requires_grad: True\n","Step 6340: Loss: 0.2005, requires_grad: True\n","Step 6350: Loss: 0.2893, requires_grad: True\n","Step 6350: Gradients computed successfully\n","Step 6360: Loss: 0.4499, requires_grad: True\n","Step 6370: Loss: 0.2031, requires_grad: True\n","Step 6380: Loss: 0.1225, requires_grad: True\n","Step 6390: Loss: 0.3498, requires_grad: True\n","Step 6400: Loss: 0.2973, requires_grad: True\n","Step 6400: Gradients computed successfully\n","Step 6410: Loss: 0.1271, requires_grad: True\n","Step 6420: Loss: 0.0916, requires_grad: True\n","Step 6430: Loss: 0.0713, requires_grad: True\n","Step 6440: Loss: 0.0632, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-5200\n","Step 6450: Loss: 0.4153, requires_grad: True\n","Step 6450: Gradients computed successfully\n","Step 6460: Loss: 0.1288, requires_grad: True\n","Step 6470: Loss: 0.1803, requires_grad: True\n","Step 6480: Loss: 0.0988, requires_grad: True\n","Step 6490: Loss: 0.1857, requires_grad: True\n","Step 6500: Loss: 0.2343, requires_grad: True\n","Step 6500: Gradients computed successfully\n","Step 6510: Loss: 0.1802, requires_grad: True\n","Step 6520: Loss: 0.3489, requires_grad: True\n","Step 6530: Loss: 0.1561, requires_grad: True\n","Step 6540: Loss: 0.2183, requires_grad: True\n","Step 6550: Loss: 0.1919, requires_grad: True\n","Step 6550: Gradients computed successfully\n","Step 6560: Loss: 0.0890, requires_grad: True\n","Step 6570: Loss: 0.2267, requires_grad: True\n","Step 6580: Loss: 0.2724, requires_grad: True\n","Step 6590: Loss: 0.0818, requires_grad: True\n","Step 6600: Loss: 0.6567, requires_grad: True\n","Step 6600: Gradients computed successfully\n","Step 6610: Loss: 0.1178, requires_grad: True\n","Step 6620: Loss: 0.0827, requires_grad: True\n","Step 6630: Loss: 0.1476, requires_grad: True\n","Step 6640: Loss: 0.3390, requires_grad: True\n","Step 6650: Loss: 0.0530, requires_grad: True\n","Step 6650: Gradients computed successfully\n","Step 6660: Loss: 0.1510, requires_grad: True\n","Step 6670: Loss: 0.1964, requires_grad: True\n","Step 6680: Loss: 0.3389, requires_grad: True\n","Step 6690: Loss: 0.2802, requires_grad: True\n","Step 6700: Loss: 0.1846, requires_grad: True\n","Step 6700: Gradients computed successfully\n","Step 6710: Loss: 0.1949, requires_grad: True\n","Step 6720: Loss: 0.1102, requires_grad: True\n","Step 6730: Loss: 0.3621, requires_grad: True\n","Step 6740: Loss: 0.1688, requires_grad: True\n","Step 6750: Loss: 0.0756, requires_grad: True\n","Step 6750: Gradients computed successfully\n","Step 6760: Loss: 0.1609, requires_grad: True\n","Step 6770: Loss: 0.4737, requires_grad: True\n","Step 6780: Loss: 0.0993, requires_grad: True\n","Step 6790: Loss: 0.2131, requires_grad: True\n","Step 6800: Loss: 0.4040, requires_grad: True\n","Step 6800: Gradients computed successfully\n","Step 6810: Loss: 0.1442, requires_grad: True\n","Step 6820: Loss: 0.0978, requires_grad: True\n","Step 6830: Loss: 0.1571, requires_grad: True\n","Step 6840: Loss: 0.0981, requires_grad: True\n","Step 6850: Loss: 0.2006, requires_grad: True\n","Step 6850: Gradients computed successfully\n","Step 6860: Loss: 0.2286, requires_grad: True\n","Step 6870: Loss: 0.1199, requires_grad: True\n","Step 6880: Loss: 0.1244, requires_grad: True\n","Step 6890: Loss: 0.4942, requires_grad: True\n","Step 6900: Loss: 0.1005, requires_grad: True\n","Step 6900: Gradients computed successfully\n","Step 6910: Loss: 0.4664, requires_grad: True\n","Step 6920: Loss: 0.2119, requires_grad: True\n","Step 6930: Loss: 0.1337, requires_grad: True\n","Step 6940: Loss: 0.4236, requires_grad: True\n","Step 6950: Loss: 0.0981, requires_grad: True\n","Step 6950: Gradients computed successfully\n","Step 6960: Loss: 0.0563, requires_grad: True\n","Step 6970: Loss: 0.1580, requires_grad: True\n","Step 6980: Loss: 0.0680, requires_grad: True\n","Step 6990: Loss: 0.6135, requires_grad: True\n","Step 7000: Loss: 0.1895, requires_grad: True\n","Step 7000: Gradients computed successfully\n","Step 7010: Loss: 0.2038, requires_grad: True\n","Step 7020: Loss: 0.1413, requires_grad: True\n","Step 7030: Loss: 0.1966, requires_grad: True\n","Step 7040: Loss: 0.1361, requires_grad: True\n","Step 7050: Loss: 0.2195, requires_grad: True\n","Step 7050: Gradients computed successfully\n","Step 7060: Loss: 0.1818, requires_grad: True\n","Step 7070: Loss: 0.1322, requires_grad: True\n","Step 7080: Loss: 0.2678, requires_grad: True\n","Step 7090: Loss: 0.2155, requires_grad: True\n","Step 7100: Loss: 0.1267, requires_grad: True\n","Step 7100: Gradients computed successfully\n","Step 7110: Loss: 0.0725, requires_grad: True\n","Step 7120: Loss: 0.2019, requires_grad: True\n","Step 7130: Loss: 0.1443, requires_grad: True\n","Step 7140: Loss: 0.2684, requires_grad: True\n","Step 7150: Loss: 0.1149, requires_grad: True\n","Step 7150: Gradients computed successfully\n","Step 7160: Loss: 0.2332, requires_grad: True\n","Step 7170: Loss: 0.2776, requires_grad: True\n","Step 7180: Loss: 0.3152, requires_grad: True\n","Step 7190: Loss: 0.3705, requires_grad: True\n","Step 7200: Loss: 0.4465, requires_grad: True\n","Step 7200: Gradients computed successfully\n","Step 7210: Loss: 0.1321, requires_grad: True\n","Step 7220: Loss: 0.2489, requires_grad: True\n","Step 7230: Loss: 0.4927, requires_grad: True\n","Step 7240: Loss: 0.1514, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-5300\n","Step 7250: Loss: 0.1700, requires_grad: True\n","Step 7250: Gradients computed successfully\n","Step 7260: Loss: 0.5434, requires_grad: True\n","Step 7270: Loss: 0.2673, requires_grad: True\n","Step 7280: Loss: 0.1594, requires_grad: True\n","Step 7290: Loss: 0.2277, requires_grad: True\n","Step 7300: Loss: 0.1307, requires_grad: True\n","Step 7300: Gradients computed successfully\n","Step 7310: Loss: 0.1333, requires_grad: True\n","Step 7320: Loss: 0.0831, requires_grad: True\n","Step 7330: Loss: 0.2532, requires_grad: True\n","Step 7340: Loss: 0.2095, requires_grad: True\n","Step 7350: Loss: 0.2845, requires_grad: True\n","Step 7350: Gradients computed successfully\n","Step 7360: Loss: 0.1342, requires_grad: True\n","Step 7370: Loss: 0.1503, requires_grad: True\n","Step 7380: Loss: 0.1746, requires_grad: True\n","Step 7390: Loss: 0.2635, requires_grad: True\n","Step 7400: Loss: 0.3181, requires_grad: True\n","Step 7400: Gradients computed successfully\n","Step 7410: Loss: 0.3644, requires_grad: True\n","Step 7420: Loss: 0.2510, requires_grad: True\n","Step 7430: Loss: 0.3320, requires_grad: True\n","Step 7440: Loss: 0.2474, requires_grad: True\n","Step 7450: Loss: 0.4504, requires_grad: True\n","Step 7450: Gradients computed successfully\n","Step 7460: Loss: 0.2135, requires_grad: True\n","Step 7470: Loss: 0.4194, requires_grad: True\n","Step 7480: Loss: 0.4802, requires_grad: True\n","Step 7490: Loss: 0.0649, requires_grad: True\n","Step 7500: Loss: 0.1532, requires_grad: True\n","Step 7500: Gradients computed successfully\n","Step 7510: Loss: 0.1062, requires_grad: True\n","Step 7520: Loss: 0.4239, requires_grad: True\n","Step 7530: Loss: 0.1163, requires_grad: True\n","Step 7540: Loss: 0.3574, requires_grad: True\n","Step 7550: Loss: 0.0816, requires_grad: True\n","Step 7550: Gradients computed successfully\n","Step 7560: Loss: 0.1515, requires_grad: True\n","Step 7570: Loss: 0.2135, requires_grad: True\n","Step 7580: Loss: 0.1213, requires_grad: True\n","Step 7590: Loss: 0.2352, requires_grad: True\n","Step 7600: Loss: 0.1869, requires_grad: True\n","Step 7600: Gradients computed successfully\n","Step 7610: Loss: 0.1380, requires_grad: True\n","Step 7620: Loss: 0.1817, requires_grad: True\n","Step 7630: Loss: 0.1575, requires_grad: True\n","Step 7640: Loss: 0.2366, requires_grad: True\n","Step 7650: Loss: 0.4273, requires_grad: True\n","Step 7650: Gradients computed successfully\n","Step 7660: Loss: 0.1663, requires_grad: True\n","Step 7670: Loss: 0.4314, requires_grad: True\n","Step 7680: Loss: 0.1030, requires_grad: True\n","Step 7690: Loss: 0.5882, requires_grad: True\n","Step 7700: Loss: 0.3619, requires_grad: True\n","Step 7700: Gradients computed successfully\n","Step 7710: Loss: 0.0496, requires_grad: True\n","Step 7720: Loss: 0.0948, requires_grad: True\n","Step 7730: Loss: 0.3654, requires_grad: True\n","Step 7740: Loss: 0.1066, requires_grad: True\n","Step 7750: Loss: 0.0990, requires_grad: True\n","Step 7750: Gradients computed successfully\n","Step 7760: Loss: 0.1038, requires_grad: True\n","Step 7770: Loss: 0.1997, requires_grad: True\n","Step 7780: Loss: 0.2693, requires_grad: True\n","Step 7790: Loss: 0.2352, requires_grad: True\n","Step 7800: Loss: 0.1772, requires_grad: True\n","Step 7800: Gradients computed successfully\n","Step 7810: Loss: 0.1377, requires_grad: True\n","Step 7820: Loss: 0.0706, requires_grad: True\n","Step 7830: Loss: 0.1422, requires_grad: True\n","Step 7840: Loss: 0.1094, requires_grad: True\n","Step 7850: Loss: 0.4560, requires_grad: True\n","Step 7850: Gradients computed successfully\n","Step 7860: Loss: 0.1348, requires_grad: True\n","Step 7870: Loss: 0.1922, requires_grad: True\n","Step 7880: Loss: 0.1185, requires_grad: True\n","Step 7890: Loss: 0.2230, requires_grad: True\n","Step 7900: Loss: 0.1551, requires_grad: True\n","Step 7900: Gradients computed successfully\n","Step 7910: Loss: 0.0678, requires_grad: True\n","Step 7920: Loss: 0.2870, requires_grad: True\n","Step 7930: Loss: 0.1457, requires_grad: True\n","Step 7940: Loss: 0.0952, requires_grad: True\n","Step 7950: Loss: 0.2349, requires_grad: True\n","Step 7950: Gradients computed successfully\n","Step 7960: Loss: 0.0911, requires_grad: True\n","Step 7970: Loss: 0.2041, requires_grad: True\n","Step 7980: Loss: 0.1483, requires_grad: True\n","Step 7990: Loss: 0.1464, requires_grad: True\n","Step 8000: Loss: 0.4759, requires_grad: True\n","Step 8000: Gradients computed successfully\n","Step 8010: Loss: 0.1751, requires_grad: True\n","Step 8020: Loss: 0.2573, requires_grad: True\n","Step 8030: Loss: 0.0896, requires_grad: True\n","Step 8040: Loss: 0.2023, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-5400\n","Step 8050: Loss: 0.1250, requires_grad: True\n","Step 8050: Gradients computed successfully\n","Step 8060: Loss: 0.1175, requires_grad: True\n","Step 8070: Loss: 0.1668, requires_grad: True\n","Step 8080: Loss: 0.1104, requires_grad: True\n","Step 8090: Loss: 0.3384, requires_grad: True\n","Step 8100: Loss: 0.3209, requires_grad: True\n","Step 8100: Gradients computed successfully\n","Step 8110: Loss: 0.7991, requires_grad: True\n","Step 8120: Loss: 0.3039, requires_grad: True\n","Step 8130: Loss: 0.0635, requires_grad: True\n","Step 8140: Loss: 0.1710, requires_grad: True\n","Step 8150: Loss: 0.1730, requires_grad: True\n","Step 8150: Gradients computed successfully\n","Step 8160: Loss: 0.0354, requires_grad: True\n","Step 8170: Loss: 0.1651, requires_grad: True\n","Step 8180: Loss: 0.1238, requires_grad: True\n","Step 8190: Loss: 0.1347, requires_grad: True\n","Step 8200: Loss: 0.0897, requires_grad: True\n","Step 8200: Gradients computed successfully\n","Step 8210: Loss: 0.0519, requires_grad: True\n","Step 8220: Loss: 0.1005, requires_grad: True\n","Step 8230: Loss: 0.1838, requires_grad: True\n","Step 8240: Loss: 0.1184, requires_grad: True\n","Step 8250: Loss: 0.2679, requires_grad: True\n","Step 8250: Gradients computed successfully\n","Step 8260: Loss: 0.0900, requires_grad: True\n","Step 8270: Loss: 0.1487, requires_grad: True\n","Step 8280: Loss: 0.0989, requires_grad: True\n","Step 8290: Loss: 0.1797, requires_grad: True\n","Step 8300: Loss: 0.0922, requires_grad: True\n","Step 8300: Gradients computed successfully\n","Step 8310: Loss: 0.2822, requires_grad: True\n","Step 8320: Loss: 0.1018, requires_grad: True\n","Step 8330: Loss: 0.2510, requires_grad: True\n","Step 8340: Loss: 0.1690, requires_grad: True\n","Step 8350: Loss: 0.1764, requires_grad: True\n","Step 8350: Gradients computed successfully\n","Step 8360: Loss: 0.2666, requires_grad: True\n","Step 8370: Loss: 0.0968, requires_grad: True\n","Step 8380: Loss: 0.4402, requires_grad: True\n","Step 8390: Loss: 0.0838, requires_grad: True\n","Step 8400: Loss: 0.2553, requires_grad: True\n","Step 8400: Gradients computed successfully\n","Step 8410: Loss: 0.1941, requires_grad: True\n","Step 8420: Loss: 0.2709, requires_grad: True\n","Step 8430: Loss: 0.1774, requires_grad: True\n","Step 8440: Loss: 0.1585, requires_grad: True\n","Step 8450: Loss: 0.2346, requires_grad: True\n","Step 8450: Gradients computed successfully\n","Step 8460: Loss: 0.1606, requires_grad: True\n","Step 8470: Loss: 0.3373, requires_grad: True\n","Step 8480: Loss: 0.1898, requires_grad: True\n","Step 8490: Loss: 0.1373, requires_grad: True\n","Step 8500: Loss: 0.1308, requires_grad: True\n","Step 8500: Gradients computed successfully\n","Step 8510: Loss: 0.5482, requires_grad: True\n","Step 8520: Loss: 0.0403, requires_grad: True\n","Step 8530: Loss: 0.1439, requires_grad: True\n","Step 8540: Loss: 0.2765, requires_grad: True\n","Step 8550: Loss: 0.1628, requires_grad: True\n","Step 8550: Gradients computed successfully\n","Step 8560: Loss: 0.1290, requires_grad: True\n","Step 8570: Loss: 0.2893, requires_grad: True\n","Step 8580: Loss: 0.0442, requires_grad: True\n","Step 8590: Loss: 0.1220, requires_grad: True\n","Step 8600: Loss: 0.1778, requires_grad: True\n","Step 8600: Gradients computed successfully\n","Step 8610: Loss: 0.1803, requires_grad: True\n","Step 8620: Loss: 0.1446, requires_grad: True\n","Step 8630: Loss: 0.0795, requires_grad: True\n","Step 8640: Loss: 0.1640, requires_grad: True\n","Step 8650: Loss: 0.3746, requires_grad: True\n","Step 8650: Gradients computed successfully\n","Step 8660: Loss: 0.5038, requires_grad: True\n","Step 8670: Loss: 0.0759, requires_grad: True\n","Step 8680: Loss: 0.3576, requires_grad: True\n","Step 8690: Loss: 0.3932, requires_grad: True\n","Step 8700: Loss: 0.2418, requires_grad: True\n","Step 8700: Gradients computed successfully\n","Step 8710: Loss: 0.2777, requires_grad: True\n","Step 8720: Loss: 0.2245, requires_grad: True\n","Step 8730: Loss: 0.0883, requires_grad: True\n","Step 8740: Loss: 0.2118, requires_grad: True\n","Step 8750: Loss: 0.2446, requires_grad: True\n","Step 8750: Gradients computed successfully\n","Step 8760: Loss: 0.1473, requires_grad: True\n","Step 8770: Loss: 0.3366, requires_grad: True\n","Step 8780: Loss: 0.1498, requires_grad: True\n","Step 8790: Loss: 0.2497, requires_grad: True\n","Step 8800: Loss: 0.1900, requires_grad: True\n","Step 8800: Gradients computed successfully\n","Step 8810: Loss: 0.3411, requires_grad: True\n","Step 8820: Loss: 0.3338, requires_grad: True\n","Step 8830: Loss: 0.5973, requires_grad: True\n","Step 8840: Loss: 0.1524, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-5500\n","Step 8850: Loss: 0.1360, requires_grad: True\n","Step 8850: Gradients computed successfully\n","Step 8860: Loss: 0.2165, requires_grad: True\n","Step 8870: Loss: 0.1766, requires_grad: True\n","Step 8880: Loss: 0.6832, requires_grad: True\n","Step 8890: Loss: 0.1273, requires_grad: True\n","Step 8900: Loss: 0.3829, requires_grad: True\n","Step 8900: Gradients computed successfully\n","Step 8910: Loss: 0.1262, requires_grad: True\n","Step 8920: Loss: 0.2317, requires_grad: True\n","Step 8930: Loss: 0.1188, requires_grad: True\n","Step 8940: Loss: 0.1650, requires_grad: True\n","Step 8950: Loss: 0.1037, requires_grad: True\n","Step 8950: Gradients computed successfully\n","Step 8960: Loss: 0.3107, requires_grad: True\n","Step 8970: Loss: 0.2593, requires_grad: True\n","Step 8980: Loss: 0.1108, requires_grad: True\n","Step 8990: Loss: 0.1689, requires_grad: True\n","Step 9000: Loss: 0.3359, requires_grad: True\n","Step 9000: Gradients computed successfully\n","Step 9010: Loss: 0.2047, requires_grad: True\n","Step 9020: Loss: 0.1008, requires_grad: True\n","Step 9030: Loss: 0.1487, requires_grad: True\n","Step 9040: Loss: 0.1753, requires_grad: True\n","Step 9050: Loss: 0.0667, requires_grad: True\n","Step 9050: Gradients computed successfully\n","Step 9060: Loss: 0.1057, requires_grad: True\n","Step 9070: Loss: 0.2005, requires_grad: True\n","Step 9080: Loss: 0.1457, requires_grad: True\n","Step 9090: Loss: 0.1549, requires_grad: True\n","Step 9100: Loss: 0.0980, requires_grad: True\n","Step 9100: Gradients computed successfully\n","Step 9110: Loss: 0.1226, requires_grad: True\n","Step 9120: Loss: 0.0800, requires_grad: True\n","Step 9130: Loss: 0.2543, requires_grad: True\n","Step 9140: Loss: 0.0711, requires_grad: True\n","Step 9150: Loss: 0.1486, requires_grad: True\n","Step 9150: Gradients computed successfully\n","Step 9160: Loss: 0.1056, requires_grad: True\n","Step 9170: Loss: 0.2090, requires_grad: True\n","Step 9180: Loss: 0.1128, requires_grad: True\n","Step 9190: Loss: 0.3408, requires_grad: True\n","Step 9200: Loss: 0.1149, requires_grad: True\n","Step 9200: Gradients computed successfully\n","Step 9210: Loss: 0.3049, requires_grad: True\n","Step 9220: Loss: 0.3750, requires_grad: True\n","Step 9230: Loss: 0.0985, requires_grad: True\n","Step 9240: Loss: 0.0540, requires_grad: True\n","Step 9250: Loss: 0.0794, requires_grad: True\n","Step 9250: Gradients computed successfully\n","Step 9260: Loss: 0.2570, requires_grad: True\n","Step 9270: Loss: 0.3051, requires_grad: True\n","Step 9280: Loss: 0.1053, requires_grad: True\n","Step 9290: Loss: 0.1704, requires_grad: True\n","Step 9300: Loss: 0.1768, requires_grad: True\n","Step 9300: Gradients computed successfully\n","Step 9310: Loss: 0.2211, requires_grad: True\n","Step 9320: Loss: 0.1912, requires_grad: True\n","Step 9330: Loss: 0.1230, requires_grad: True\n","Step 9340: Loss: 0.2169, requires_grad: True\n","Step 9350: Loss: 0.0760, requires_grad: True\n","Step 9350: Gradients computed successfully\n","Step 9360: Loss: 0.1563, requires_grad: True\n","Step 9370: Loss: 0.1134, requires_grad: True\n","Step 9380: Loss: 0.4022, requires_grad: True\n","Step 9390: Loss: 0.2579, requires_grad: True\n","Step 9400: Loss: 0.1296, requires_grad: True\n","Step 9400: Gradients computed successfully\n","Step 9410: Loss: 0.1726, requires_grad: True\n","Step 9420: Loss: 0.3493, requires_grad: True\n","Step 9430: Loss: 0.0813, requires_grad: True\n","Step 9440: Loss: 0.2404, requires_grad: True\n","Step 9450: Loss: 0.0586, requires_grad: True\n","Step 9450: Gradients computed successfully\n","Step 9460: Loss: 0.0963, requires_grad: True\n","Step 9470: Loss: 0.1678, requires_grad: True\n","Step 9480: Loss: 0.1153, requires_grad: True\n","Step 9490: Loss: 0.2090, requires_grad: True\n","Step 9500: Loss: 0.1014, requires_grad: True\n","Step 9500: Gradients computed successfully\n","Step 9510: Loss: 0.1162, requires_grad: True\n","Step 9520: Loss: 0.2541, requires_grad: True\n","Step 9530: Loss: 0.0943, requires_grad: True\n","Step 9540: Loss: 0.1769, requires_grad: True\n","Step 9550: Loss: 0.1288, requires_grad: True\n","Step 9550: Gradients computed successfully\n","Step 9560: Loss: 0.3125, requires_grad: True\n","Step 9570: Loss: 0.2690, requires_grad: True\n","Step 9580: Loss: 0.4193, requires_grad: True\n","Step 9590: Loss: 0.1941, requires_grad: True\n","Step 9600: Loss: 0.2911, requires_grad: True\n","Step 9600: Gradients computed successfully\n","Step 9610: Loss: 0.1924, requires_grad: True\n","Step 9620: Loss: 0.3714, requires_grad: True\n","Step 9630: Loss: 0.1900, requires_grad: True\n","Step 9640: Loss: 0.0953, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-5600\n","Step 9650: Loss: 0.3644, requires_grad: True\n","Step 9650: Gradients computed successfully\n","Step 9660: Loss: 0.2554, requires_grad: True\n","Step 9670: Loss: 0.1831, requires_grad: True\n","Step 9680: Loss: 0.1546, requires_grad: True\n","Step 9690: Loss: 0.1517, requires_grad: True\n","Step 9700: Loss: 0.2185, requires_grad: True\n","Step 9700: Gradients computed successfully\n","Step 9710: Loss: 0.0944, requires_grad: True\n","Step 9720: Loss: 0.0635, requires_grad: True\n","Step 9730: Loss: 0.1288, requires_grad: True\n","Step 9740: Loss: 0.0891, requires_grad: True\n","Step 9750: Loss: 0.1306, requires_grad: True\n","Step 9750: Gradients computed successfully\n","Step 9760: Loss: 0.0986, requires_grad: True\n","Step 9770: Loss: 0.4135, requires_grad: True\n","Step 9780: Loss: 0.4520, requires_grad: True\n","Step 9790: Loss: 0.2726, requires_grad: True\n","Step 9800: Loss: 0.0951, requires_grad: True\n","Step 9800: Gradients computed successfully\n","Step 9810: Loss: 0.1005, requires_grad: True\n","Step 9820: Loss: 0.1823, requires_grad: True\n","Step 9830: Loss: 0.0364, requires_grad: True\n","Step 9840: Loss: 0.3043, requires_grad: True\n","Step 9850: Loss: 0.0824, requires_grad: True\n","Step 9850: Gradients computed successfully\n","Step 9860: Loss: 0.0635, requires_grad: True\n","Step 9870: Loss: 0.0646, requires_grad: True\n","Step 9880: Loss: 0.1105, requires_grad: True\n","Step 9890: Loss: 0.3199, requires_grad: True\n","Step 9900: Loss: 0.1074, requires_grad: True\n","Step 9900: Gradients computed successfully\n","Step 9910: Loss: 0.5350, requires_grad: True\n","Step 9920: Loss: 0.1267, requires_grad: True\n","Step 9930: Loss: 0.4197, requires_grad: True\n","Step 9940: Loss: 0.3220, requires_grad: True\n","Step 9950: Loss: 0.1602, requires_grad: True\n","Step 9950: Gradients computed successfully\n","Step 9960: Loss: 0.1687, requires_grad: True\n","Step 9970: Loss: 0.0572, requires_grad: True\n","Step 9980: Loss: 0.0704, requires_grad: True\n","Step 9990: Loss: 0.1432, requires_grad: True\n","Step 10000: Loss: 0.3925, requires_grad: True\n","Step 10000: Gradients computed successfully\n","Step 10010: Loss: 0.2225, requires_grad: True\n","Step 10020: Loss: 0.7076, requires_grad: True\n","Step 10030: Loss: 0.3770, requires_grad: True\n","Step 10040: Loss: 0.1510, requires_grad: True\n","Step 10050: Loss: 0.2006, requires_grad: True\n","Step 10050: Gradients computed successfully\n","Step 10060: Loss: 0.2518, requires_grad: True\n","Step 10070: Loss: 0.1332, requires_grad: True\n","Step 10080: Loss: 0.3132, requires_grad: True\n","Step 10090: Loss: 0.0859, requires_grad: True\n","Step 10100: Loss: 0.2427, requires_grad: True\n","Step 10100: Gradients computed successfully\n","Step 10110: Loss: 0.1933, requires_grad: True\n","Step 10120: Loss: 0.2010, requires_grad: True\n","Step 10130: Loss: 0.0655, requires_grad: True\n","Step 10140: Loss: 0.3770, requires_grad: True\n","Step 10150: Loss: 0.3215, requires_grad: True\n","Step 10150: Gradients computed successfully\n","Step 10160: Loss: 0.3141, requires_grad: True\n","Step 10170: Loss: 0.1127, requires_grad: True\n","Step 10180: Loss: 0.2470, requires_grad: True\n","Step 10190: Loss: 0.3230, requires_grad: True\n","Step 10200: Loss: 0.1951, requires_grad: True\n","Step 10200: Gradients computed successfully\n","Step 10210: Loss: 0.2287, requires_grad: True\n","Step 10220: Loss: 0.2597, requires_grad: True\n","Step 10230: Loss: 0.5170, requires_grad: True\n","Step 10240: Loss: 0.1035, requires_grad: True\n","Step 10250: Loss: 0.1908, requires_grad: True\n","Step 10250: Gradients computed successfully\n","Step 10260: Loss: 0.1992, requires_grad: True\n","Step 10270: Loss: 0.1080, requires_grad: True\n","Step 10280: Loss: 0.2182, requires_grad: True\n","Step 10290: Loss: 0.2954, requires_grad: True\n","Step 10300: Loss: 0.3183, requires_grad: True\n","Step 10300: Gradients computed successfully\n","Step 10310: Loss: 0.1741, requires_grad: True\n","Step 10320: Loss: 0.3138, requires_grad: True\n","Step 10330: Loss: 0.0842, requires_grad: True\n","Step 10340: Loss: 0.2263, requires_grad: True\n","Step 10350: Loss: 0.3931, requires_grad: True\n","Step 10350: Gradients computed successfully\n","Step 10360: Loss: 0.2811, requires_grad: True\n","Step 10370: Loss: 0.1236, requires_grad: True\n","Step 10380: Loss: 0.1205, requires_grad: True\n","Step 10390: Loss: 0.3193, requires_grad: True\n","Step 10400: Loss: 0.0934, requires_grad: True\n","Step 10400: Gradients computed successfully\n","Step 10410: Loss: 0.2146, requires_grad: True\n","Step 10420: Loss: 0.1673, requires_grad: True\n","Step 10430: Loss: 0.1060, requires_grad: True\n","Step 10440: Loss: 0.1492, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-5700\n","Step 10450: Loss: 0.1961, requires_grad: True\n","Step 10450: Gradients computed successfully\n","Step 10460: Loss: 0.1186, requires_grad: True\n","Step 10470: Loss: 0.2857, requires_grad: True\n","Step 10480: Loss: 0.0731, requires_grad: True\n","Step 10490: Loss: 0.1496, requires_grad: True\n","Step 10500: Loss: 0.1275, requires_grad: True\n","Step 10500: Gradients computed successfully\n","Step 10510: Loss: 0.1301, requires_grad: True\n","Step 10520: Loss: 0.0609, requires_grad: True\n","Step 10530: Loss: 0.1369, requires_grad: True\n","Step 10540: Loss: 0.1507, requires_grad: True\n","Step 10550: Loss: 0.4879, requires_grad: True\n","Step 10550: Gradients computed successfully\n","Step 10560: Loss: 0.1306, requires_grad: True\n","Step 10570: Loss: 0.1870, requires_grad: True\n","Step 10580: Loss: 0.2341, requires_grad: True\n","Step 10590: Loss: 0.3150, requires_grad: True\n","Step 10600: Loss: 0.1996, requires_grad: True\n","Step 10600: Gradients computed successfully\n","Step 10610: Loss: 0.0736, requires_grad: True\n","Step 10620: Loss: 0.1253, requires_grad: True\n","Step 10630: Loss: 0.1813, requires_grad: True\n","Step 10640: Loss: 0.3091, requires_grad: True\n","Step 10650: Loss: 0.3321, requires_grad: True\n","Step 10650: Gradients computed successfully\n","Step 10660: Loss: 0.1970, requires_grad: True\n","Step 10670: Loss: 0.3644, requires_grad: True\n","Step 10680: Loss: 0.4175, requires_grad: True\n","Step 10690: Loss: 0.1210, requires_grad: True\n","Step 10700: Loss: 0.1696, requires_grad: True\n","Step 10700: Gradients computed successfully\n","Step 10710: Loss: 0.1746, requires_grad: True\n","Step 10720: Loss: 0.1820, requires_grad: True\n","Step 10730: Loss: 0.3400, requires_grad: True\n","Step 10740: Loss: 0.2208, requires_grad: True\n","Step 10750: Loss: 0.3036, requires_grad: True\n","Step 10750: Gradients computed successfully\n","Step 10760: Loss: 0.0883, requires_grad: True\n","Step 10770: Loss: 0.2599, requires_grad: True\n","Step 10780: Loss: 0.2425, requires_grad: True\n","Step 10790: Loss: 0.2087, requires_grad: True\n","Step 10800: Loss: 0.2371, requires_grad: True\n","Step 10800: Gradients computed successfully\n","Step 10810: Loss: 0.3853, requires_grad: True\n","Step 10820: Loss: 0.3121, requires_grad: True\n","Step 10830: Loss: 0.0551, requires_grad: True\n","Step 10840: Loss: 0.1986, requires_grad: True\n","Step 10850: Loss: 0.3512, requires_grad: True\n","Step 10850: Gradients computed successfully\n","Step 10860: Loss: 0.1237, requires_grad: True\n","Step 10870: Loss: 0.1187, requires_grad: True\n","Step 10880: Loss: 0.1955, requires_grad: True\n","Step 10890: Loss: 0.0787, requires_grad: True\n","Step 10900: Loss: 0.2661, requires_grad: True\n","Step 10900: Gradients computed successfully\n","Step 10910: Loss: 0.2387, requires_grad: True\n","Step 10920: Loss: 0.2196, requires_grad: True\n","Step 10930: Loss: 0.1783, requires_grad: True\n","Step 10940: Loss: 0.1760, requires_grad: True\n","Step 10950: Loss: 0.1341, requires_grad: True\n","Step 10950: Gradients computed successfully\n","Step 10960: Loss: 0.2522, requires_grad: True\n","Step 10970: Loss: 0.1845, requires_grad: True\n","Step 10980: Loss: 0.9049, requires_grad: True\n","Step 10990: Loss: 0.3720, requires_grad: True\n","Step 11000: Loss: 0.3931, requires_grad: True\n","Step 11000: Gradients computed successfully\n","Step 11010: Loss: 0.3101, requires_grad: True\n","Step 11020: Loss: 0.1257, requires_grad: True\n","Step 11030: Loss: 0.2544, requires_grad: True\n","Step 11040: Loss: 0.3297, requires_grad: True\n","Step 11050: Loss: 0.1145, requires_grad: True\n","Step 11050: Gradients computed successfully\n","Step 11060: Loss: 0.1551, requires_grad: True\n","Step 11070: Loss: 0.0768, requires_grad: True\n","Step 11080: Loss: 0.1869, requires_grad: True\n","Step 11090: Loss: 0.1521, requires_grad: True\n","Step 11100: Loss: 0.2152, requires_grad: True\n","Step 11100: Gradients computed successfully\n","Step 11110: Loss: 0.2054, requires_grad: True\n","Step 11120: Loss: 0.0793, requires_grad: True\n","Step 11130: Loss: 0.2078, requires_grad: True\n","Step 11140: Loss: 0.2581, requires_grad: True\n","Step 11150: Loss: 0.5052, requires_grad: True\n","Step 11150: Gradients computed successfully\n","Step 11160: Loss: 0.0882, requires_grad: True\n","Step 11170: Loss: 0.1062, requires_grad: True\n","Step 11180: Loss: 0.1920, requires_grad: True\n","Step 11190: Loss: 0.2304, requires_grad: True\n","Step 11200: Loss: 0.1038, requires_grad: True\n","Step 11200: Gradients computed successfully\n","Step 11210: Loss: 0.3898, requires_grad: True\n","Step 11220: Loss: 0.1995, requires_grad: True\n","Step 11230: Loss: 0.3223, requires_grad: True\n","Step 11240: Loss: 0.1318, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-5800\n","Step 11250: Loss: 0.5104, requires_grad: True\n","Step 11250: Gradients computed successfully\n","Step 11260: Loss: 0.2415, requires_grad: True\n","Step 11270: Loss: 0.1786, requires_grad: True\n","Step 11280: Loss: 0.2268, requires_grad: True\n","Step 11290: Loss: 0.6489, requires_grad: True\n","Step 11300: Loss: 0.1573, requires_grad: True\n","Step 11300: Gradients computed successfully\n","Step 11310: Loss: 0.5822, requires_grad: True\n","Step 11320: Loss: 0.2893, requires_grad: True\n","Step 11330: Loss: 0.1270, requires_grad: True\n","Step 11340: Loss: 0.2885, requires_grad: True\n","Step 11350: Loss: 0.2334, requires_grad: True\n","Step 11350: Gradients computed successfully\n","Step 11360: Loss: 0.1559, requires_grad: True\n","Step 11370: Loss: 0.3807, requires_grad: True\n","Step 11380: Loss: 0.1738, requires_grad: True\n","Step 11390: Loss: 0.2526, requires_grad: True\n","Step 11400: Loss: 0.1821, requires_grad: True\n","Step 11400: Gradients computed successfully\n","Step 11410: Loss: 0.1416, requires_grad: True\n","Step 11420: Loss: 0.1173, requires_grad: True\n","Step 11430: Loss: 0.1283, requires_grad: True\n","Step 11440: Loss: 0.1758, requires_grad: True\n","Step 11450: Loss: 0.0382, requires_grad: True\n","Step 11450: Gradients computed successfully\n","Step 11460: Loss: 0.1494, requires_grad: True\n","Step 11470: Loss: 0.4354, requires_grad: True\n","Step 11480: Loss: 0.1444, requires_grad: True\n","Step 11490: Loss: 0.0858, requires_grad: True\n","Step 11500: Loss: 0.2934, requires_grad: True\n","Step 11500: Gradients computed successfully\n","Step 11510: Loss: 0.0952, requires_grad: True\n","Step 11520: Loss: 0.0952, requires_grad: True\n","Step 11530: Loss: 0.1580, requires_grad: True\n","Step 11540: Loss: 0.4030, requires_grad: True\n","Step 11550: Loss: 0.1490, requires_grad: True\n","Step 11550: Gradients computed successfully\n","Step 11560: Loss: 0.0502, requires_grad: True\n","Step 11570: Loss: 0.0900, requires_grad: True\n","Step 11580: Loss: 0.3176, requires_grad: True\n","Step 11590: Loss: 0.3442, requires_grad: True\n","Step 11600: Loss: 0.1458, requires_grad: True\n","Step 11600: Gradients computed successfully\n","Step 11610: Loss: 0.5508, requires_grad: True\n","Step 11620: Loss: 0.1115, requires_grad: True\n","Step 11630: Loss: 0.2275, requires_grad: True\n","Step 11640: Loss: 0.3116, requires_grad: True\n","Step 11650: Loss: 0.2200, requires_grad: True\n","Step 11650: Gradients computed successfully\n","Step 11660: Loss: 0.1523, requires_grad: True\n","Step 11670: Loss: 0.1480, requires_grad: True\n","Step 11680: Loss: 0.1584, requires_grad: True\n","Step 11690: Loss: 0.1836, requires_grad: True\n","Step 11700: Loss: 0.3272, requires_grad: True\n","Step 11700: Gradients computed successfully\n","Step 11710: Loss: 0.3256, requires_grad: True\n","Step 11720: Loss: 0.2136, requires_grad: True\n","Step 11730: Loss: 0.2360, requires_grad: True\n","Step 11740: Loss: 0.1202, requires_grad: True\n","Step 11750: Loss: 0.1756, requires_grad: True\n","Step 11750: Gradients computed successfully\n","Step 11760: Loss: 0.4860, requires_grad: True\n","Step 11770: Loss: 0.2172, requires_grad: True\n","Step 11780: Loss: 0.2104, requires_grad: True\n","Step 11790: Loss: 0.1424, requires_grad: True\n","Step 11800: Loss: 0.2332, requires_grad: True\n","Step 11800: Gradients computed successfully\n","Step 11810: Loss: 0.3726, requires_grad: True\n","Step 11820: Loss: 0.0696, requires_grad: True\n","Step 11830: Loss: 0.2824, requires_grad: True\n","Step 11840: Loss: 0.1468, requires_grad: True\n","Step 11850: Loss: 0.2492, requires_grad: True\n","Step 11850: Gradients computed successfully\n","Step 11860: Loss: 0.0776, requires_grad: True\n","Step 11870: Loss: 0.1121, requires_grad: True\n","Step 11880: Loss: 0.3242, requires_grad: True\n","Step 11890: Loss: 0.1744, requires_grad: True\n","Step 11900: Loss: 0.1718, requires_grad: True\n","Step 11900: Gradients computed successfully\n","Step 11910: Loss: 0.1102, requires_grad: True\n","Step 11920: Loss: 0.2354, requires_grad: True\n","Step 11930: Loss: 0.2902, requires_grad: True\n","Step 11940: Loss: 0.3241, requires_grad: True\n","Step 11950: Loss: 0.3301, requires_grad: True\n","Step 11950: Gradients computed successfully\n","Step 11960: Loss: 0.1195, requires_grad: True\n","Step 11970: Loss: 0.0579, requires_grad: True\n","Step 11980: Loss: 0.2112, requires_grad: True\n","Step 11990: Loss: 0.1091, requires_grad: True\n","Step 12000: Loss: 0.2852, requires_grad: True\n","Step 12000: Gradients computed successfully\n","Step 12010: Loss: 0.2503, requires_grad: True\n","Step 12020: Loss: 0.1225, requires_grad: True\n","Step 12030: Loss: 0.1747, requires_grad: True\n","Step 12040: Loss: 0.4039, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-5900\n","Step 12050: Loss: 0.2186, requires_grad: True\n","Step 12050: Gradients computed successfully\n","Step 12060: Loss: 0.1132, requires_grad: True\n","Step 12070: Loss: 0.2498, requires_grad: True\n","Step 12080: Loss: 0.1596, requires_grad: True\n","Step 12090: Loss: 0.1072, requires_grad: True\n","Step 12100: Loss: 0.0640, requires_grad: True\n","Step 12100: Gradients computed successfully\n","Step 12110: Loss: 0.2924, requires_grad: True\n","Step 12120: Loss: 0.1394, requires_grad: True\n","Step 12130: Loss: 0.4378, requires_grad: True\n","Step 12140: Loss: 0.1818, requires_grad: True\n","Step 12150: Loss: 0.2336, requires_grad: True\n","Step 12150: Gradients computed successfully\n","Step 12160: Loss: 0.2022, requires_grad: True\n","Step 12170: Loss: 0.2540, requires_grad: True\n","Step 12180: Loss: 0.3618, requires_grad: True\n","Step 12190: Loss: 0.2341, requires_grad: True\n","Step 12200: Loss: 0.2061, requires_grad: True\n","Step 12200: Gradients computed successfully\n","Step 12210: Loss: 0.1011, requires_grad: True\n","Step 12220: Loss: 0.1836, requires_grad: True\n","Step 12230: Loss: 0.1665, requires_grad: True\n","Step 12240: Loss: 0.1463, requires_grad: True\n","Step 12250: Loss: 0.2085, requires_grad: True\n","Step 12250: Gradients computed successfully\n","Step 12260: Loss: 0.1133, requires_grad: True\n","Step 12270: Loss: 0.3317, requires_grad: True\n","Step 12280: Loss: 0.1782, requires_grad: True\n","Step 12290: Loss: 0.5688, requires_grad: True\n","Step 12300: Loss: 0.4460, requires_grad: True\n","Step 12300: Gradients computed successfully\n","Step 12310: Loss: 0.1077, requires_grad: True\n","Step 12320: Loss: 0.1939, requires_grad: True\n","Step 12330: Loss: 0.1490, requires_grad: True\n","Step 12340: Loss: 0.1905, requires_grad: True\n","Step 12350: Loss: 0.5897, requires_grad: True\n","Step 12350: Gradients computed successfully\n","Step 12360: Loss: 0.1207, requires_grad: True\n","Step 12370: Loss: 0.1943, requires_grad: True\n","Step 12380: Loss: 0.1261, requires_grad: True\n","Step 12390: Loss: 0.2655, requires_grad: True\n","Step 12400: Loss: 0.1186, requires_grad: True\n","Step 12400: Gradients computed successfully\n","Step 12410: Loss: 0.1018, requires_grad: True\n","Step 12420: Loss: 0.1206, requires_grad: True\n","Step 12430: Loss: 0.0836, requires_grad: True\n","Step 12440: Loss: 0.4449, requires_grad: True\n","Step 12450: Loss: 0.1213, requires_grad: True\n","Step 12450: Gradients computed successfully\n","Step 12460: Loss: 0.3092, requires_grad: True\n","Step 12470: Loss: 0.2003, requires_grad: True\n","Step 12480: Loss: 0.1992, requires_grad: True\n","Step 12490: Loss: 0.3220, requires_grad: True\n","Step 12500: Loss: 0.1913, requires_grad: True\n","Step 12500: Gradients computed successfully\n","Step 12510: Loss: 0.2585, requires_grad: True\n","Step 12520: Loss: 0.2781, requires_grad: True\n","Step 12530: Loss: 0.1538, requires_grad: True\n","Step 12540: Loss: 0.2277, requires_grad: True\n","Step 12550: Loss: 0.4846, requires_grad: True\n","Step 12550: Gradients computed successfully\n","Step 12560: Loss: 0.2565, requires_grad: True\n","Step 12570: Loss: 0.3138, requires_grad: True\n","Step 12580: Loss: 0.1479, requires_grad: True\n","Step 12590: Loss: 0.3492, requires_grad: True\n","Step 12600: Loss: 0.2061, requires_grad: True\n","Step 12600: Gradients computed successfully\n","Step 12610: Loss: 0.0543, requires_grad: True\n","Step 12620: Loss: 0.1790, requires_grad: True\n","Step 12630: Loss: 0.1837, requires_grad: True\n","Step 12640: Loss: 0.1453, requires_grad: True\n","Step 12650: Loss: 0.1305, requires_grad: True\n","Step 12650: Gradients computed successfully\n","Step 12660: Loss: 0.1959, requires_grad: True\n","Step 12670: Loss: 0.1204, requires_grad: True\n","Step 12680: Loss: 0.1522, requires_grad: True\n","Step 12690: Loss: 0.1460, requires_grad: True\n","Step 12700: Loss: 0.1778, requires_grad: True\n","Step 12700: Gradients computed successfully\n","Step 12710: Loss: 0.2622, requires_grad: True\n","Step 12720: Loss: 0.1283, requires_grad: True\n","Step 12730: Loss: 0.2293, requires_grad: True\n","Step 12740: Loss: 0.1436, requires_grad: True\n","Step 12750: Loss: 0.2958, requires_grad: True\n","Step 12750: Gradients computed successfully\n","Step 12760: Loss: 0.1492, requires_grad: True\n","Step 12770: Loss: 0.1524, requires_grad: True\n","Step 12780: Loss: 0.4032, requires_grad: True\n","Step 12790: Loss: 0.6035, requires_grad: True\n","Step 12800: Loss: 0.2022, requires_grad: True\n","Step 12800: Gradients computed successfully\n","Step 12810: Loss: 0.1066, requires_grad: True\n","Step 12820: Loss: 0.3931, requires_grad: True\n","Step 12830: Loss: 0.3312, requires_grad: True\n","Step 12840: Loss: 0.2449, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-6000\n","Step 12850: Loss: 0.0879, requires_grad: True\n","Step 12850: Gradients computed successfully\n","Step 12860: Loss: 0.4269, requires_grad: True\n","Step 12870: Loss: 0.1243, requires_grad: True\n","Step 12880: Loss: 0.3562, requires_grad: True\n","Step 12890: Loss: 0.2220, requires_grad: True\n","Step 12900: Loss: 0.1810, requires_grad: True\n","Step 12900: Gradients computed successfully\n","Step 12910: Loss: 0.1157, requires_grad: True\n","Step 12920: Loss: 0.2390, requires_grad: True\n","Step 12930: Loss: 0.3911, requires_grad: True\n","Step 12940: Loss: 0.1120, requires_grad: True\n","Step 12950: Loss: 0.2597, requires_grad: True\n","Step 12950: Gradients computed successfully\n","Step 12960: Loss: 0.1361, requires_grad: True\n","Step 12970: Loss: 0.0887, requires_grad: True\n","Step 12980: Loss: 0.0335, requires_grad: True\n","Step 12990: Loss: 0.0630, requires_grad: True\n","Step 13000: Loss: 0.0814, requires_grad: True\n","Step 13000: Gradients computed successfully\n","Step 13010: Loss: 0.1521, requires_grad: True\n","Step 13020: Loss: 0.2015, requires_grad: True\n","Step 13030: Loss: 0.2562, requires_grad: True\n","Step 13040: Loss: 0.0955, requires_grad: True\n","Step 13050: Loss: 0.1485, requires_grad: True\n","Step 13050: Gradients computed successfully\n","Step 13060: Loss: 0.1395, requires_grad: True\n","Step 13070: Loss: 0.1846, requires_grad: True\n","Step 13080: Loss: 0.1063, requires_grad: True\n","Step 13090: Loss: 0.2726, requires_grad: True\n","Step 13100: Loss: 0.3340, requires_grad: True\n","Step 13100: Gradients computed successfully\n","Step 13110: Loss: 0.0928, requires_grad: True\n","Step 13120: Loss: 0.1808, requires_grad: True\n","Step 13130: Loss: 0.1234, requires_grad: True\n","Step 13140: Loss: 0.0944, requires_grad: True\n","Step 13150: Loss: 0.5078, requires_grad: True\n","Step 13150: Gradients computed successfully\n","Step 13160: Loss: 0.4052, requires_grad: True\n","Step 13170: Loss: 0.1999, requires_grad: True\n","Step 13180: Loss: 0.1636, requires_grad: True\n","Step 13190: Loss: 0.2942, requires_grad: True\n","Step 13200: Loss: 0.4831, requires_grad: True\n","Step 13200: Gradients computed successfully\n","Step 13210: Loss: 0.3187, requires_grad: True\n","Step 13220: Loss: 0.1438, requires_grad: True\n","Step 13230: Loss: 0.4325, requires_grad: True\n","Step 13240: Loss: 0.3382, requires_grad: True\n","Step 13250: Loss: 0.2987, requires_grad: True\n","Step 13250: Gradients computed successfully\n","Step 13260: Loss: 0.1384, requires_grad: True\n","Step 13270: Loss: 0.5911, requires_grad: True\n","Step 13280: Loss: 0.2371, requires_grad: True\n","Step 13290: Loss: 0.3538, requires_grad: True\n","Step 13300: Loss: 0.3285, requires_grad: True\n","Step 13300: Gradients computed successfully\n","Step 13310: Loss: 0.0391, requires_grad: True\n","Step 13320: Loss: 0.4868, requires_grad: True\n","Step 13330: Loss: 0.3092, requires_grad: True\n","Step 13340: Loss: 0.2211, requires_grad: True\n","Step 13350: Loss: 0.1853, requires_grad: True\n","Step 13350: Gradients computed successfully\n","Step 13360: Loss: 0.3419, requires_grad: True\n","Step 13370: Loss: 0.2195, requires_grad: True\n","Step 13380: Loss: 0.1581, requires_grad: True\n","Step 13390: Loss: 0.2610, requires_grad: True\n","Step 13400: Loss: 0.2459, requires_grad: True\n","Step 13400: Gradients computed successfully\n","Step 13410: Loss: 0.3307, requires_grad: True\n","Step 13420: Loss: 0.1126, requires_grad: True\n","Step 13430: Loss: 0.2041, requires_grad: True\n","Step 13440: Loss: 0.4927, requires_grad: True\n","Step 13450: Loss: 0.1652, requires_grad: True\n","Step 13450: Gradients computed successfully\n","Step 13460: Loss: 0.2041, requires_grad: True\n","Step 13470: Loss: 0.2295, requires_grad: True\n","Step 13480: Loss: 0.1218, requires_grad: True\n","Step 13490: Loss: 0.0918, requires_grad: True\n","Step 13500: Loss: 0.1555, requires_grad: True\n","Step 13500: Gradients computed successfully\n","Step 13510: Loss: 0.5777, requires_grad: True\n","Step 13520: Loss: 0.1052, requires_grad: True\n","Step 13530: Loss: 0.3286, requires_grad: True\n","Step 13540: Loss: 0.1368, requires_grad: True\n","Step 13550: Loss: 0.1965, requires_grad: True\n","Step 13550: Gradients computed successfully\n","Step 13560: Loss: 0.2171, requires_grad: True\n","Step 13570: Loss: 0.2466, requires_grad: True\n","Step 13580: Loss: 0.2680, requires_grad: True\n","Step 13590: Loss: 0.3612, requires_grad: True\n","Step 13600: Loss: 0.1483, requires_grad: True\n","Step 13600: Gradients computed successfully\n","Step 13610: Loss: 0.2366, requires_grad: True\n","Step 13620: Loss: 0.2097, requires_grad: True\n","Step 13630: Loss: 0.1561, requires_grad: True\n","Step 13640: Loss: 0.2449, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-6100\n","Step 13650: Loss: 0.1321, requires_grad: True\n","Step 13650: Gradients computed successfully\n","Step 13660: Loss: 0.2099, requires_grad: True\n","Step 13670: Loss: 0.0952, requires_grad: True\n","Step 13680: Loss: 0.2568, requires_grad: True\n","Step 13690: Loss: 0.1430, requires_grad: True\n","Step 13700: Loss: 0.2672, requires_grad: True\n","Step 13700: Gradients computed successfully\n","Step 13710: Loss: 0.2899, requires_grad: True\n","Step 13720: Loss: 0.2043, requires_grad: True\n","Step 13730: Loss: 0.0745, requires_grad: True\n","Step 13740: Loss: 0.1769, requires_grad: True\n","Step 13750: Loss: 0.1445, requires_grad: True\n","Step 13750: Gradients computed successfully\n","Step 13760: Loss: 0.2757, requires_grad: True\n","Step 13770: Loss: 0.1171, requires_grad: True\n","Step 13780: Loss: 0.1588, requires_grad: True\n","Step 13790: Loss: 0.3176, requires_grad: True\n","Step 13800: Loss: 0.1335, requires_grad: True\n","Step 13800: Gradients computed successfully\n","Step 13810: Loss: 0.3132, requires_grad: True\n","Step 13820: Loss: 0.1783, requires_grad: True\n","Step 13830: Loss: 0.3995, requires_grad: True\n","Step 13840: Loss: 0.4207, requires_grad: True\n","Step 13850: Loss: 0.1253, requires_grad: True\n","Step 13850: Gradients computed successfully\n","Step 13860: Loss: 0.3027, requires_grad: True\n","Step 13870: Loss: 0.1385, requires_grad: True\n","Step 13880: Loss: 0.5501, requires_grad: True\n","Step 13890: Loss: 0.3423, requires_grad: True\n","Step 13900: Loss: 0.1448, requires_grad: True\n","Step 13900: Gradients computed successfully\n","Step 13910: Loss: 0.2990, requires_grad: True\n","Step 13920: Loss: 0.1373, requires_grad: True\n","Step 13930: Loss: 0.1603, requires_grad: True\n","Step 13940: Loss: 0.0938, requires_grad: True\n","Step 13950: Loss: 0.2908, requires_grad: True\n","Step 13950: Gradients computed successfully\n","Step 13960: Loss: 0.1143, requires_grad: True\n","Step 13970: Loss: 0.1018, requires_grad: True\n","Step 13980: Loss: 0.2017, requires_grad: True\n","Step 13990: Loss: 0.0877, requires_grad: True\n","Step 14000: Loss: 0.1969, requires_grad: True\n","Step 14000: Gradients computed successfully\n","Step 14010: Loss: 0.2006, requires_grad: True\n","Step 14020: Loss: 0.2854, requires_grad: True\n","Step 14030: Loss: 0.4538, requires_grad: True\n","Step 14040: Loss: 0.1010, requires_grad: True\n","Step 14050: Loss: 0.2191, requires_grad: True\n","Step 14050: Gradients computed successfully\n","Step 14060: Loss: 0.1257, requires_grad: True\n","Step 14070: Loss: 0.2028, requires_grad: True\n","Step 14080: Loss: 0.2187, requires_grad: True\n","Step 14090: Loss: 0.3241, requires_grad: True\n","Step 14100: Loss: 0.1823, requires_grad: True\n","Step 14100: Gradients computed successfully\n","Step 14110: Loss: 0.3706, requires_grad: True\n","Step 14120: Loss: 0.6053, requires_grad: True\n","Step 14130: Loss: 0.3759, requires_grad: True\n","Step 14140: Loss: 0.1569, requires_grad: True\n","Step 14150: Loss: 0.4974, requires_grad: True\n","Step 14150: Gradients computed successfully\n","Step 14160: Loss: 0.4124, requires_grad: True\n","Step 14170: Loss: 0.1106, requires_grad: True\n","Step 14180: Loss: 0.3244, requires_grad: True\n","Step 14190: Loss: 0.3941, requires_grad: True\n","Step 14200: Loss: 0.1587, requires_grad: True\n","Step 14200: Gradients computed successfully\n","Step 14210: Loss: 0.1989, requires_grad: True\n","Step 14220: Loss: 0.0780, requires_grad: True\n","Step 14230: Loss: 0.4020, requires_grad: True\n","Step 14240: Loss: 0.1923, requires_grad: True\n","Step 14250: Loss: 0.1428, requires_grad: True\n","Step 14250: Gradients computed successfully\n","Step 14260: Loss: 0.1087, requires_grad: True\n","Step 14270: Loss: 0.1432, requires_grad: True\n","Step 14280: Loss: 0.2127, requires_grad: True\n","Step 14290: Loss: 0.1284, requires_grad: True\n","Step 14300: Loss: 0.1066, requires_grad: True\n","Step 14300: Gradients computed successfully\n","Step 14310: Loss: 0.2097, requires_grad: True\n","Step 14320: Loss: 0.1963, requires_grad: True\n","Step 14330: Loss: 0.3267, requires_grad: True\n","Step 14340: Loss: 0.2460, requires_grad: True\n","Step 14350: Loss: 0.1682, requires_grad: True\n","Step 14350: Gradients computed successfully\n","Step 14360: Loss: 0.2763, requires_grad: True\n","Step 14370: Loss: 0.0712, requires_grad: True\n","Step 14380: Loss: 0.1546, requires_grad: True\n","Step 14390: Loss: 0.1273, requires_grad: True\n","Step 14400: Loss: 0.0531, requires_grad: True\n","Step 14400: Gradients computed successfully\n","Step 14410: Loss: 0.2232, requires_grad: True\n","Step 14420: Loss: 0.3205, requires_grad: True\n","Step 14430: Loss: 0.2145, requires_grad: True\n","Step 14440: Loss: 0.3522, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-6200\n","Step 14450: Loss: 0.2253, requires_grad: True\n","Step 14450: Gradients computed successfully\n","Step 14460: Loss: 0.3349, requires_grad: True\n","Step 14470: Loss: 0.1234, requires_grad: True\n","Step 14480: Loss: 0.1527, requires_grad: True\n","Step 14490: Loss: 0.2068, requires_grad: True\n","Step 14500: Loss: 0.1843, requires_grad: True\n","Step 14500: Gradients computed successfully\n","Step 14510: Loss: 0.2527, requires_grad: True\n","Step 14520: Loss: 0.1568, requires_grad: True\n","Step 14530: Loss: 0.2381, requires_grad: True\n","Step 14540: Loss: 0.2944, requires_grad: True\n","Step 14550: Loss: 0.2028, requires_grad: True\n","Step 14550: Gradients computed successfully\n","Step 14560: Loss: 0.1584, requires_grad: True\n","Step 14570: Loss: 0.2824, requires_grad: True\n","Step 14580: Loss: 0.1923, requires_grad: True\n","Step 14590: Loss: 0.2860, requires_grad: True\n","Step 14600: Loss: 0.4982, requires_grad: True\n","Step 14600: Gradients computed successfully\n","Step 14610: Loss: 0.0614, requires_grad: True\n","Step 14620: Loss: 0.3246, requires_grad: True\n","Step 14630: Loss: 0.1575, requires_grad: True\n","Step 14640: Loss: 0.1496, requires_grad: True\n","Step 14650: Loss: 0.1346, requires_grad: True\n","Step 14650: Gradients computed successfully\n","Step 14660: Loss: 0.1229, requires_grad: True\n","Step 14670: Loss: 0.1929, requires_grad: True\n","Step 14680: Loss: 0.2122, requires_grad: True\n","Step 14690: Loss: 0.0996, requires_grad: True\n","Step 14700: Loss: 0.1128, requires_grad: True\n","Step 14700: Gradients computed successfully\n","Step 14710: Loss: 0.3289, requires_grad: True\n","Step 14720: Loss: 0.3151, requires_grad: True\n","Step 14730: Loss: 0.2781, requires_grad: True\n","Step 14740: Loss: 0.0876, requires_grad: True\n","Step 14750: Loss: 0.1380, requires_grad: True\n","Step 14750: Gradients computed successfully\n","Step 14760: Loss: 0.2448, requires_grad: True\n","Step 14770: Loss: 0.0725, requires_grad: True\n","Step 14780: Loss: 0.4776, requires_grad: True\n","Step 14790: Loss: 0.0767, requires_grad: True\n","Step 14800: Loss: 0.2675, requires_grad: True\n","Step 14800: Gradients computed successfully\n","Step 14810: Loss: 0.2783, requires_grad: True\n","Step 14820: Loss: 0.2211, requires_grad: True\n","Step 14830: Loss: 0.1540, requires_grad: True\n","Step 14840: Loss: 0.2826, requires_grad: True\n","Step 14850: Loss: 0.2138, requires_grad: True\n","Step 14850: Gradients computed successfully\n","Step 14860: Loss: 0.1743, requires_grad: True\n","Step 14870: Loss: 0.0976, requires_grad: True\n","Step 14880: Loss: 0.0906, requires_grad: True\n","Step 14890: Loss: 0.0578, requires_grad: True\n","Step 14900: Loss: 0.2097, requires_grad: True\n","Step 14900: Gradients computed successfully\n","Step 14910: Loss: 0.0653, requires_grad: True\n","Step 14920: Loss: 0.2506, requires_grad: True\n","Step 14930: Loss: 0.0553, requires_grad: True\n","Step 14940: Loss: 0.1609, requires_grad: True\n","Step 14950: Loss: 0.1987, requires_grad: True\n","Step 14950: Gradients computed successfully\n","Step 14960: Loss: 0.1887, requires_grad: True\n","Step 14970: Loss: 0.2234, requires_grad: True\n","Step 14980: Loss: 0.1521, requires_grad: True\n","Step 14990: Loss: 0.4898, requires_grad: True\n","Step 15000: Loss: 0.0946, requires_grad: True\n","Step 15000: Gradients computed successfully\n","Step 15010: Loss: 0.1623, requires_grad: True\n","Step 15020: Loss: 0.2989, requires_grad: True\n","Step 15030: Loss: 0.4009, requires_grad: True\n","Step 15040: Loss: 0.3021, requires_grad: True\n","Step 15050: Loss: 0.2520, requires_grad: True\n","Step 15050: Gradients computed successfully\n","Step 15060: Loss: 0.1080, requires_grad: True\n","Step 15070: Loss: 0.0589, requires_grad: True\n","Step 15080: Loss: 0.2115, requires_grad: True\n","Step 15090: Loss: 0.1192, requires_grad: True\n","Step 15100: Loss: 0.1293, requires_grad: True\n","Step 15100: Gradients computed successfully\n","Step 15110: Loss: 0.1693, requires_grad: True\n","Step 15120: Loss: 0.3460, requires_grad: True\n","Step 15130: Loss: 0.2089, requires_grad: True\n","Step 15140: Loss: 0.1769, requires_grad: True\n","Step 15150: Loss: 0.3475, requires_grad: True\n","Step 15150: Gradients computed successfully\n","Step 15160: Loss: 0.1448, requires_grad: True\n","Step 15170: Loss: 0.2783, requires_grad: True\n","Step 15180: Loss: 0.1334, requires_grad: True\n","Step 15190: Loss: 0.3482, requires_grad: True\n","Step 15200: Loss: 0.4359, requires_grad: True\n","Step 15200: Gradients computed successfully\n","Step 15210: Loss: 0.2593, requires_grad: True\n","Step 15220: Loss: 0.1866, requires_grad: True\n","Step 15230: Loss: 0.1142, requires_grad: True\n","Step 15240: Loss: 0.0729, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-6300\n","Step 15250: Loss: 0.3827, requires_grad: True\n","Step 15250: Gradients computed successfully\n","Step 15260: Loss: 0.1653, requires_grad: True\n","Step 15270: Loss: 0.0435, requires_grad: True\n","Step 15280: Loss: 0.3945, requires_grad: True\n","Step 15290: Loss: 0.1598, requires_grad: True\n","Step 15300: Loss: 0.2835, requires_grad: True\n","Step 15300: Gradients computed successfully\n","Step 15310: Loss: 0.2285, requires_grad: True\n","Step 15320: Loss: 0.2287, requires_grad: True\n","Step 15330: Loss: 0.3073, requires_grad: True\n","Step 15340: Loss: 0.1871, requires_grad: True\n","Step 15350: Loss: 0.2409, requires_grad: True\n","Step 15350: Gradients computed successfully\n","Step 15360: Loss: 0.1092, requires_grad: True\n","Step 15370: Loss: 0.1470, requires_grad: True\n","Step 15380: Loss: 0.2357, requires_grad: True\n","Step 15390: Loss: 0.2495, requires_grad: True\n","Step 15400: Loss: 0.4102, requires_grad: True\n","Step 15400: Gradients computed successfully\n","Step 15410: Loss: 0.1298, requires_grad: True\n","Step 15420: Loss: 0.1484, requires_grad: True\n","Step 15430: Loss: 0.1106, requires_grad: True\n","Step 15440: Loss: 0.1996, requires_grad: True\n","Step 15450: Loss: 0.1756, requires_grad: True\n","Step 15450: Gradients computed successfully\n","Step 15460: Loss: 0.1865, requires_grad: True\n","Step 15470: Loss: 0.1470, requires_grad: True\n","Step 15480: Loss: 0.0884, requires_grad: True\n","Step 15490: Loss: 0.3003, requires_grad: True\n","Step 15500: Loss: 0.2960, requires_grad: True\n","Step 15500: Gradients computed successfully\n","Step 15510: Loss: 0.1000, requires_grad: True\n","Step 15520: Loss: 0.3050, requires_grad: True\n","Step 15530: Loss: 0.2443, requires_grad: True\n","Step 15540: Loss: 0.0973, requires_grad: True\n","Step 15550: Loss: 0.0488, requires_grad: True\n","Step 15550: Gradients computed successfully\n","Step 15560: Loss: 0.2804, requires_grad: True\n","Step 15570: Loss: 0.0792, requires_grad: True\n","Step 15580: Loss: 0.1006, requires_grad: True\n","Step 15590: Loss: 0.1165, requires_grad: True\n","Step 15600: Loss: 0.1563, requires_grad: True\n","Step 15600: Gradients computed successfully\n","Step 15610: Loss: 0.3524, requires_grad: True\n","Step 15620: Loss: 0.2034, requires_grad: True\n","Step 15630: Loss: 0.1805, requires_grad: True\n","Step 15640: Loss: 0.0898, requires_grad: True\n","Step 15650: Loss: 0.2555, requires_grad: True\n","Step 15650: Gradients computed successfully\n","Step 15660: Loss: 0.0900, requires_grad: True\n","Step 15670: Loss: 0.1106, requires_grad: True\n","Step 15680: Loss: 0.3592, requires_grad: True\n","Step 15690: Loss: 0.4020, requires_grad: True\n","Step 15700: Loss: 0.2210, requires_grad: True\n","Step 15700: Gradients computed successfully\n","Step 15710: Loss: 0.3338, requires_grad: True\n","Step 15720: Loss: 0.2692, requires_grad: True\n","Step 15730: Loss: 0.4382, requires_grad: True\n","Step 15740: Loss: 0.2843, requires_grad: True\n","Step 15750: Loss: 0.3665, requires_grad: True\n","Step 15750: Gradients computed successfully\n","Step 15760: Loss: 0.1044, requires_grad: True\n","Step 15770: Loss: 0.1944, requires_grad: True\n","Step 15780: Loss: 0.1111, requires_grad: True\n","Step 15790: Loss: 0.3445, requires_grad: True\n","Step 15800: Loss: 0.4260, requires_grad: True\n","Step 15800: Gradients computed successfully\n","Step 15810: Loss: 0.1484, requires_grad: True\n","Step 15820: Loss: 0.1593, requires_grad: True\n","Step 15830: Loss: 0.1035, requires_grad: True\n","Step 15840: Loss: 0.1851, requires_grad: True\n","Step 15850: Loss: 0.0770, requires_grad: True\n","Step 15850: Gradients computed successfully\n","Step 15860: Loss: 0.1880, requires_grad: True\n","Step 15870: Loss: 0.1176, requires_grad: True\n","Step 15880: Loss: 0.1293, requires_grad: True\n","Step 15890: Loss: 0.4200, requires_grad: True\n","Step 15900: Loss: 0.1352, requires_grad: True\n","Step 15900: Gradients computed successfully\n","Step 15910: Loss: 0.1261, requires_grad: True\n","Step 15920: Loss: 0.1888, requires_grad: True\n","Step 15930: Loss: 0.2142, requires_grad: True\n","Step 15940: Loss: 0.2554, requires_grad: True\n","Step 15950: Loss: 0.1095, requires_grad: True\n","Step 15950: Gradients computed successfully\n","Step 15960: Loss: 0.0941, requires_grad: True\n","Step 15970: Loss: 0.0973, requires_grad: True\n","Step 15980: Loss: 0.1105, requires_grad: True\n","Step 15990: Loss: 0.0745, requires_grad: True\n","Step 16000: Loss: 0.2912, requires_grad: True\n","Step 16000: Gradients computed successfully\n","Step 16010: Loss: 0.1156, requires_grad: True\n","Step 16020: Loss: 0.1656, requires_grad: True\n","Step 16030: Loss: 0.2335, requires_grad: True\n","Step 16040: Loss: 0.3232, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-6400\n","Step 16050: Loss: 0.1532, requires_grad: True\n","Step 16050: Gradients computed successfully\n","Step 16060: Loss: 0.2370, requires_grad: True\n","Step 16070: Loss: 0.1888, requires_grad: True\n","Step 16080: Loss: 0.1892, requires_grad: True\n","Step 16090: Loss: 0.2429, requires_grad: True\n","Step 16100: Loss: 0.1548, requires_grad: True\n","Step 16100: Gradients computed successfully\n","Step 16110: Loss: 0.1727, requires_grad: True\n","Step 16120: Loss: 0.3022, requires_grad: True\n","Step 16130: Loss: 0.1374, requires_grad: True\n","Step 16140: Loss: 0.2068, requires_grad: True\n","Step 16150: Loss: 0.2261, requires_grad: True\n","Step 16150: Gradients computed successfully\n","Step 16160: Loss: 0.1396, requires_grad: True\n","Step 16170: Loss: 0.2046, requires_grad: True\n","Step 16180: Loss: 0.0960, requires_grad: True\n","Step 16190: Loss: 0.1010, requires_grad: True\n","Step 16200: Loss: 0.2638, requires_grad: True\n","Step 16200: Gradients computed successfully\n","Step 16210: Loss: 0.1710, requires_grad: True\n","Step 16220: Loss: 0.1685, requires_grad: True\n","Step 16230: Loss: 0.1749, requires_grad: True\n","Step 16240: Loss: 0.0913, requires_grad: True\n","Step 16250: Loss: 0.0467, requires_grad: True\n","Step 16250: Gradients computed successfully\n","Step 16260: Loss: 0.1035, requires_grad: True\n","Step 16270: Loss: 0.2905, requires_grad: True\n","Step 16280: Loss: 0.2804, requires_grad: True\n","Step 16290: Loss: 0.1023, requires_grad: True\n","Step 16300: Loss: 0.1355, requires_grad: True\n","Step 16300: Gradients computed successfully\n","Step 16310: Loss: 0.4402, requires_grad: True\n","Step 16320: Loss: 0.4391, requires_grad: True\n","Step 16330: Loss: 0.1256, requires_grad: True\n","Step 16340: Loss: 0.3257, requires_grad: True\n","Step 16350: Loss: 0.2570, requires_grad: True\n","Step 16350: Gradients computed successfully\n","Step 16360: Loss: 0.1041, requires_grad: True\n","Step 16370: Loss: 0.1271, requires_grad: True\n","Step 16380: Loss: 0.1596, requires_grad: True\n","Step 16390: Loss: 0.3622, requires_grad: True\n","Step 16400: Loss: 0.2061, requires_grad: True\n","Step 16400: Gradients computed successfully\n","Step 16410: Loss: 0.1914, requires_grad: True\n","Step 16420: Loss: 0.0716, requires_grad: True\n","Step 16430: Loss: 0.3289, requires_grad: True\n","Step 16440: Loss: 0.3104, requires_grad: True\n","Step 16450: Loss: 0.2337, requires_grad: True\n","Step 16450: Gradients computed successfully\n","Step 16460: Loss: 0.5137, requires_grad: True\n","Step 16470: Loss: 0.0855, requires_grad: True\n","Step 16480: Loss: 0.1574, requires_grad: True\n","Step 16490: Loss: 0.3004, requires_grad: True\n","Step 16500: Loss: 0.0943, requires_grad: True\n","Step 16500: Gradients computed successfully\n","Step 16510: Loss: 0.0632, requires_grad: True\n","Step 16520: Loss: 0.1092, requires_grad: True\n","Step 16530: Loss: 0.2192, requires_grad: True\n","Step 16540: Loss: 0.2611, requires_grad: True\n","Step 16550: Loss: 0.0966, requires_grad: True\n","Step 16550: Gradients computed successfully\n","Step 16560: Loss: 0.1332, requires_grad: True\n","Step 16570: Loss: 0.3130, requires_grad: True\n","Step 16580: Loss: 0.0434, requires_grad: True\n","Step 16590: Loss: 0.0811, requires_grad: True\n","Step 16600: Loss: 0.1168, requires_grad: True\n","Step 16600: Gradients computed successfully\n","Step 16610: Loss: 0.2596, requires_grad: True\n","Step 16620: Loss: 0.1672, requires_grad: True\n","Step 16630: Loss: 0.1744, requires_grad: True\n","Step 16640: Loss: 0.3193, requires_grad: True\n","Step 16650: Loss: 0.2352, requires_grad: True\n","Step 16650: Gradients computed successfully\n","Step 16660: Loss: 0.1959, requires_grad: True\n","Step 16670: Loss: 0.0526, requires_grad: True\n","Step 16680: Loss: 0.1649, requires_grad: True\n","Step 16690: Loss: 0.0607, requires_grad: True\n","Step 16700: Loss: 0.1368, requires_grad: True\n","Step 16700: Gradients computed successfully\n","Step 16710: Loss: 0.1356, requires_grad: True\n","Step 16720: Loss: 0.1974, requires_grad: True\n","Step 16730: Loss: 0.2208, requires_grad: True\n","Step 16740: Loss: 0.1404, requires_grad: True\n","Step 16750: Loss: 0.1072, requires_grad: True\n","Step 16750: Gradients computed successfully\n","Step 16760: Loss: 0.2775, requires_grad: True\n","Step 16770: Loss: 0.3637, requires_grad: True\n","Step 16780: Loss: 0.0660, requires_grad: True\n","Step 16790: Loss: 0.5010, requires_grad: True\n","Step 16800: Loss: 0.1558, requires_grad: True\n","Step 16800: Gradients computed successfully\n","Step 16810: Loss: 0.2294, requires_grad: True\n","Step 16820: Loss: 0.1554, requires_grad: True\n","Step 16830: Loss: 0.1222, requires_grad: True\n","Step 16840: Loss: 0.1647, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-6500\n","Step 16850: Loss: 0.3171, requires_grad: True\n","Step 16850: Gradients computed successfully\n","Step 16860: Loss: 0.0272, requires_grad: True\n","Step 16870: Loss: 0.0955, requires_grad: True\n","Step 16880: Loss: 0.2634, requires_grad: True\n","Step 16890: Loss: 0.0436, requires_grad: True\n","Step 16900: Loss: 0.3134, requires_grad: True\n","Step 16900: Gradients computed successfully\n","Step 16910: Loss: 0.2852, requires_grad: True\n","Step 16920: Loss: 0.1056, requires_grad: True\n","Step 16930: Loss: 0.3319, requires_grad: True\n","Step 16940: Loss: 0.4280, requires_grad: True\n","Step 16950: Loss: 0.1440, requires_grad: True\n","Step 16950: Gradients computed successfully\n","Step 16960: Loss: 0.1351, requires_grad: True\n","Step 16970: Loss: 0.3920, requires_grad: True\n","Step 16980: Loss: 0.4163, requires_grad: True\n","Step 16990: Loss: 0.2090, requires_grad: True\n","Step 17000: Loss: 0.3380, requires_grad: True\n","Step 17000: Gradients computed successfully\n","Step 17010: Loss: 0.3801, requires_grad: True\n","Step 17020: Loss: 0.1877, requires_grad: True\n","Step 17030: Loss: 0.4484, requires_grad: True\n","Step 17040: Loss: 0.1626, requires_grad: True\n","Step 17050: Loss: 0.3121, requires_grad: True\n","Step 17050: Gradients computed successfully\n","Step 17060: Loss: 0.3025, requires_grad: True\n","Step 17070: Loss: 0.2234, requires_grad: True\n","Step 17080: Loss: 0.3123, requires_grad: True\n","Step 17090: Loss: 0.2947, requires_grad: True\n","Step 17100: Loss: 0.0440, requires_grad: True\n","Step 17100: Gradients computed successfully\n","Step 17110: Loss: 0.2045, requires_grad: True\n","Step 17120: Loss: 0.0691, requires_grad: True\n","Step 17130: Loss: 0.3267, requires_grad: True\n","Step 17140: Loss: 0.1053, requires_grad: True\n","Step 17150: Loss: 0.4324, requires_grad: True\n","Step 17150: Gradients computed successfully\n","Step 17160: Loss: 0.1813, requires_grad: True\n","Step 17170: Loss: 0.0647, requires_grad: True\n","Step 17180: Loss: 0.1668, requires_grad: True\n","Step 17190: Loss: 0.2244, requires_grad: True\n","Step 17200: Loss: 0.0970, requires_grad: True\n","Step 17200: Gradients computed successfully\n","Step 17210: Loss: 0.1676, requires_grad: True\n","Step 17220: Loss: 0.0545, requires_grad: True\n","Step 17230: Loss: 0.3354, requires_grad: True\n","Step 17240: Loss: 0.1443, requires_grad: True\n","Step 17250: Loss: 0.2267, requires_grad: True\n","Step 17250: Gradients computed successfully\n","Step 17260: Loss: 0.3036, requires_grad: True\n","Step 17270: Loss: 0.2396, requires_grad: True\n","Step 17280: Loss: 0.0501, requires_grad: True\n","Step 17290: Loss: 0.1025, requires_grad: True\n","Step 17300: Loss: 0.3084, requires_grad: True\n","Step 17300: Gradients computed successfully\n","Step 17310: Loss: 0.2166, requires_grad: True\n","Step 17320: Loss: 0.5416, requires_grad: True\n","Step 17330: Loss: 0.1775, requires_grad: True\n","Step 17340: Loss: 0.1262, requires_grad: True\n","Step 17350: Loss: 0.2143, requires_grad: True\n","Step 17350: Gradients computed successfully\n","Step 17360: Loss: 0.2969, requires_grad: True\n","Step 17370: Loss: 0.1230, requires_grad: True\n","Step 17380: Loss: 0.3029, requires_grad: True\n","Step 17390: Loss: 0.0691, requires_grad: True\n","Step 17400: Loss: 0.1580, requires_grad: True\n","Step 17400: Gradients computed successfully\n","Step 17410: Loss: 0.1038, requires_grad: True\n","Step 17420: Loss: 0.2988, requires_grad: True\n","Step 17430: Loss: 0.1778, requires_grad: True\n","Step 17440: Loss: 0.3684, requires_grad: True\n","Step 17450: Loss: 0.1035, requires_grad: True\n","Step 17450: Gradients computed successfully\n","Step 17460: Loss: 0.1012, requires_grad: True\n","Step 17470: Loss: 0.0971, requires_grad: True\n","Step 17480: Loss: 0.3461, requires_grad: True\n","Step 17490: Loss: 0.3532, requires_grad: True\n","Step 17500: Loss: 0.1493, requires_grad: True\n","Step 17500: Gradients computed successfully\n","Step 17510: Loss: 0.2602, requires_grad: True\n","Step 17520: Loss: 0.1118, requires_grad: True\n","Step 17530: Loss: 0.0849, requires_grad: True\n","Step 17540: Loss: 0.3030, requires_grad: True\n","Step 17550: Loss: 0.1433, requires_grad: True\n","Step 17550: Gradients computed successfully\n","Step 17560: Loss: 0.1389, requires_grad: True\n","Step 17570: Loss: 0.1895, requires_grad: True\n","Step 17580: Loss: 0.2978, requires_grad: True\n","Step 17590: Loss: 0.2648, requires_grad: True\n","Step 17600: Loss: 0.1570, requires_grad: True\n","Step 17600: Gradients computed successfully\n","Step 17610: Loss: 0.0715, requires_grad: True\n","Step 17620: Loss: 0.1603, requires_grad: True\n","Step 17630: Loss: 0.3687, requires_grad: True\n","Step 17640: Loss: 0.0900, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-6600\n","Step 17650: Loss: 0.2269, requires_grad: True\n","Step 17650: Gradients computed successfully\n","Step 17660: Loss: 0.2183, requires_grad: True\n","Step 17670: Loss: 0.1317, requires_grad: True\n","Step 17680: Loss: 0.1327, requires_grad: True\n","Step 17690: Loss: 0.0744, requires_grad: True\n","Step 17700: Loss: 0.1658, requires_grad: True\n","Step 17700: Gradients computed successfully\n","Step 17710: Loss: 0.3795, requires_grad: True\n","Step 17720: Loss: 0.2122, requires_grad: True\n","Step 17730: Loss: 0.1381, requires_grad: True\n","Step 17740: Loss: 0.0767, requires_grad: True\n","Step 17750: Loss: 0.2326, requires_grad: True\n","Step 17750: Gradients computed successfully\n","Step 17760: Loss: 0.0582, requires_grad: True\n","Step 17770: Loss: 0.1788, requires_grad: True\n","Step 17780: Loss: 0.2855, requires_grad: True\n","Step 17790: Loss: 0.1329, requires_grad: True\n","Step 17800: Loss: 0.1897, requires_grad: True\n","Step 17800: Gradients computed successfully\n","Step 17810: Loss: 0.2150, requires_grad: True\n","Step 17820: Loss: 0.1444, requires_grad: True\n","Step 17830: Loss: 0.3091, requires_grad: True\n","Step 17840: Loss: 0.1619, requires_grad: True\n","Step 17850: Loss: 0.1362, requires_grad: True\n","Step 17850: Gradients computed successfully\n","Step 17860: Loss: 0.1325, requires_grad: True\n","Step 17870: Loss: 0.2186, requires_grad: True\n","Step 17880: Loss: 0.0875, requires_grad: True\n","Step 17890: Loss: 0.3030, requires_grad: True\n","Step 17900: Loss: 0.4439, requires_grad: True\n","Step 17900: Gradients computed successfully\n","Step 17910: Loss: 0.2584, requires_grad: True\n","Step 17920: Loss: 0.2245, requires_grad: True\n","Step 17930: Loss: 0.4976, requires_grad: True\n","Step 17940: Loss: 0.2220, requires_grad: True\n","Step 17950: Loss: 0.1272, requires_grad: True\n","Step 17950: Gradients computed successfully\n","Step 17960: Loss: 0.1944, requires_grad: True\n","Step 17970: Loss: 0.1723, requires_grad: True\n","Step 17980: Loss: 0.1654, requires_grad: True\n","Step 17990: Loss: 0.3493, requires_grad: True\n","Step 18000: Loss: 0.0784, requires_grad: True\n","Step 18000: Gradients computed successfully\n","Step 18010: Loss: 0.1641, requires_grad: True\n","Step 18020: Loss: 0.2171, requires_grad: True\n","Step 18030: Loss: 0.1688, requires_grad: True\n","Step 18040: Loss: 0.1190, requires_grad: True\n","Step 18050: Loss: 0.1199, requires_grad: True\n","Step 18050: Gradients computed successfully\n","Step 18060: Loss: 0.0972, requires_grad: True\n","Step 18070: Loss: 0.1547, requires_grad: True\n","Step 18080: Loss: 0.1396, requires_grad: True\n","Step 18090: Loss: 0.2293, requires_grad: True\n","Step 18100: Loss: 0.2175, requires_grad: True\n","Step 18100: Gradients computed successfully\n","Step 18110: Loss: 0.1782, requires_grad: True\n","Step 18120: Loss: 0.0819, requires_grad: True\n","Step 18130: Loss: 0.5428, requires_grad: True\n","Step 18140: Loss: 0.1725, requires_grad: True\n","Step 18150: Loss: 0.1035, requires_grad: True\n","Step 18150: Gradients computed successfully\n","Step 18160: Loss: 0.2242, requires_grad: True\n","Step 18170: Loss: 0.2103, requires_grad: True\n","Step 18180: Loss: 0.1598, requires_grad: True\n","Step 18190: Loss: 0.3986, requires_grad: True\n","Step 18200: Loss: 0.0739, requires_grad: True\n","Step 18200: Gradients computed successfully\n","Step 18210: Loss: 0.1259, requires_grad: True\n","Step 18220: Loss: 0.0785, requires_grad: True\n","Step 18230: Loss: 0.0912, requires_grad: True\n","Step 18240: Loss: 0.0670, requires_grad: True\n","Step 18250: Loss: 0.1794, requires_grad: True\n","Step 18250: Gradients computed successfully\n","Step 18260: Loss: 0.3176, requires_grad: True\n","Step 18270: Loss: 0.1728, requires_grad: True\n","Step 18280: Loss: 0.1573, requires_grad: True\n","Step 18290: Loss: 0.2832, requires_grad: True\n","Step 18300: Loss: 0.1726, requires_grad: True\n","Step 18300: Gradients computed successfully\n","Step 18310: Loss: 0.1292, requires_grad: True\n","Step 18320: Loss: 0.3339, requires_grad: True\n","Step 18330: Loss: 0.1162, requires_grad: True\n","Step 18340: Loss: 0.2692, requires_grad: True\n","Step 18350: Loss: 0.3044, requires_grad: True\n","Step 18350: Gradients computed successfully\n","Step 18360: Loss: 0.4428, requires_grad: True\n","Step 18370: Loss: 0.1435, requires_grad: True\n","Step 18380: Loss: 0.1645, requires_grad: True\n","Step 18390: Loss: 0.2230, requires_grad: True\n","Step 18400: Loss: 0.3148, requires_grad: True\n","Step 18400: Gradients computed successfully\n","Step 18410: Loss: 0.3272, requires_grad: True\n","Step 18420: Loss: 0.0587, requires_grad: True\n","Step 18430: Loss: 0.0841, requires_grad: True\n","Step 18440: Loss: 0.1908, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-6700\n","Step 18450: Loss: 0.1684, requires_grad: True\n","Step 18450: Gradients computed successfully\n","Step 18460: Loss: 0.3600, requires_grad: True\n","Step 18470: Loss: 0.1686, requires_grad: True\n","Step 18480: Loss: 0.2327, requires_grad: True\n","Step 18490: Loss: 0.3475, requires_grad: True\n","Step 18500: Loss: 0.1151, requires_grad: True\n","Step 18500: Gradients computed successfully\n","Step 18510: Loss: 0.2119, requires_grad: True\n","Step 18520: Loss: 0.2887, requires_grad: True\n","Step 18530: Loss: 0.1350, requires_grad: True\n","Step 18540: Loss: 0.1108, requires_grad: True\n","Step 18550: Loss: 0.1181, requires_grad: True\n","Step 18550: Gradients computed successfully\n","Step 18560: Loss: 0.1857, requires_grad: True\n","Step 18570: Loss: 0.4410, requires_grad: True\n","Step 18580: Loss: 0.2323, requires_grad: True\n","Step 18590: Loss: 0.0984, requires_grad: True\n","Step 18600: Loss: 0.3418, requires_grad: True\n","Step 18600: Gradients computed successfully\n","Step 18610: Loss: 0.0686, requires_grad: True\n","Step 18620: Loss: 0.1232, requires_grad: True\n","Step 18630: Loss: 0.0707, requires_grad: True\n","Step 18640: Loss: 0.2036, requires_grad: True\n","Step 18650: Loss: 0.1226, requires_grad: True\n","Step 18650: Gradients computed successfully\n","Step 18660: Loss: 0.2822, requires_grad: True\n","Step 18670: Loss: 0.2072, requires_grad: True\n","Step 18680: Loss: 0.1688, requires_grad: True\n","Step 18690: Loss: 0.0908, requires_grad: True\n","Step 18700: Loss: 0.1152, requires_grad: True\n","Step 18700: Gradients computed successfully\n","Step 18710: Loss: 0.4070, requires_grad: True\n","Step 18720: Loss: 0.1409, requires_grad: True\n","Step 18730: Loss: 0.2963, requires_grad: True\n","Step 18740: Loss: 0.2369, requires_grad: True\n","Step 18750: Loss: 0.0953, requires_grad: True\n","Step 18750: Gradients computed successfully\n","Step 18760: Loss: 0.1442, requires_grad: True\n","Step 18770: Loss: 0.1703, requires_grad: True\n","Step 18780: Loss: 0.1613, requires_grad: True\n","Step 18790: Loss: 0.2373, requires_grad: True\n","Step 18800: Loss: 0.0629, requires_grad: True\n","Step 18800: Gradients computed successfully\n","Step 18810: Loss: 0.2902, requires_grad: True\n","Step 18820: Loss: 0.1019, requires_grad: True\n","Step 18830: Loss: 0.2353, requires_grad: True\n","Step 18840: Loss: 0.1711, requires_grad: True\n","Step 18850: Loss: 0.2252, requires_grad: True\n","Step 18850: Gradients computed successfully\n","Step 18860: Loss: 0.1926, requires_grad: True\n","Step 18870: Loss: 0.1420, requires_grad: True\n","Step 18880: Loss: 0.0487, requires_grad: True\n","Step 18890: Loss: 0.2328, requires_grad: True\n","Step 18900: Loss: 0.1468, requires_grad: True\n","Step 18900: Gradients computed successfully\n","Step 18910: Loss: 0.2790, requires_grad: True\n","Step 18920: Loss: 0.0582, requires_grad: True\n","Step 18930: Loss: 0.3041, requires_grad: True\n","Step 18940: Loss: 0.2281, requires_grad: True\n","Step 18950: Loss: 0.1992, requires_grad: True\n","Step 18950: Gradients computed successfully\n","Step 18960: Loss: 0.1608, requires_grad: True\n","Step 18970: Loss: 0.1073, requires_grad: True\n","Step 18980: Loss: 0.2858, requires_grad: True\n","Step 18990: Loss: 0.4683, requires_grad: True\n","Step 19000: Loss: 0.1961, requires_grad: True\n","Step 19000: Gradients computed successfully\n","Step 19010: Loss: 0.5469, requires_grad: True\n","Step 19020: Loss: 0.1882, requires_grad: True\n","Step 19030: Loss: 0.1031, requires_grad: True\n","Step 19040: Loss: 0.2531, requires_grad: True\n","Step 19050: Loss: 0.2021, requires_grad: True\n","Step 19050: Gradients computed successfully\n","Step 19060: Loss: 0.1807, requires_grad: True\n","Step 19070: Loss: 0.2951, requires_grad: True\n","Step 19080: Loss: 0.3579, requires_grad: True\n","Step 19090: Loss: 0.2270, requires_grad: True\n","Step 19100: Loss: 0.2083, requires_grad: True\n","Step 19100: Gradients computed successfully\n","Step 19110: Loss: 0.2084, requires_grad: True\n","Step 19120: Loss: 0.1729, requires_grad: True\n","Step 19130: Loss: 0.2578, requires_grad: True\n","Step 19140: Loss: 0.3239, requires_grad: True\n","Step 19150: Loss: 0.2742, requires_grad: True\n","Step 19150: Gradients computed successfully\n","Step 19160: Loss: 0.2475, requires_grad: True\n","Step 19170: Loss: 0.0782, requires_grad: True\n","Step 19180: Loss: 0.1245, requires_grad: True\n","Step 19190: Loss: 0.4067, requires_grad: True\n","Step 19200: Loss: 0.2028, requires_grad: True\n","Step 19200: Gradients computed successfully\n","Step 19210: Loss: 0.3487, requires_grad: True\n","Step 19220: Loss: 0.1518, requires_grad: True\n","Step 19230: Loss: 0.2680, requires_grad: True\n","Step 19240: Loss: 0.2504, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-6800\n","Step 19250: Loss: 0.3441, requires_grad: True\n","Step 19250: Gradients computed successfully\n","Step 19260: Loss: 0.0941, requires_grad: True\n","Step 19270: Loss: 0.1716, requires_grad: True\n","Step 19280: Loss: 0.0584, requires_grad: True\n","Step 19290: Loss: 0.0740, requires_grad: True\n","Step 19300: Loss: 0.3016, requires_grad: True\n","Step 19300: Gradients computed successfully\n","Step 19310: Loss: 0.0790, requires_grad: True\n","Step 19320: Loss: 0.4558, requires_grad: True\n","Step 19330: Loss: 0.4012, requires_grad: True\n","Step 19340: Loss: 0.0848, requires_grad: True\n","Step 19350: Loss: 0.2221, requires_grad: True\n","Step 19350: Gradients computed successfully\n","Step 19360: Loss: 0.1570, requires_grad: True\n","Step 19370: Loss: 0.0973, requires_grad: True\n","Step 19380: Loss: 0.1505, requires_grad: True\n","Step 19390: Loss: 0.1099, requires_grad: True\n","Step 19400: Loss: 0.1208, requires_grad: True\n","Step 19400: Gradients computed successfully\n","Step 19410: Loss: 0.1170, requires_grad: True\n","Step 19420: Loss: 0.3358, requires_grad: True\n","Step 19430: Loss: 0.0845, requires_grad: True\n","Step 19440: Loss: 0.0799, requires_grad: True\n","Step 19450: Loss: 0.3017, requires_grad: True\n","Step 19450: Gradients computed successfully\n","Step 19460: Loss: 0.2283, requires_grad: True\n","Step 19470: Loss: 0.2249, requires_grad: True\n","Step 19480: Loss: 0.1706, requires_grad: True\n","Step 19490: Loss: 0.1467, requires_grad: True\n","Step 19500: Loss: 0.1054, requires_grad: True\n","Step 19500: Gradients computed successfully\n","Step 19510: Loss: 0.3284, requires_grad: True\n","Step 19520: Loss: 0.6886, requires_grad: True\n","Step 19530: Loss: 0.0859, requires_grad: True\n","Step 19540: Loss: 0.1855, requires_grad: True\n","Step 19550: Loss: 0.3642, requires_grad: True\n","Step 19550: Gradients computed successfully\n","Step 19560: Loss: 0.2713, requires_grad: True\n","Step 19570: Loss: 0.1859, requires_grad: True\n","Step 19580: Loss: 0.3820, requires_grad: True\n","Step 19590: Loss: 0.2600, requires_grad: True\n","Step 19600: Loss: 0.0387, requires_grad: True\n","Step 19600: Gradients computed successfully\n","Step 19610: Loss: 0.2059, requires_grad: True\n","Step 19620: Loss: 0.1280, requires_grad: True\n","Step 19630: Loss: 0.1365, requires_grad: True\n","Step 19640: Loss: 0.1452, requires_grad: True\n","Step 19650: Loss: 0.2774, requires_grad: True\n","Step 19650: Gradients computed successfully\n","Step 19660: Loss: 0.0837, requires_grad: True\n","Step 19670: Loss: 0.1322, requires_grad: True\n","Step 19680: Loss: 0.1093, requires_grad: True\n","Step 19690: Loss: 0.1286, requires_grad: True\n","Step 19700: Loss: 0.2623, requires_grad: True\n","Step 19700: Gradients computed successfully\n","Step 19710: Loss: 0.2210, requires_grad: True\n","Step 19720: Loss: 0.0838, requires_grad: True\n","Step 19730: Loss: 0.2300, requires_grad: True\n","Step 19740: Loss: 0.1059, requires_grad: True\n","Step 19750: Loss: 0.1461, requires_grad: True\n","Step 19750: Gradients computed successfully\n","Step 19760: Loss: 0.4362, requires_grad: True\n","Step 19770: Loss: 0.1149, requires_grad: True\n","Step 19780: Loss: 0.1353, requires_grad: True\n","Step 19790: Loss: 0.2357, requires_grad: True\n","Step 19800: Loss: 0.0824, requires_grad: True\n","Step 19800: Gradients computed successfully\n","Step 19810: Loss: 0.1919, requires_grad: True\n","Step 19820: Loss: 0.1193, requires_grad: True\n","Step 19830: Loss: 0.1149, requires_grad: True\n","Step 19840: Loss: 0.2705, requires_grad: True\n","Step 19850: Loss: 0.0953, requires_grad: True\n","Step 19850: Gradients computed successfully\n","Step 19860: Loss: 0.2714, requires_grad: True\n","Step 19870: Loss: 0.0974, requires_grad: True\n","Step 19880: Loss: 0.0827, requires_grad: True\n","Step 19890: Loss: 0.1362, requires_grad: True\n","Step 19900: Loss: 0.1447, requires_grad: True\n","Step 19900: Gradients computed successfully\n","Step 19910: Loss: 0.1124, requires_grad: True\n","Step 19920: Loss: 0.1837, requires_grad: True\n","Step 19930: Loss: 0.2136, requires_grad: True\n","Step 19940: Loss: 0.2537, requires_grad: True\n","Step 19950: Loss: 0.1983, requires_grad: True\n","Step 19950: Gradients computed successfully\n","Step 19960: Loss: 0.1246, requires_grad: True\n","Step 19970: Loss: 0.0520, requires_grad: True\n","Step 19980: Loss: 0.1642, requires_grad: True\n","Step 19990: Loss: 0.1597, requires_grad: True\n","Step 20000: Loss: 0.1531, requires_grad: True\n","Step 20000: Gradients computed successfully\n","Step 20010: Loss: 0.1676, requires_grad: True\n","Step 20020: Loss: 0.2985, requires_grad: True\n","Step 20030: Loss: 0.2520, requires_grad: True\n","Step 20040: Loss: 0.1761, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-6900\n","Step 20050: Loss: 0.1574, requires_grad: True\n","Step 20050: Gradients computed successfully\n","Step 20060: Loss: 0.2983, requires_grad: True\n","Step 20070: Loss: 0.1538, requires_grad: True\n","Step 20080: Loss: 0.4571, requires_grad: True\n","Step 20090: Loss: 0.2277, requires_grad: True\n","Step 20100: Loss: 0.4519, requires_grad: True\n","Step 20100: Gradients computed successfully\n","Step 20110: Loss: 0.1580, requires_grad: True\n","Step 20120: Loss: 0.2379, requires_grad: True\n","Step 20130: Loss: 0.1364, requires_grad: True\n","Step 20140: Loss: 0.2071, requires_grad: True\n","Step 20150: Loss: 0.5325, requires_grad: True\n","Step 20150: Gradients computed successfully\n","Step 20160: Loss: 0.2084, requires_grad: True\n","Step 20170: Loss: 0.1393, requires_grad: True\n","Step 20180: Loss: 0.3050, requires_grad: True\n","Step 20190: Loss: 0.2898, requires_grad: True\n","Step 20200: Loss: 0.1751, requires_grad: True\n","Step 20200: Gradients computed successfully\n","Step 20210: Loss: 0.1582, requires_grad: True\n","Step 20220: Loss: 0.2164, requires_grad: True\n","Step 20230: Loss: 0.2079, requires_grad: True\n","Step 20240: Loss: 0.4299, requires_grad: True\n","Step 20250: Loss: 0.2958, requires_grad: True\n","Step 20250: Gradients computed successfully\n","Step 20260: Loss: 0.1189, requires_grad: True\n","Step 20270: Loss: 0.1594, requires_grad: True\n","Step 20280: Loss: 0.1752, requires_grad: True\n","Step 20290: Loss: 0.3387, requires_grad: True\n","Step 20300: Loss: 0.0793, requires_grad: True\n","Step 20300: Gradients computed successfully\n","Step 20310: Loss: 0.2547, requires_grad: True\n","Step 20320: Loss: 0.1413, requires_grad: True\n","Step 20330: Loss: 0.2714, requires_grad: True\n","Step 20340: Loss: 0.3516, requires_grad: True\n","Step 20350: Loss: 0.2937, requires_grad: True\n","Step 20350: Gradients computed successfully\n","Step 20360: Loss: 0.0750, requires_grad: True\n","Step 20370: Loss: 0.1551, requires_grad: True\n","Step 20380: Loss: 0.1607, requires_grad: True\n","Step 20390: Loss: 0.3205, requires_grad: True\n","Step 20400: Loss: 0.0713, requires_grad: True\n","Step 20400: Gradients computed successfully\n","Step 20410: Loss: 0.1826, requires_grad: True\n","Step 20420: Loss: 0.0950, requires_grad: True\n","Step 20430: Loss: 0.2279, requires_grad: True\n","Step 20440: Loss: 0.2536, requires_grad: True\n","Step 20450: Loss: 0.3744, requires_grad: True\n","Step 20450: Gradients computed successfully\n","Step 20460: Loss: 0.1813, requires_grad: True\n","Step 20470: Loss: 0.2274, requires_grad: True\n","Step 20480: Loss: 0.5032, requires_grad: True\n","Step 20490: Loss: 0.1448, requires_grad: True\n","Step 20500: Loss: 0.3305, requires_grad: True\n","Step 20500: Gradients computed successfully\n","Step 20510: Loss: 0.3179, requires_grad: True\n","Step 20520: Loss: 0.3382, requires_grad: True\n","Step 20530: Loss: 0.1501, requires_grad: True\n","Step 20540: Loss: 0.2341, requires_grad: True\n","Step 20550: Loss: 0.2399, requires_grad: True\n","Step 20550: Gradients computed successfully\n","Step 20560: Loss: 0.1077, requires_grad: True\n","Step 20570: Loss: 0.3067, requires_grad: True\n","Step 20580: Loss: 0.1674, requires_grad: True\n","Step 20590: Loss: 0.1183, requires_grad: True\n","Step 20600: Loss: 0.1301, requires_grad: True\n","Step 20600: Gradients computed successfully\n","Step 20610: Loss: 0.0908, requires_grad: True\n","Step 20620: Loss: 0.1979, requires_grad: True\n","Step 20630: Loss: 0.1132, requires_grad: True\n","Step 20640: Loss: 0.1029, requires_grad: True\n","Step 20650: Loss: 0.2835, requires_grad: True\n","Step 20650: Gradients computed successfully\n","Step 20660: Loss: 0.2165, requires_grad: True\n","Step 20670: Loss: 0.1877, requires_grad: True\n","Step 20680: Loss: 0.1087, requires_grad: True\n","Step 20690: Loss: 0.2671, requires_grad: True\n","Step 20700: Loss: 0.2120, requires_grad: True\n","Step 20700: Gradients computed successfully\n","Step 20710: Loss: 0.2187, requires_grad: True\n","Step 20720: Loss: 0.1638, requires_grad: True\n","Step 20730: Loss: 0.1885, requires_grad: True\n","Step 20740: Loss: 0.2169, requires_grad: True\n","Step 20750: Loss: 0.1454, requires_grad: True\n","Step 20750: Gradients computed successfully\n","Step 20760: Loss: 0.4116, requires_grad: True\n","Step 20770: Loss: 0.1601, requires_grad: True\n","Step 20780: Loss: 0.1427, requires_grad: True\n","Step 20790: Loss: 0.2284, requires_grad: True\n","Step 20800: Loss: 0.0835, requires_grad: True\n","Step 20800: Gradients computed successfully\n","Step 20810: Loss: 0.1192, requires_grad: True\n","Step 20820: Loss: 0.1128, requires_grad: True\n","Step 20830: Loss: 0.0588, requires_grad: True\n","Step 20840: Loss: 0.2174, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-7000\n","Step 20850: Loss: 0.2176, requires_grad: True\n","Step 20850: Gradients computed successfully\n","Step 20860: Loss: 0.2327, requires_grad: True\n","Step 20870: Loss: 0.0614, requires_grad: True\n","Step 20880: Loss: 0.2816, requires_grad: True\n","Step 20890: Loss: 0.1845, requires_grad: True\n","Step 20900: Loss: 0.3046, requires_grad: True\n","Step 20900: Gradients computed successfully\n","Step 20910: Loss: 0.3149, requires_grad: True\n","Step 20920: Loss: 0.1867, requires_grad: True\n","Step 20930: Loss: 0.2040, requires_grad: True\n","Step 20940: Loss: 0.0831, requires_grad: True\n","Step 20950: Loss: 0.0995, requires_grad: True\n","Step 20950: Gradients computed successfully\n","Step 20960: Loss: 0.2621, requires_grad: True\n","Step 20970: Loss: 0.0775, requires_grad: True\n","Step 20980: Loss: 0.3464, requires_grad: True\n","Step 20990: Loss: 0.2368, requires_grad: True\n","Step 21000: Loss: 0.1079, requires_grad: True\n","Step 21000: Gradients computed successfully\n","Step 21010: Loss: 0.4963, requires_grad: True\n","Step 21020: Loss: 0.0426, requires_grad: True\n","Step 21030: Loss: 0.1595, requires_grad: True\n","Step 21040: Loss: 0.0842, requires_grad: True\n","Step 21050: Loss: 0.2425, requires_grad: True\n","Step 21050: Gradients computed successfully\n","Step 21060: Loss: 0.1501, requires_grad: True\n","Step 21070: Loss: 0.4139, requires_grad: True\n","Step 21080: Loss: 0.1077, requires_grad: True\n","Step 21090: Loss: 0.1227, requires_grad: True\n","Step 21100: Loss: 0.1320, requires_grad: True\n","Step 21100: Gradients computed successfully\n","Step 21110: Loss: 0.0891, requires_grad: True\n","Step 21120: Loss: 0.2910, requires_grad: True\n","Step 21130: Loss: 0.4642, requires_grad: True\n","Step 21140: Loss: 0.1091, requires_grad: True\n","Step 21150: Loss: 0.1303, requires_grad: True\n","Step 21150: Gradients computed successfully\n","Step 21160: Loss: 0.3522, requires_grad: True\n","Step 21170: Loss: 0.3229, requires_grad: True\n","Step 21180: Loss: 0.1651, requires_grad: True\n","Step 21190: Loss: 0.1982, requires_grad: True\n","Step 21200: Loss: 0.4343, requires_grad: True\n","Step 21200: Gradients computed successfully\n","Step 21210: Loss: 0.2226, requires_grad: True\n","Step 21220: Loss: 0.4213, requires_grad: True\n","Step 21230: Loss: 0.1206, requires_grad: True\n","Step 21240: Loss: 0.2701, requires_grad: True\n","Step 21250: Loss: 0.3173, requires_grad: True\n","Step 21250: Gradients computed successfully\n","Step 21260: Loss: 0.1477, requires_grad: True\n","Step 21270: Loss: 0.1318, requires_grad: True\n","Step 21280: Loss: 0.1932, requires_grad: True\n","Step 21290: Loss: 0.2732, requires_grad: True\n","Step 21300: Loss: 0.1263, requires_grad: True\n","Step 21300: Gradients computed successfully\n","Step 21310: Loss: 0.2867, requires_grad: True\n","Step 21320: Loss: 0.3132, requires_grad: True\n","Step 21330: Loss: 0.2417, requires_grad: True\n","Step 21340: Loss: 0.2768, requires_grad: True\n","Step 21350: Loss: 0.3309, requires_grad: True\n","Step 21350: Gradients computed successfully\n","Step 21360: Loss: 0.1228, requires_grad: True\n","Step 21370: Loss: 0.0972, requires_grad: True\n","Step 21380: Loss: 0.0540, requires_grad: True\n","Step 21390: Loss: 0.1285, requires_grad: True\n","Step 21400: Loss: 0.2543, requires_grad: True\n","Step 21400: Gradients computed successfully\n","Step 21410: Loss: 0.1357, requires_grad: True\n","Step 21420: Loss: 0.1562, requires_grad: True\n","Step 21430: Loss: 0.1831, requires_grad: True\n","Step 21440: Loss: 0.1377, requires_grad: True\n","Step 21450: Loss: 0.2237, requires_grad: True\n","Step 21450: Gradients computed successfully\n","Step 21460: Loss: 0.1733, requires_grad: True\n","Step 21470: Loss: 0.0704, requires_grad: True\n","Step 21480: Loss: 0.2065, requires_grad: True\n","Step 21490: Loss: 0.2083, requires_grad: True\n","Step 21500: Loss: 0.3275, requires_grad: True\n","Step 21500: Gradients computed successfully\n","Step 21510: Loss: 0.1865, requires_grad: True\n","Step 21520: Loss: 0.2869, requires_grad: True\n","Step 21530: Loss: 0.0751, requires_grad: True\n","Step 21540: Loss: 0.5595, requires_grad: True\n","Step 21550: Loss: 0.2218, requires_grad: True\n","Step 21550: Gradients computed successfully\n","Step 21560: Loss: 0.2001, requires_grad: True\n","Step 21570: Loss: 0.2771, requires_grad: True\n","Step 21580: Loss: 0.1324, requires_grad: True\n","Step 21590: Loss: 0.2202, requires_grad: True\n","Step 21600: Loss: 0.0861, requires_grad: True\n","Step 21600: Gradients computed successfully\n","Step 21610: Loss: 0.3007, requires_grad: True\n","Step 21620: Loss: 0.1884, requires_grad: True\n","Step 21630: Loss: 0.1487, requires_grad: True\n","Step 21640: Loss: 0.5092, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-7100\n","Step 21650: Loss: 0.1524, requires_grad: True\n","Step 21650: Gradients computed successfully\n","Step 21660: Loss: 0.0779, requires_grad: True\n","Step 21670: Loss: 0.3128, requires_grad: True\n","Step 21680: Loss: 0.2210, requires_grad: True\n","Step 21690: Loss: 0.0566, requires_grad: True\n","Step 21700: Loss: 0.3790, requires_grad: True\n","Step 21700: Gradients computed successfully\n","Step 21710: Loss: 0.1643, requires_grad: True\n","Step 21720: Loss: 0.1690, requires_grad: True\n","Step 21730: Loss: 0.1092, requires_grad: True\n","Step 21740: Loss: 0.1324, requires_grad: True\n","Step 21750: Loss: 0.3100, requires_grad: True\n","Step 21750: Gradients computed successfully\n","Step 21760: Loss: 0.1262, requires_grad: True\n","Step 21770: Loss: 0.1434, requires_grad: True\n","Step 21780: Loss: 0.1449, requires_grad: True\n","Step 21790: Loss: 0.3666, requires_grad: True\n","Step 21800: Loss: 0.2488, requires_grad: True\n","Step 21800: Gradients computed successfully\n","Step 21810: Loss: 0.1815, requires_grad: True\n","Step 21820: Loss: 0.3355, requires_grad: True\n","Step 21830: Loss: 0.2417, requires_grad: True\n","Step 21840: Loss: 0.2030, requires_grad: True\n","Step 21850: Loss: 0.1184, requires_grad: True\n","Step 21850: Gradients computed successfully\n","Step 21860: Loss: 0.2876, requires_grad: True\n","Step 21870: Loss: 0.1406, requires_grad: True\n","Step 21880: Loss: 0.1265, requires_grad: True\n","Step 21890: Loss: 0.2861, requires_grad: True\n","Step 21900: Loss: 0.1254, requires_grad: True\n","Step 21900: Gradients computed successfully\n","Step 21910: Loss: 0.4175, requires_grad: True\n","Step 21920: Loss: 0.1965, requires_grad: True\n","Step 21930: Loss: 0.2562, requires_grad: True\n","Step 21940: Loss: 0.2743, requires_grad: True\n","Step 21950: Loss: 0.3315, requires_grad: True\n","Step 21950: Gradients computed successfully\n","Step 21960: Loss: 0.2045, requires_grad: True\n","Step 21970: Loss: 0.1344, requires_grad: True\n","Step 21980: Loss: 0.2681, requires_grad: True\n","Step 21990: Loss: 0.2154, requires_grad: True\n","Step 22000: Loss: 0.1981, requires_grad: True\n","Step 22000: Gradients computed successfully\n","Step 22010: Loss: 0.2215, requires_grad: True\n","Step 22020: Loss: 0.3079, requires_grad: True\n","Step 22030: Loss: 0.1448, requires_grad: True\n","Step 22040: Loss: 0.2185, requires_grad: True\n","Step 22050: Loss: 0.2772, requires_grad: True\n","Step 22050: Gradients computed successfully\n","Step 22060: Loss: 0.3704, requires_grad: True\n","Step 22070: Loss: 0.2606, requires_grad: True\n","Step 22080: Loss: 0.4712, requires_grad: True\n","Step 22090: Loss: 0.1627, requires_grad: True\n","Step 22100: Loss: 0.1908, requires_grad: True\n","Step 22100: Gradients computed successfully\n","Step 22110: Loss: 0.3976, requires_grad: True\n","Step 22120: Loss: 0.3571, requires_grad: True\n","Step 22130: Loss: 0.3701, requires_grad: True\n","Step 22140: Loss: 0.0904, requires_grad: True\n","Step 22150: Loss: 0.1201, requires_grad: True\n","Step 22150: Gradients computed successfully\n","Step 22160: Loss: 0.2889, requires_grad: True\n","Step 22170: Loss: 0.2097, requires_grad: True\n","Step 22180: Loss: 0.1549, requires_grad: True\n","Step 22190: Loss: 0.3057, requires_grad: True\n","Step 22200: Loss: 0.2423, requires_grad: True\n","Step 22200: Gradients computed successfully\n","Step 22210: Loss: 0.1405, requires_grad: True\n","Step 22220: Loss: 0.2070, requires_grad: True\n","Step 22230: Loss: 0.3989, requires_grad: True\n","Step 22240: Loss: 0.3058, requires_grad: True\n","Step 22250: Loss: 0.2487, requires_grad: True\n","Step 22250: Gradients computed successfully\n","Step 22260: Loss: 0.2031, requires_grad: True\n","Step 22270: Loss: 0.1931, requires_grad: True\n","Step 22280: Loss: 0.0794, requires_grad: True\n","Step 22290: Loss: 0.1545, requires_grad: True\n","Step 22300: Loss: 0.0624, requires_grad: True\n","Step 22300: Gradients computed successfully\n","Step 22310: Loss: 0.1317, requires_grad: True\n","Step 22320: Loss: 0.2980, requires_grad: True\n","Step 22330: Loss: 0.1945, requires_grad: True\n","Step 22340: Loss: 0.2033, requires_grad: True\n","Step 22350: Loss: 0.1712, requires_grad: True\n","Step 22350: Gradients computed successfully\n","Step 22360: Loss: 0.2602, requires_grad: True\n","Step 22370: Loss: 0.3303, requires_grad: True\n","Step 22380: Loss: 0.1870, requires_grad: True\n","Step 22390: Loss: 0.4241, requires_grad: True\n","Step 22400: Loss: 0.2384, requires_grad: True\n","Step 22400: Gradients computed successfully\n","Step 22410: Loss: 0.1500, requires_grad: True\n","Step 22420: Loss: 0.2157, requires_grad: True\n","Step 22430: Loss: 0.1517, requires_grad: True\n","Step 22440: Loss: 0.1956, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-7200\n","Step 22450: Loss: 0.2694, requires_grad: True\n","Step 22450: Gradients computed successfully\n","Step 22460: Loss: 0.3762, requires_grad: True\n","Step 22470: Loss: 0.1018, requires_grad: True\n","Step 22480: Loss: 0.2320, requires_grad: True\n","Step 22490: Loss: 0.0751, requires_grad: True\n","Step 22500: Loss: 0.1371, requires_grad: True\n","Step 22500: Gradients computed successfully\n","Step 22510: Loss: 0.3233, requires_grad: True\n","Step 22520: Loss: 0.2240, requires_grad: True\n","Step 22530: Loss: 0.2964, requires_grad: True\n","Step 22540: Loss: 0.2689, requires_grad: True\n","Step 22550: Loss: 0.3140, requires_grad: True\n","Step 22550: Gradients computed successfully\n","Step 22560: Loss: 0.2549, requires_grad: True\n","Step 22570: Loss: 0.0574, requires_grad: True\n","Step 22580: Loss: 0.0927, requires_grad: True\n","Step 22590: Loss: 0.2108, requires_grad: True\n","Step 22600: Loss: 0.1373, requires_grad: True\n","Step 22600: Gradients computed successfully\n","Step 22610: Loss: 0.2610, requires_grad: True\n","Step 22620: Loss: 0.1454, requires_grad: True\n","Step 22630: Loss: 0.1986, requires_grad: True\n","Step 22640: Loss: 0.2912, requires_grad: True\n","Step 22650: Loss: 0.1733, requires_grad: True\n","Step 22650: Gradients computed successfully\n","Step 22660: Loss: 0.1990, requires_grad: True\n","Step 22670: Loss: 0.3145, requires_grad: True\n","Step 22680: Loss: 0.3769, requires_grad: True\n","Step 22690: Loss: 0.2489, requires_grad: True\n","Step 22700: Loss: 0.2958, requires_grad: True\n","Step 22700: Gradients computed successfully\n","Step 22710: Loss: 0.3872, requires_grad: True\n","Step 22720: Loss: 0.0822, requires_grad: True\n","Step 22730: Loss: 0.2348, requires_grad: True\n","Step 22740: Loss: 0.3039, requires_grad: True\n","Step 22750: Loss: 0.2821, requires_grad: True\n","Step 22750: Gradients computed successfully\n","Step 22760: Loss: 0.2407, requires_grad: True\n","Step 22770: Loss: 0.0591, requires_grad: True\n","Step 22780: Loss: 0.1591, requires_grad: True\n","Step 22790: Loss: 0.2564, requires_grad: True\n","Step 22800: Loss: 0.3440, requires_grad: True\n","Step 22800: Gradients computed successfully\n","Step 22810: Loss: 0.0846, requires_grad: True\n","Step 22820: Loss: 0.0901, requires_grad: True\n","Step 22830: Loss: 0.0606, requires_grad: True\n","Step 22840: Loss: 0.1624, requires_grad: True\n","Step 22850: Loss: 0.0795, requires_grad: True\n","Step 22850: Gradients computed successfully\n","Step 22860: Loss: 0.1213, requires_grad: True\n","Step 22870: Loss: 0.1398, requires_grad: True\n","Step 22880: Loss: 0.1228, requires_grad: True\n","Step 22890: Loss: 0.1192, requires_grad: True\n","Step 22900: Loss: 0.3860, requires_grad: True\n","Step 22900: Gradients computed successfully\n","Step 22910: Loss: 0.2161, requires_grad: True\n","Step 22920: Loss: 0.1582, requires_grad: True\n","Step 22930: Loss: 0.2119, requires_grad: True\n","Step 22940: Loss: 0.1037, requires_grad: True\n","Step 22950: Loss: 0.0942, requires_grad: True\n","Step 22950: Gradients computed successfully\n","Step 22960: Loss: 0.0894, requires_grad: True\n","Step 22970: Loss: 0.4547, requires_grad: True\n","Step 22980: Loss: 0.4441, requires_grad: True\n","Step 22990: Loss: 0.1511, requires_grad: True\n","Step 23000: Loss: 0.0779, requires_grad: True\n","Step 23000: Gradients computed successfully\n","Step 23010: Loss: 0.0819, requires_grad: True\n","Step 23020: Loss: 0.2346, requires_grad: True\n","Step 23030: Loss: 0.1881, requires_grad: True\n","Step 23040: Loss: 0.1661, requires_grad: True\n","Step 23050: Loss: 0.2460, requires_grad: True\n","Step 23050: Gradients computed successfully\n","Step 23060: Loss: 0.1424, requires_grad: True\n","Step 23070: Loss: 0.3792, requires_grad: True\n","Step 23080: Loss: 0.1652, requires_grad: True\n","Step 23090: Loss: 0.4990, requires_grad: True\n","Step 23100: Loss: 0.3036, requires_grad: True\n","Step 23100: Gradients computed successfully\n","Step 23110: Loss: 0.1011, requires_grad: True\n","Step 23120: Loss: 0.2318, requires_grad: True\n","Step 23130: Loss: 0.2816, requires_grad: True\n","Step 23140: Loss: 0.0362, requires_grad: True\n","Step 23150: Loss: 0.2018, requires_grad: True\n","Step 23150: Gradients computed successfully\n","Step 23160: Loss: 0.3292, requires_grad: True\n","Step 23170: Loss: 0.3638, requires_grad: True\n","Step 23180: Loss: 0.3060, requires_grad: True\n","Step 23190: Loss: 0.2141, requires_grad: True\n","Step 23200: Loss: 0.1672, requires_grad: True\n","Step 23200: Gradients computed successfully\n","Step 23210: Loss: 0.2265, requires_grad: True\n","Step 23220: Loss: 0.3867, requires_grad: True\n","Step 23230: Loss: 0.3287, requires_grad: True\n","Step 23240: Loss: 0.1818, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-7300\n","Step 23250: Loss: 0.0996, requires_grad: True\n","Step 23250: Gradients computed successfully\n","Step 23260: Loss: 0.1090, requires_grad: True\n","Step 23270: Loss: 0.2951, requires_grad: True\n","Step 23280: Loss: 0.1383, requires_grad: True\n","Step 23290: Loss: 0.1296, requires_grad: True\n","Step 23300: Loss: 0.4104, requires_grad: True\n","Step 23300: Gradients computed successfully\n","Step 23310: Loss: 0.2685, requires_grad: True\n","Step 23320: Loss: 0.1164, requires_grad: True\n","Step 23330: Loss: 0.1080, requires_grad: True\n","Step 23340: Loss: 0.3427, requires_grad: True\n","Step 23350: Loss: 0.1444, requires_grad: True\n","Step 23350: Gradients computed successfully\n","Step 23360: Loss: 0.3933, requires_grad: True\n","Step 23370: Loss: 0.1916, requires_grad: True\n","Step 23380: Loss: 0.2588, requires_grad: True\n","Step 23390: Loss: 0.2046, requires_grad: True\n","Step 23400: Loss: 0.1331, requires_grad: True\n","Step 23400: Gradients computed successfully\n","Step 23410: Loss: 0.4582, requires_grad: True\n","Step 23420: Loss: 0.2166, requires_grad: True\n","Step 23430: Loss: 0.1178, requires_grad: True\n","Step 23440: Loss: 0.1106, requires_grad: True\n","Step 23450: Loss: 0.4112, requires_grad: True\n","Step 23450: Gradients computed successfully\n","Step 23460: Loss: 0.2433, requires_grad: True\n","Step 23470: Loss: 0.1311, requires_grad: True\n","Step 23480: Loss: 0.2949, requires_grad: True\n","Step 23490: Loss: 0.2182, requires_grad: True\n","Step 23500: Loss: 0.0737, requires_grad: True\n","Step 23500: Gradients computed successfully\n","Step 23510: Loss: 0.1350, requires_grad: True\n","Step 23520: Loss: 0.2825, requires_grad: True\n","Step 23530: Loss: 0.6207, requires_grad: True\n","Step 23540: Loss: 0.1184, requires_grad: True\n","Step 23550: Loss: 0.1170, requires_grad: True\n","Step 23550: Gradients computed successfully\n","Step 23560: Loss: 0.1726, requires_grad: True\n","Step 23570: Loss: 0.2324, requires_grad: True\n","Step 23580: Loss: 0.1887, requires_grad: True\n","Step 23590: Loss: 0.2650, requires_grad: True\n","Step 23600: Loss: 0.2085, requires_grad: True\n","Step 23600: Gradients computed successfully\n","Step 23610: Loss: 0.2571, requires_grad: True\n","Step 23620: Loss: 0.2115, requires_grad: True\n","Step 23630: Loss: 0.3588, requires_grad: True\n","Step 23640: Loss: 0.1325, requires_grad: True\n","Step 23650: Loss: 0.1691, requires_grad: True\n","Step 23650: Gradients computed successfully\n","Step 23660: Loss: 0.2583, requires_grad: True\n","Step 23670: Loss: 0.1537, requires_grad: True\n","Step 23680: Loss: 0.1367, requires_grad: True\n","Step 23690: Loss: 0.1717, requires_grad: True\n","Step 23700: Loss: 0.1267, requires_grad: True\n","Step 23700: Gradients computed successfully\n","Step 23710: Loss: 0.1524, requires_grad: True\n","Step 23720: Loss: 0.1851, requires_grad: True\n","Step 23730: Loss: 0.3782, requires_grad: True\n","Step 23740: Loss: 0.3654, requires_grad: True\n","Step 23750: Loss: 0.2789, requires_grad: True\n","Step 23750: Gradients computed successfully\n","Step 23760: Loss: 0.3474, requires_grad: True\n","Step 23770: Loss: 0.1167, requires_grad: True\n","Step 23780: Loss: 0.2608, requires_grad: True\n","Step 23790: Loss: 0.4236, requires_grad: True\n","Step 23800: Loss: 0.3024, requires_grad: True\n","Step 23800: Gradients computed successfully\n","Step 23810: Loss: 0.1707, requires_grad: True\n","Step 23820: Loss: 0.2474, requires_grad: True\n","Step 23830: Loss: 0.1852, requires_grad: True\n","Step 23840: Loss: 0.1247, requires_grad: True\n","Step 23850: Loss: 0.1609, requires_grad: True\n","Step 23850: Gradients computed successfully\n","Step 23860: Loss: 0.1052, requires_grad: True\n","Step 23870: Loss: 0.3329, requires_grad: True\n","Step 23880: Loss: 0.1812, requires_grad: True\n","Step 23890: Loss: 0.1420, requires_grad: True\n","Step 23900: Loss: 0.4487, requires_grad: True\n","Step 23900: Gradients computed successfully\n","Step 23910: Loss: 0.1374, requires_grad: True\n","Step 23920: Loss: 0.0594, requires_grad: True\n","Step 23930: Loss: 0.2654, requires_grad: True\n","Step 23940: Loss: 0.3108, requires_grad: True\n","Step 23950: Loss: 0.3768, requires_grad: True\n","Step 23950: Gradients computed successfully\n","Step 23960: Loss: 0.3361, requires_grad: True\n","Step 23970: Loss: 0.1901, requires_grad: True\n","Step 23980: Loss: 0.1055, requires_grad: True\n","Step 23990: Loss: 0.2187, requires_grad: True\n","Step 24000: Loss: 0.1265, requires_grad: True\n","Step 24000: Gradients computed successfully\n","Step 24010: Loss: 0.1170, requires_grad: True\n","Step 24020: Loss: 0.3873, requires_grad: True\n","Step 24030: Loss: 0.2104, requires_grad: True\n","Step 24040: Loss: 0.2141, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-7400\n","Step 24050: Loss: 0.1202, requires_grad: True\n","Step 24050: Gradients computed successfully\n","Step 24060: Loss: 0.3304, requires_grad: True\n","Step 24070: Loss: 0.2699, requires_grad: True\n","Step 24080: Loss: 0.1829, requires_grad: True\n","Step 24090: Loss: 0.2165, requires_grad: True\n","Step 24100: Loss: 0.0865, requires_grad: True\n","Step 24100: Gradients computed successfully\n","Step 24110: Loss: 0.0574, requires_grad: True\n","Step 24120: Loss: 0.0826, requires_grad: True\n","Step 24130: Loss: 0.0993, requires_grad: True\n","Step 24140: Loss: 0.0827, requires_grad: True\n","Step 24150: Loss: 0.0955, requires_grad: True\n","Step 24150: Gradients computed successfully\n","Step 24160: Loss: 0.2825, requires_grad: True\n","Step 24170: Loss: 0.1537, requires_grad: True\n","Step 24180: Loss: 0.2192, requires_grad: True\n","Step 24190: Loss: 0.1319, requires_grad: True\n","Step 24200: Loss: 0.2404, requires_grad: True\n","Step 24200: Gradients computed successfully\n","Step 24210: Loss: 0.0799, requires_grad: True\n","Step 24220: Loss: 0.0996, requires_grad: True\n","Step 24230: Loss: 0.2091, requires_grad: True\n","Step 24240: Loss: 0.2095, requires_grad: True\n","Step 24250: Loss: 0.1145, requires_grad: True\n","Step 24250: Gradients computed successfully\n","Step 24260: Loss: 0.0554, requires_grad: True\n","Step 24270: Loss: 0.1578, requires_grad: True\n","Step 24280: Loss: 0.2671, requires_grad: True\n","Step 24290: Loss: 0.0849, requires_grad: True\n","Step 24300: Loss: 0.2019, requires_grad: True\n","Step 24300: Gradients computed successfully\n","Step 24310: Loss: 0.3545, requires_grad: True\n","Step 24320: Loss: 0.1467, requires_grad: True\n","Step 24330: Loss: 0.1868, requires_grad: True\n","Step 24340: Loss: 0.3157, requires_grad: True\n","Step 24350: Loss: 0.2988, requires_grad: True\n","Step 24350: Gradients computed successfully\n","Step 24360: Loss: 0.1638, requires_grad: True\n","Step 24370: Loss: 0.2617, requires_grad: True\n","Step 24380: Loss: 0.2867, requires_grad: True\n","Step 24390: Loss: 0.1673, requires_grad: True\n","Step 24400: Loss: 0.6028, requires_grad: True\n","Step 24400: Gradients computed successfully\n","Step 24410: Loss: 0.4296, requires_grad: True\n","Step 24420: Loss: 0.3565, requires_grad: True\n","Step 24430: Loss: 0.3617, requires_grad: True\n","Step 24440: Loss: 0.0714, requires_grad: True\n","Step 24450: Loss: 0.1872, requires_grad: True\n","Step 24450: Gradients computed successfully\n","Step 24460: Loss: 0.1109, requires_grad: True\n","Step 24470: Loss: 0.4242, requires_grad: True\n","Step 24480: Loss: 0.0434, requires_grad: True\n","Step 24490: Loss: 0.1087, requires_grad: True\n","Step 24500: Loss: 0.1062, requires_grad: True\n","Step 24500: Gradients computed successfully\n","Step 24510: Loss: 0.1611, requires_grad: True\n","Step 24520: Loss: 0.2014, requires_grad: True\n","Step 24530: Loss: 0.1707, requires_grad: True\n","Step 24540: Loss: 0.0620, requires_grad: True\n","Step 24550: Loss: 0.1468, requires_grad: True\n","Step 24550: Gradients computed successfully\n","Step 24560: Loss: 0.0993, requires_grad: True\n","Step 24570: Loss: 0.0968, requires_grad: True\n","Step 24580: Loss: 0.0805, requires_grad: True\n","Step 24590: Loss: 0.1943, requires_grad: True\n","Step 24600: Loss: 0.2296, requires_grad: True\n","Step 24600: Gradients computed successfully\n","Step 24610: Loss: 0.3740, requires_grad: True\n","Step 24620: Loss: 0.1432, requires_grad: True\n","Step 24630: Loss: 0.1263, requires_grad: True\n","Step 24640: Loss: 0.3293, requires_grad: True\n","Step 24650: Loss: 0.1898, requires_grad: True\n","Step 24650: Gradients computed successfully\n","Step 24660: Loss: 0.3073, requires_grad: True\n","Step 24670: Loss: 0.2274, requires_grad: True\n","Step 24680: Loss: 0.0662, requires_grad: True\n","Step 24690: Loss: 0.2793, requires_grad: True\n","Step 24700: Loss: 0.2374, requires_grad: True\n","Step 24700: Gradients computed successfully\n","Step 24710: Loss: 0.1011, requires_grad: True\n","Step 24720: Loss: 0.0808, requires_grad: True\n","Step 24730: Loss: 0.2448, requires_grad: True\n","Step 24740: Loss: 0.1808, requires_grad: True\n","Step 24750: Loss: 0.4654, requires_grad: True\n","Step 24750: Gradients computed successfully\n","Step 24760: Loss: 0.2623, requires_grad: True\n","Step 24770: Loss: 0.3009, requires_grad: True\n","Step 24780: Loss: 0.1055, requires_grad: True\n","Step 24790: Loss: 0.1204, requires_grad: True\n","Step 24800: Loss: 0.2312, requires_grad: True\n","Step 24800: Gradients computed successfully\n","Step 24810: Loss: 0.1654, requires_grad: True\n","Step 24820: Loss: 0.2441, requires_grad: True\n","Step 24830: Loss: 0.1991, requires_grad: True\n","Step 24840: Loss: 0.2275, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-7500\n","Step 24850: Loss: 0.1420, requires_grad: True\n","Step 24850: Gradients computed successfully\n","Step 24860: Loss: 0.1214, requires_grad: True\n","Step 24870: Loss: 0.1070, requires_grad: True\n","Step 24880: Loss: 0.1620, requires_grad: True\n","Step 24890: Loss: 0.3310, requires_grad: True\n","Step 24900: Loss: 0.1778, requires_grad: True\n","Step 24900: Gradients computed successfully\n","Step 24910: Loss: 0.4158, requires_grad: True\n","Step 24920: Loss: 0.1800, requires_grad: True\n","Step 24930: Loss: 0.2236, requires_grad: True\n","Step 24940: Loss: 0.1892, requires_grad: True\n","Step 24950: Loss: 0.3873, requires_grad: True\n","Step 24950: Gradients computed successfully\n","Step 24960: Loss: 0.1727, requires_grad: True\n","Step 24970: Loss: 0.0721, requires_grad: True\n","Step 24980: Loss: 0.0784, requires_grad: True\n","Step 24990: Loss: 0.0774, requires_grad: True\n","Step 25000: Loss: 0.3155, requires_grad: True\n","Step 25000: Gradients computed successfully\n","Step 25010: Loss: 0.3235, requires_grad: True\n","Step 25020: Loss: 0.0869, requires_grad: True\n","Step 25030: Loss: 0.0891, requires_grad: True\n","Step 25040: Loss: 0.0892, requires_grad: True\n","Step 25050: Loss: 0.1244, requires_grad: True\n","Step 25050: Gradients computed successfully\n","Step 25060: Loss: 0.1200, requires_grad: True\n","Step 25070: Loss: 0.4235, requires_grad: True\n","Step 25080: Loss: 0.3038, requires_grad: True\n","Step 25090: Loss: 0.2351, requires_grad: True\n","Step 25100: Loss: 0.2547, requires_grad: True\n","Step 25100: Gradients computed successfully\n","Step 25110: Loss: 0.5420, requires_grad: True\n","Step 25120: Loss: 0.0753, requires_grad: True\n","Step 25130: Loss: 0.2769, requires_grad: True\n","Step 25140: Loss: 0.1753, requires_grad: True\n","Step 25150: Loss: 0.1831, requires_grad: True\n","Step 25150: Gradients computed successfully\n","Step 25160: Loss: 0.6484, requires_grad: True\n","Step 25170: Loss: 0.1792, requires_grad: True\n","Step 25180: Loss: 0.3272, requires_grad: True\n","Step 25190: Loss: 0.1808, requires_grad: True\n","Step 25200: Loss: 0.1779, requires_grad: True\n","Step 25200: Gradients computed successfully\n","Step 25210: Loss: 0.4155, requires_grad: True\n","Step 25220: Loss: 0.2853, requires_grad: True\n","Step 25230: Loss: 0.1354, requires_grad: True\n","Step 25240: Loss: 0.1681, requires_grad: True\n","Step 25250: Loss: 0.2015, requires_grad: True\n","Step 25250: Gradients computed successfully\n","Step 25260: Loss: 0.2502, requires_grad: True\n","Step 25270: Loss: 0.3088, requires_grad: True\n","Step 25280: Loss: 0.2092, requires_grad: True\n","Step 25290: Loss: 0.2045, requires_grad: True\n","Step 25300: Loss: 0.2489, requires_grad: True\n","Step 25300: Gradients computed successfully\n","Step 25310: Loss: 0.1765, requires_grad: True\n","Step 25320: Loss: 0.2536, requires_grad: True\n","Step 25330: Loss: 0.1972, requires_grad: True\n","Step 25340: Loss: 0.1638, requires_grad: True\n","Step 25350: Loss: 0.2150, requires_grad: True\n","Step 25350: Gradients computed successfully\n","Step 25360: Loss: 0.0598, requires_grad: True\n","Step 25370: Loss: 0.2417, requires_grad: True\n","Step 25380: Loss: 0.1340, requires_grad: True\n","Step 25390: Loss: 0.0939, requires_grad: True\n","Step 25400: Loss: 0.1129, requires_grad: True\n","Step 25400: Gradients computed successfully\n","Step 25410: Loss: 0.1187, requires_grad: True\n","Step 25420: Loss: 0.3540, requires_grad: True\n","Step 25430: Loss: 0.1521, requires_grad: True\n","Step 25440: Loss: 0.2760, requires_grad: True\n","Step 25450: Loss: 0.1285, requires_grad: True\n","Step 25450: Gradients computed successfully\n","Step 25460: Loss: 0.1288, requires_grad: True\n","Step 25470: Loss: 0.1530, requires_grad: True\n","Step 25480: Loss: 0.1395, requires_grad: True\n","Step 25490: Loss: 0.1332, requires_grad: True\n","Step 25500: Loss: 0.3043, requires_grad: True\n","Step 25500: Gradients computed successfully\n","Step 25510: Loss: 0.2784, requires_grad: True\n","Step 25520: Loss: 0.2156, requires_grad: True\n","Step 25530: Loss: 0.1662, requires_grad: True\n","Step 25540: Loss: 0.2390, requires_grad: True\n","Step 25550: Loss: 0.1453, requires_grad: True\n","Step 25550: Gradients computed successfully\n","Step 25560: Loss: 0.4123, requires_grad: True\n","Step 25570: Loss: 0.3003, requires_grad: True\n","Step 25580: Loss: 0.0769, requires_grad: True\n","Step 25590: Loss: 0.1073, requires_grad: True\n","Step 25600: Loss: 0.0861, requires_grad: True\n","Step 25600: Gradients computed successfully\n","Step 25610: Loss: 0.1100, requires_grad: True\n","Step 25620: Loss: 0.1157, requires_grad: True\n","Step 25630: Loss: 0.3152, requires_grad: True\n","Step 25640: Loss: 0.2401, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-7600\n","Step 25650: Loss: 0.2313, requires_grad: True\n","Step 25650: Gradients computed successfully\n","Step 25660: Loss: 0.3745, requires_grad: True\n","Step 25670: Loss: 0.1075, requires_grad: True\n","Step 25680: Loss: 0.2668, requires_grad: True\n","Step 25690: Loss: 0.2947, requires_grad: True\n","Step 25700: Loss: 0.1253, requires_grad: True\n","Step 25700: Gradients computed successfully\n","Step 25710: Loss: 0.3225, requires_grad: True\n","Step 25720: Loss: 0.2539, requires_grad: True\n","Step 25730: Loss: 0.1474, requires_grad: True\n","Step 25740: Loss: 0.1205, requires_grad: True\n","Step 25750: Loss: 0.4254, requires_grad: True\n","Step 25750: Gradients computed successfully\n","Step 25760: Loss: 0.3445, requires_grad: True\n","Step 25770: Loss: 0.2072, requires_grad: True\n","Step 25780: Loss: 0.1698, requires_grad: True\n","Step 25790: Loss: 0.2799, requires_grad: True\n","Step 25800: Loss: 0.1314, requires_grad: True\n","Step 25800: Gradients computed successfully\n","Step 25810: Loss: 0.2140, requires_grad: True\n","Step 25820: Loss: 0.0935, requires_grad: True\n","Step 25830: Loss: 0.1865, requires_grad: True\n","Step 25840: Loss: 0.3781, requires_grad: True\n","Step 25850: Loss: 0.1373, requires_grad: True\n","Step 25850: Gradients computed successfully\n","Step 25860: Loss: 0.0601, requires_grad: True\n","Step 25870: Loss: 0.3406, requires_grad: True\n","Step 25880: Loss: 0.1450, requires_grad: True\n","Step 25890: Loss: 0.0971, requires_grad: True\n","Step 25900: Loss: 0.2770, requires_grad: True\n","Step 25900: Gradients computed successfully\n","Step 25910: Loss: 0.1562, requires_grad: True\n","Step 25920: Loss: 0.1446, requires_grad: True\n","Step 25930: Loss: 0.3894, requires_grad: True\n","Step 25940: Loss: 0.1033, requires_grad: True\n","Step 25950: Loss: 0.1956, requires_grad: True\n","Step 25950: Gradients computed successfully\n","Step 25960: Loss: 0.1121, requires_grad: True\n","Step 25970: Loss: 0.1988, requires_grad: True\n","Step 25980: Loss: 0.2326, requires_grad: True\n","Step 25990: Loss: 0.2227, requires_grad: True\n","Step 26000: Loss: 0.5600, requires_grad: True\n","Step 26000: Gradients computed successfully\n","Step 26010: Loss: 0.1889, requires_grad: True\n","Step 26020: Loss: 0.4689, requires_grad: True\n","Step 26030: Loss: 0.3646, requires_grad: True\n","Step 26040: Loss: 0.1806, requires_grad: True\n","Step 26050: Loss: 0.0619, requires_grad: True\n","Step 26050: Gradients computed successfully\n","Step 26060: Loss: 0.2411, requires_grad: True\n","Step 26070: Loss: 0.1476, requires_grad: True\n","Step 26080: Loss: 0.6378, requires_grad: True\n","Step 26090: Loss: 0.1612, requires_grad: True\n","Step 26100: Loss: 0.1998, requires_grad: True\n","Step 26100: Gradients computed successfully\n","Step 26110: Loss: 0.4578, requires_grad: True\n","Step 26120: Loss: 0.7421, requires_grad: True\n","Step 26130: Loss: 0.1077, requires_grad: True\n","Step 26140: Loss: 0.3839, requires_grad: True\n","Step 26150: Loss: 0.2985, requires_grad: True\n","Step 26150: Gradients computed successfully\n","Step 26160: Loss: 0.1839, requires_grad: True\n","Step 26170: Loss: 0.2719, requires_grad: True\n","Step 26180: Loss: 0.4155, requires_grad: True\n","Step 26190: Loss: 0.4466, requires_grad: True\n","Step 26200: Loss: 0.0860, requires_grad: True\n","Step 26200: Gradients computed successfully\n","Step 26210: Loss: 0.1291, requires_grad: True\n","Step 26220: Loss: 0.5084, requires_grad: True\n","Step 26230: Loss: 0.3750, requires_grad: True\n","Step 26240: Loss: 0.1745, requires_grad: True\n","Step 26250: Loss: 0.1994, requires_grad: True\n","Step 26250: Gradients computed successfully\n","Step 26260: Loss: 0.2127, requires_grad: True\n","Step 26270: Loss: 0.1214, requires_grad: True\n","Step 26280: Loss: 0.0925, requires_grad: True\n","Step 26290: Loss: 0.2247, requires_grad: True\n","Step 26300: Loss: 0.1510, requires_grad: True\n","Step 26300: Gradients computed successfully\n","Step 26310: Loss: 0.1750, requires_grad: True\n","Step 26320: Loss: 0.2067, requires_grad: True\n","Step 26330: Loss: 0.1150, requires_grad: True\n","Step 26340: Loss: 0.3302, requires_grad: True\n","Step 26350: Loss: 0.2668, requires_grad: True\n","Step 26350: Gradients computed successfully\n","Step 26360: Loss: 0.1921, requires_grad: True\n","Step 26370: Loss: 0.3570, requires_grad: True\n","Step 26380: Loss: 0.1673, requires_grad: True\n","Step 26390: Loss: 0.2362, requires_grad: True\n","Step 26400: Loss: 0.3808, requires_grad: True\n","Step 26400: Gradients computed successfully\n","Step 26410: Loss: 0.1357, requires_grad: True\n","Step 26420: Loss: 0.3119, requires_grad: True\n","Step 26430: Loss: 0.3645, requires_grad: True\n","Step 26440: Loss: 0.1593, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-7700\n","Step 26450: Loss: 0.3432, requires_grad: True\n","Step 26450: Gradients computed successfully\n","Step 26460: Loss: 0.1741, requires_grad: True\n","Step 26470: Loss: 0.1020, requires_grad: True\n","Step 26480: Loss: 0.2026, requires_grad: True\n","Step 26490: Loss: 0.1527, requires_grad: True\n","Step 26500: Loss: 0.1984, requires_grad: True\n","Step 26500: Gradients computed successfully\n","Step 26510: Loss: 0.1538, requires_grad: True\n","Step 26520: Loss: 0.1074, requires_grad: True\n","Step 26530: Loss: 0.2501, requires_grad: True\n","Step 26540: Loss: 0.2218, requires_grad: True\n","Step 26550: Loss: 0.2294, requires_grad: True\n","Step 26550: Gradients computed successfully\n","Step 26560: Loss: 0.1006, requires_grad: True\n","Step 26570: Loss: 0.5015, requires_grad: True\n","Step 26580: Loss: 0.2631, requires_grad: True\n","Step 26590: Loss: 0.3036, requires_grad: True\n","Step 26600: Loss: 0.5309, requires_grad: True\n","Step 26600: Gradients computed successfully\n","Step 26610: Loss: 0.2380, requires_grad: True\n","Step 26620: Loss: 0.0572, requires_grad: True\n","Step 26630: Loss: 0.2674, requires_grad: True\n","Step 26640: Loss: 0.0930, requires_grad: True\n","Step 26650: Loss: 0.0454, requires_grad: True\n","Step 26650: Gradients computed successfully\n","Step 26660: Loss: 0.1200, requires_grad: True\n","Step 26670: Loss: 0.2678, requires_grad: True\n","Step 26680: Loss: 0.0935, requires_grad: True\n","Step 26690: Loss: 0.1752, requires_grad: True\n","Step 26700: Loss: 0.1155, requires_grad: True\n","Step 26700: Gradients computed successfully\n","Step 26710: Loss: 0.2989, requires_grad: True\n","Step 26720: Loss: 0.1775, requires_grad: True\n","Step 26730: Loss: 0.2158, requires_grad: True\n","Step 26740: Loss: 0.1489, requires_grad: True\n","Step 26750: Loss: 0.2792, requires_grad: True\n","Step 26750: Gradients computed successfully\n","Step 26760: Loss: 0.1238, requires_grad: True\n","Step 26770: Loss: 0.1126, requires_grad: True\n","Step 26780: Loss: 0.1644, requires_grad: True\n","Step 26790: Loss: 0.3674, requires_grad: True\n","Step 26800: Loss: 0.2417, requires_grad: True\n","Step 26800: Gradients computed successfully\n","Step 26810: Loss: 0.4014, requires_grad: True\n","Step 26820: Loss: 0.1655, requires_grad: True\n","Step 26830: Loss: 0.1232, requires_grad: True\n","Step 26840: Loss: 0.3025, requires_grad: True\n","Step 26850: Loss: 0.2901, requires_grad: True\n","Step 26850: Gradients computed successfully\n","Step 26860: Loss: 0.1753, requires_grad: True\n","Step 26870: Loss: 0.2236, requires_grad: True\n","Step 26880: Loss: 0.1381, requires_grad: True\n","Step 26890: Loss: 0.0939, requires_grad: True\n","Step 26900: Loss: 0.2913, requires_grad: True\n","Step 26900: Gradients computed successfully\n","Step 26910: Loss: 0.2287, requires_grad: True\n","Step 26920: Loss: 0.2313, requires_grad: True\n","Step 26930: Loss: 0.3178, requires_grad: True\n","Step 26940: Loss: 0.1242, requires_grad: True\n","Step 26950: Loss: 0.1993, requires_grad: True\n","Step 26950: Gradients computed successfully\n","Step 26960: Loss: 0.0983, requires_grad: True\n","Step 26970: Loss: 0.0795, requires_grad: True\n","Step 26980: Loss: 0.7954, requires_grad: True\n","Step 26990: Loss: 0.1478, requires_grad: True\n","Step 27000: Loss: 0.1179, requires_grad: True\n","Step 27000: Gradients computed successfully\n","Step 27010: Loss: 0.3371, requires_grad: True\n","Step 27020: Loss: 0.1044, requires_grad: True\n","Step 27030: Loss: 0.1762, requires_grad: True\n","Step 27040: Loss: 0.0818, requires_grad: True\n","Step 27050: Loss: 0.1443, requires_grad: True\n","Step 27050: Gradients computed successfully\n","Step 27060: Loss: 0.1837, requires_grad: True\n","Step 27070: Loss: 0.5636, requires_grad: True\n","Step 27080: Loss: 0.2098, requires_grad: True\n","Step 27090: Loss: 0.1530, requires_grad: True\n","Step 27100: Loss: 0.2884, requires_grad: True\n","Step 27100: Gradients computed successfully\n","Step 27110: Loss: 0.1854, requires_grad: True\n","Step 27120: Loss: 0.2357, requires_grad: True\n","Step 27130: Loss: 0.2562, requires_grad: True\n","Step 27140: Loss: 0.6839, requires_grad: True\n","Step 27150: Loss: 0.4930, requires_grad: True\n","Step 27150: Gradients computed successfully\n","Step 27160: Loss: 0.1744, requires_grad: True\n","Step 27170: Loss: 0.3663, requires_grad: True\n","Step 27180: Loss: 0.3903, requires_grad: True\n","Step 27190: Loss: 0.3948, requires_grad: True\n","Step 27200: Loss: 0.1280, requires_grad: True\n","Step 27200: Gradients computed successfully\n","Step 27210: Loss: 0.1703, requires_grad: True\n","Step 27220: Loss: 0.1746, requires_grad: True\n","Step 27230: Loss: 0.0413, requires_grad: True\n","Step 27240: Loss: 0.1348, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-7800\n","Step 27250: Loss: 0.1150, requires_grad: True\n","Step 27250: Gradients computed successfully\n","Step 27260: Loss: 0.1352, requires_grad: True\n","Step 27270: Loss: 0.1175, requires_grad: True\n","Step 27280: Loss: 0.1001, requires_grad: True\n","Step 27290: Loss: 0.2564, requires_grad: True\n","Step 27300: Loss: 0.2943, requires_grad: True\n","Step 27300: Gradients computed successfully\n","Step 27310: Loss: 0.2173, requires_grad: True\n","Step 27320: Loss: 0.0940, requires_grad: True\n","Step 27330: Loss: 0.0831, requires_grad: True\n","Step 27340: Loss: 0.1380, requires_grad: True\n","Step 27350: Loss: 0.1058, requires_grad: True\n","Step 27350: Gradients computed successfully\n","Step 27360: Loss: 0.1749, requires_grad: True\n","Step 27370: Loss: 0.4596, requires_grad: True\n","Step 27380: Loss: 0.2696, requires_grad: True\n","Step 27390: Loss: 0.0673, requires_grad: True\n","Step 27400: Loss: 0.1486, requires_grad: True\n","Step 27400: Gradients computed successfully\n","Step 27410: Loss: 0.2423, requires_grad: True\n","Step 27420: Loss: 0.5621, requires_grad: True\n","Step 27430: Loss: 0.2139, requires_grad: True\n","Step 27440: Loss: 0.3545, requires_grad: True\n","Step 27450: Loss: 0.1908, requires_grad: True\n","Step 27450: Gradients computed successfully\n","Step 27460: Loss: 0.0908, requires_grad: True\n","Step 27470: Loss: 0.0651, requires_grad: True\n","Step 27480: Loss: 0.1275, requires_grad: True\n","Step 27490: Loss: 0.1514, requires_grad: True\n","Step 27500: Loss: 0.2459, requires_grad: True\n","Step 27500: Gradients computed successfully\n","Step 27510: Loss: 0.2467, requires_grad: True\n","Step 27520: Loss: 0.2078, requires_grad: True\n","Step 27530: Loss: 0.0979, requires_grad: True\n","Step 27540: Loss: 0.0728, requires_grad: True\n","Step 27550: Loss: 0.2662, requires_grad: True\n","Step 27550: Gradients computed successfully\n","Step 27560: Loss: 0.1678, requires_grad: True\n","Step 27570: Loss: 0.7284, requires_grad: True\n","Step 27580: Loss: 0.0923, requires_grad: True\n","Step 27590: Loss: 0.3769, requires_grad: True\n","Step 27600: Loss: 0.2617, requires_grad: True\n","Step 27600: Gradients computed successfully\n","Step 27610: Loss: 0.2905, requires_grad: True\n","Step 27620: Loss: 0.1086, requires_grad: True\n","Step 27630: Loss: 0.1327, requires_grad: True\n","Step 27640: Loss: 0.2377, requires_grad: True\n","Step 27650: Loss: 0.1235, requires_grad: True\n","Step 27650: Gradients computed successfully\n","Step 27660: Loss: 0.2732, requires_grad: True\n","Step 27670: Loss: 0.2625, requires_grad: True\n","Step 27680: Loss: 0.3646, requires_grad: True\n","Step 27690: Loss: 0.4324, requires_grad: True\n","Step 27700: Loss: 0.1296, requires_grad: True\n","Step 27700: Gradients computed successfully\n","Step 27710: Loss: 0.0921, requires_grad: True\n","Step 27720: Loss: 0.1222, requires_grad: True\n","Step 27730: Loss: 0.1111, requires_grad: True\n","Step 27740: Loss: 0.2141, requires_grad: True\n","Step 27750: Loss: 0.3363, requires_grad: True\n","Step 27750: Gradients computed successfully\n","Step 27760: Loss: 0.1005, requires_grad: True\n","Step 27770: Loss: 0.3169, requires_grad: True\n","Step 27780: Loss: 0.1446, requires_grad: True\n","Step 27790: Loss: 0.2298, requires_grad: True\n","Step 27800: Loss: 0.1713, requires_grad: True\n","Step 27800: Gradients computed successfully\n","Step 27810: Loss: 0.1282, requires_grad: True\n","Step 27820: Loss: 0.1314, requires_grad: True\n","Step 27830: Loss: 0.1288, requires_grad: True\n","Step 27840: Loss: 0.0855, requires_grad: True\n","Step 27850: Loss: 0.2068, requires_grad: True\n","Step 27850: Gradients computed successfully\n","Step 27860: Loss: 0.2593, requires_grad: True\n","Step 27870: Loss: 0.0586, requires_grad: True\n","Step 27880: Loss: 0.1572, requires_grad: True\n","Step 27890: Loss: 0.1844, requires_grad: True\n","Step 27900: Loss: 0.0757, requires_grad: True\n","Step 27900: Gradients computed successfully\n","Step 27910: Loss: 0.0865, requires_grad: True\n","Step 27920: Loss: 0.6635, requires_grad: True\n","Step 27930: Loss: 0.3955, requires_grad: True\n","Step 27940: Loss: 0.3515, requires_grad: True\n","Step 27950: Loss: 0.4111, requires_grad: True\n","Step 27950: Gradients computed successfully\n","Step 27960: Loss: 0.1918, requires_grad: True\n","Step 27970: Loss: 0.1681, requires_grad: True\n","Step 27980: Loss: 0.2439, requires_grad: True\n","Step 27990: Loss: 0.2860, requires_grad: True\n","Step 28000: Loss: 0.1506, requires_grad: True\n","Step 28000: Gradients computed successfully\n","Step 28010: Loss: 0.1579, requires_grad: True\n","Step 28020: Loss: 0.1676, requires_grad: True\n","Step 28030: Loss: 0.2192, requires_grad: True\n","Step 28040: Loss: 0.0831, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-7900\n","Step 28050: Loss: 0.3017, requires_grad: True\n","Step 28050: Gradients computed successfully\n","Step 28060: Loss: 0.1996, requires_grad: True\n","Step 28070: Loss: 0.3302, requires_grad: True\n","Step 28080: Loss: 0.3641, requires_grad: True\n","Step 28090: Loss: 0.1348, requires_grad: True\n","Step 28100: Loss: 0.0539, requires_grad: True\n","Step 28100: Gradients computed successfully\n","Step 28110: Loss: 0.1151, requires_grad: True\n","Step 28120: Loss: 0.4663, requires_grad: True\n","Step 28130: Loss: 0.4128, requires_grad: True\n","Step 28140: Loss: 0.0795, requires_grad: True\n","Step 28150: Loss: 0.0791, requires_grad: True\n","Step 28150: Gradients computed successfully\n","Step 28160: Loss: 0.4877, requires_grad: True\n","Step 28170: Loss: 0.1036, requires_grad: True\n","Step 28180: Loss: 0.0732, requires_grad: True\n","Step 28190: Loss: 0.4571, requires_grad: True\n","Step 28200: Loss: 0.2607, requires_grad: True\n","Step 28200: Gradients computed successfully\n","Step 28210: Loss: 0.2276, requires_grad: True\n","Step 28220: Loss: 0.1313, requires_grad: True\n","Step 28230: Loss: 0.3394, requires_grad: True\n","Step 28240: Loss: 0.2743, requires_grad: True\n","Step 28250: Loss: 0.2993, requires_grad: True\n","Step 28250: Gradients computed successfully\n","Step 28260: Loss: 0.1309, requires_grad: True\n","Step 28270: Loss: 0.4285, requires_grad: True\n","Step 28280: Loss: 0.1310, requires_grad: True\n","Step 28290: Loss: 0.3875, requires_grad: True\n","Step 28300: Loss: 0.3946, requires_grad: True\n","Step 28300: Gradients computed successfully\n","Step 28310: Loss: 0.1309, requires_grad: True\n","Step 28320: Loss: 0.1634, requires_grad: True\n","Step 28330: Loss: 0.4086, requires_grad: True\n","Step 28340: Loss: 0.1619, requires_grad: True\n","Step 28350: Loss: 0.4559, requires_grad: True\n","Step 28350: Gradients computed successfully\n","Step 28360: Loss: 0.2373, requires_grad: True\n","Step 28370: Loss: 0.1270, requires_grad: True\n","Step 28380: Loss: 0.1812, requires_grad: True\n","Step 28390: Loss: 0.2359, requires_grad: True\n","Step 28400: Loss: 0.2617, requires_grad: True\n","Step 28400: Gradients computed successfully\n","Step 28410: Loss: 0.1126, requires_grad: True\n","Step 28420: Loss: 0.2283, requires_grad: True\n","Step 28430: Loss: 0.3419, requires_grad: True\n","Step 28440: Loss: 0.2751, requires_grad: True\n","Step 28450: Loss: 0.2149, requires_grad: True\n","Step 28450: Gradients computed successfully\n","Step 28460: Loss: 0.2965, requires_grad: True\n","Step 28470: Loss: 0.2583, requires_grad: True\n","Step 28480: Loss: 0.2285, requires_grad: True\n","Step 28490: Loss: 0.1618, requires_grad: True\n","Step 28500: Loss: 0.1853, requires_grad: True\n","Step 28500: Gradients computed successfully\n","Step 28510: Loss: 0.6222, requires_grad: True\n","Step 28520: Loss: 0.3304, requires_grad: True\n","Step 28530: Loss: 0.5412, requires_grad: True\n","Step 28540: Loss: 0.1879, requires_grad: True\n","Step 28550: Loss: 0.1652, requires_grad: True\n","Step 28550: Gradients computed successfully\n","Step 28560: Loss: 0.1529, requires_grad: True\n","Step 28570: Loss: 0.1495, requires_grad: True\n","Step 28580: Loss: 0.5261, requires_grad: True\n","Step 28590: Loss: 0.5741, requires_grad: True\n","Step 28600: Loss: 0.1238, requires_grad: True\n","Step 28600: Gradients computed successfully\n","Step 28610: Loss: 0.0786, requires_grad: True\n","Step 28620: Loss: 0.1454, requires_grad: True\n","Step 28630: Loss: 0.2823, requires_grad: True\n","Step 28640: Loss: 0.1878, requires_grad: True\n","Step 28650: Loss: 0.1301, requires_grad: True\n","Step 28650: Gradients computed successfully\n","Step 28660: Loss: 0.1234, requires_grad: True\n","Step 28670: Loss: 0.2592, requires_grad: True\n","Step 28680: Loss: 0.0622, requires_grad: True\n","Step 28690: Loss: 0.2262, requires_grad: True\n","Step 28700: Loss: 0.3456, requires_grad: True\n","Step 28700: Gradients computed successfully\n","Step 28710: Loss: 0.1322, requires_grad: True\n","Step 28720: Loss: 0.1270, requires_grad: True\n","Step 28730: Loss: 0.2234, requires_grad: True\n","Step 28740: Loss: 0.1679, requires_grad: True\n","Step 28750: Loss: 0.1772, requires_grad: True\n","Step 28750: Gradients computed successfully\n","Step 28760: Loss: 0.1072, requires_grad: True\n","Step 28770: Loss: 0.1009, requires_grad: True\n","Step 28780: Loss: 0.3697, requires_grad: True\n","Step 28790: Loss: 0.1798, requires_grad: True\n","Step 28800: Loss: 0.0459, requires_grad: True\n","Step 28800: Gradients computed successfully\n","Step 28810: Loss: 0.7666, requires_grad: True\n","Step 28820: Loss: 0.2357, requires_grad: True\n","Step 28830: Loss: 0.1274, requires_grad: True\n","Step 28840: Loss: 0.2676, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-8000\n","Step 28850: Loss: 0.1885, requires_grad: True\n","Step 28850: Gradients computed successfully\n","Step 28860: Loss: 0.3839, requires_grad: True\n","Step 28870: Loss: 0.2530, requires_grad: True\n","Step 28880: Loss: 0.1271, requires_grad: True\n","Step 28890: Loss: 0.0994, requires_grad: True\n","Step 28900: Loss: 0.1114, requires_grad: True\n","Step 28900: Gradients computed successfully\n","Step 28910: Loss: 0.1375, requires_grad: True\n","Step 28920: Loss: 0.2614, requires_grad: True\n","Step 28930: Loss: 0.1299, requires_grad: True\n","Step 28940: Loss: 0.1717, requires_grad: True\n","Step 28950: Loss: 0.4955, requires_grad: True\n","Step 28950: Gradients computed successfully\n","Step 28960: Loss: 0.1040, requires_grad: True\n","Step 28970: Loss: 0.3256, requires_grad: True\n","Step 28980: Loss: 0.0559, requires_grad: True\n","Step 28990: Loss: 0.0973, requires_grad: True\n","Step 29000: Loss: 0.2325, requires_grad: True\n","Step 29000: Gradients computed successfully\n","Step 29010: Loss: 0.4020, requires_grad: True\n","Step 29020: Loss: 0.3802, requires_grad: True\n","Step 29030: Loss: 0.2708, requires_grad: True\n","Step 29040: Loss: 0.1067, requires_grad: True\n","Step 29050: Loss: 0.1631, requires_grad: True\n","Step 29050: Gradients computed successfully\n","Step 29060: Loss: 0.2531, requires_grad: True\n","Step 29070: Loss: 0.1280, requires_grad: True\n","Step 29080: Loss: 0.2586, requires_grad: True\n","Step 29090: Loss: 0.0491, requires_grad: True\n","Step 29100: Loss: 0.2416, requires_grad: True\n","Step 29100: Gradients computed successfully\n","Step 29110: Loss: 0.1986, requires_grad: True\n","Step 29120: Loss: 0.1399, requires_grad: True\n","Step 29130: Loss: 0.1148, requires_grad: True\n","Step 29140: Loss: 0.2026, requires_grad: True\n","Step 29150: Loss: 0.1670, requires_grad: True\n","Step 29150: Gradients computed successfully\n","Step 29160: Loss: 0.0967, requires_grad: True\n","Step 29170: Loss: 0.2663, requires_grad: True\n","Step 29180: Loss: 0.1629, requires_grad: True\n","Step 29190: Loss: 0.1686, requires_grad: True\n","Step 29200: Loss: 0.1114, requires_grad: True\n","Step 29200: Gradients computed successfully\n","Step 29210: Loss: 0.2301, requires_grad: True\n","Step 29220: Loss: 0.1862, requires_grad: True\n","Step 29230: Loss: 0.2229, requires_grad: True\n","Step 29240: Loss: 0.0550, requires_grad: True\n","Step 29250: Loss: 0.4799, requires_grad: True\n","Step 29250: Gradients computed successfully\n","Step 29260: Loss: 0.2465, requires_grad: True\n","Step 29270: Loss: 0.1931, requires_grad: True\n","Step 29280: Loss: 0.1599, requires_grad: True\n","Step 29290: Loss: 0.0685, requires_grad: True\n","Step 29300: Loss: 0.1776, requires_grad: True\n","Step 29300: Gradients computed successfully\n","Step 29310: Loss: 0.0906, requires_grad: True\n","Step 29320: Loss: 0.1263, requires_grad: True\n","Step 29330: Loss: 0.0917, requires_grad: True\n","Step 29340: Loss: 0.0416, requires_grad: True\n","Step 29350: Loss: 0.2209, requires_grad: True\n","Step 29350: Gradients computed successfully\n","Step 29360: Loss: 0.2028, requires_grad: True\n","Step 29370: Loss: 0.4225, requires_grad: True\n","Step 29380: Loss: 0.0957, requires_grad: True\n","Step 29390: Loss: 0.1329, requires_grad: True\n","Step 29400: Loss: 0.4489, requires_grad: True\n","Step 29400: Gradients computed successfully\n","Step 29410: Loss: 0.1420, requires_grad: True\n","Step 29420: Loss: 0.3369, requires_grad: True\n","Step 29430: Loss: 0.6529, requires_grad: True\n","Step 29440: Loss: 0.0912, requires_grad: True\n","Step 29450: Loss: 0.4923, requires_grad: True\n","Step 29450: Gradients computed successfully\n","Step 29460: Loss: 0.1157, requires_grad: True\n","Step 29470: Loss: 0.2985, requires_grad: True\n","Step 29480: Loss: 0.0993, requires_grad: True\n","Step 29490: Loss: 0.1800, requires_grad: True\n","Step 29500: Loss: 0.1274, requires_grad: True\n","Step 29500: Gradients computed successfully\n","Step 29510: Loss: 0.0948, requires_grad: True\n","Step 29520: Loss: 0.2470, requires_grad: True\n","Step 29530: Loss: 0.2303, requires_grad: True\n","Step 29540: Loss: 0.1284, requires_grad: True\n","Step 29550: Loss: 0.2161, requires_grad: True\n","Step 29550: Gradients computed successfully\n","Step 29560: Loss: 0.2546, requires_grad: True\n","Step 29570: Loss: 0.2090, requires_grad: True\n","Step 29580: Loss: 0.4332, requires_grad: True\n","Step 29590: Loss: 0.1671, requires_grad: True\n","Step 29600: Loss: 0.3174, requires_grad: True\n","Step 29600: Gradients computed successfully\n","Step 29610: Loss: 0.3114, requires_grad: True\n","Step 29620: Loss: 0.1726, requires_grad: True\n","Step 29630: Loss: 0.2654, requires_grad: True\n","Step 29640: Loss: 0.1350, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-8100\n","Step 29650: Loss: 0.0713, requires_grad: True\n","Step 29650: Gradients computed successfully\n","Step 29660: Loss: 0.2482, requires_grad: True\n","Step 29670: Loss: 0.0939, requires_grad: True\n","Step 29680: Loss: 0.1049, requires_grad: True\n","Step 29690: Loss: 0.1391, requires_grad: True\n","Step 29700: Loss: 0.0883, requires_grad: True\n","Step 29700: Gradients computed successfully\n","Step 29710: Loss: 0.3122, requires_grad: True\n","Step 29720: Loss: 0.2710, requires_grad: True\n","Step 29730: Loss: 0.0933, requires_grad: True\n","Step 29740: Loss: 0.0836, requires_grad: True\n","Step 29750: Loss: 0.0557, requires_grad: True\n","Step 29750: Gradients computed successfully\n","Step 29760: Loss: 0.1066, requires_grad: True\n","Step 29770: Loss: 0.0593, requires_grad: True\n","Step 29780: Loss: 0.2090, requires_grad: True\n","Step 29790: Loss: 0.1888, requires_grad: True\n","Step 29800: Loss: 0.1253, requires_grad: True\n","Step 29800: Gradients computed successfully\n","Step 29810: Loss: 0.0752, requires_grad: True\n","Step 29820: Loss: 0.1241, requires_grad: True\n","Step 29830: Loss: 0.2528, requires_grad: True\n","Step 29840: Loss: 0.4213, requires_grad: True\n","Step 29850: Loss: 0.0924, requires_grad: True\n","Step 29850: Gradients computed successfully\n","Step 29860: Loss: 0.2548, requires_grad: True\n","Step 29870: Loss: 0.2824, requires_grad: True\n","Step 29880: Loss: 0.2812, requires_grad: True\n","Step 29890: Loss: 0.3255, requires_grad: True\n","Step 29900: Loss: 0.1784, requires_grad: True\n","Step 29900: Gradients computed successfully\n","Step 29910: Loss: 0.3033, requires_grad: True\n","Step 29920: Loss: 0.2462, requires_grad: True\n","Step 29930: Loss: 0.1005, requires_grad: True\n","Step 29940: Loss: 0.1116, requires_grad: True\n","Step 29950: Loss: 0.1141, requires_grad: True\n","Step 29950: Gradients computed successfully\n","Step 29960: Loss: 0.1452, requires_grad: True\n","Step 29970: Loss: 0.2153, requires_grad: True\n","Step 29980: Loss: 0.3810, requires_grad: True\n","Step 29990: Loss: 0.1715, requires_grad: True\n","Step 30000: Loss: 0.0677, requires_grad: True\n","Step 30000: Gradients computed successfully\n","Step 30010: Loss: 0.2352, requires_grad: True\n","Step 30020: Loss: 0.1395, requires_grad: True\n","Step 30030: Loss: 0.1146, requires_grad: True\n","Step 30040: Loss: 0.3908, requires_grad: True\n","Step 30050: Loss: 0.1691, requires_grad: True\n","Step 30050: Gradients computed successfully\n","Step 30060: Loss: 0.1323, requires_grad: True\n","Step 30070: Loss: 0.3128, requires_grad: True\n","Step 30080: Loss: 0.1251, requires_grad: True\n","Step 30090: Loss: 0.1921, requires_grad: True\n","Step 30100: Loss: 0.1152, requires_grad: True\n","Step 30100: Gradients computed successfully\n","Step 30110: Loss: 0.1263, requires_grad: True\n","Step 30120: Loss: 0.1218, requires_grad: True\n","Step 30130: Loss: 0.1450, requires_grad: True\n","Step 30140: Loss: 0.0747, requires_grad: True\n","Step 30150: Loss: 0.3179, requires_grad: True\n","Step 30150: Gradients computed successfully\n","Step 30160: Loss: 0.1027, requires_grad: True\n","Step 30170: Loss: 0.1463, requires_grad: True\n","Step 30180: Loss: 0.6084, requires_grad: True\n","Step 30190: Loss: 0.2464, requires_grad: True\n","Step 30200: Loss: 0.2076, requires_grad: True\n","Step 30200: Gradients computed successfully\n","Step 30210: Loss: 0.1825, requires_grad: True\n","Step 30220: Loss: 0.1099, requires_grad: True\n","Step 30230: Loss: 0.1840, requires_grad: True\n","Step 30240: Loss: 0.1546, requires_grad: True\n","Step 30250: Loss: 0.1386, requires_grad: True\n","Step 30250: Gradients computed successfully\n","Step 30260: Loss: 0.1200, requires_grad: True\n","Step 30270: Loss: 0.2531, requires_grad: True\n","Step 30280: Loss: 0.2832, requires_grad: True\n","Step 30290: Loss: 0.2401, requires_grad: True\n","Step 30300: Loss: 0.1379, requires_grad: True\n","Step 30300: Gradients computed successfully\n","Step 30310: Loss: 0.0722, requires_grad: True\n","Step 30320: Loss: 0.0908, requires_grad: True\n","Step 30330: Loss: 0.1106, requires_grad: True\n","Step 30340: Loss: 0.0879, requires_grad: True\n","Step 30350: Loss: 0.1847, requires_grad: True\n","Step 30350: Gradients computed successfully\n","Step 30360: Loss: 0.0985, requires_grad: True\n","Step 30370: Loss: 0.3036, requires_grad: True\n","Step 30380: Loss: 0.1888, requires_grad: True\n","Step 30390: Loss: 0.3543, requires_grad: True\n","Step 30400: Loss: 0.2257, requires_grad: True\n","Step 30400: Gradients computed successfully\n","Step 30410: Loss: 0.2158, requires_grad: True\n","Step 30420: Loss: 0.3067, requires_grad: True\n","Step 30430: Loss: 0.0950, requires_grad: True\n","Step 30440: Loss: 0.1662, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-8200\n","Step 30450: Loss: 0.0690, requires_grad: True\n","Step 30450: Gradients computed successfully\n","Step 30460: Loss: 0.2142, requires_grad: True\n","Step 30470: Loss: 0.3352, requires_grad: True\n","Step 30480: Loss: 0.1024, requires_grad: True\n","Step 30490: Loss: 0.0844, requires_grad: True\n","Step 30500: Loss: 0.3097, requires_grad: True\n","Step 30500: Gradients computed successfully\n","Step 30510: Loss: 0.1443, requires_grad: True\n","Step 30520: Loss: 0.1211, requires_grad: True\n","Step 30530: Loss: 0.1487, requires_grad: True\n","Step 30540: Loss: 0.0645, requires_grad: True\n","Step 30550: Loss: 0.3927, requires_grad: True\n","Step 30550: Gradients computed successfully\n","Step 30560: Loss: 0.4225, requires_grad: True\n","Step 30570: Loss: 0.2755, requires_grad: True\n","Step 30580: Loss: 0.3716, requires_grad: True\n","Step 30590: Loss: 0.0566, requires_grad: True\n","Step 30600: Loss: 0.2831, requires_grad: True\n","Step 30600: Gradients computed successfully\n","Step 30610: Loss: 0.2064, requires_grad: True\n","Step 30620: Loss: 0.2002, requires_grad: True\n","Step 30630: Loss: 0.2736, requires_grad: True\n","Step 30640: Loss: 0.0874, requires_grad: True\n","Step 30650: Loss: 0.2438, requires_grad: True\n","Step 30650: Gradients computed successfully\n","Step 30660: Loss: 0.0982, requires_grad: True\n","Step 30670: Loss: 0.3591, requires_grad: True\n","Step 30680: Loss: 0.1751, requires_grad: True\n","Step 30690: Loss: 0.1036, requires_grad: True\n","Step 30700: Loss: 0.2802, requires_grad: True\n","Step 30700: Gradients computed successfully\n","Step 30710: Loss: 0.2025, requires_grad: True\n","Step 30720: Loss: 0.3081, requires_grad: True\n","Step 30730: Loss: 0.3313, requires_grad: True\n","Step 30740: Loss: 0.4054, requires_grad: True\n","Step 30750: Loss: 0.2558, requires_grad: True\n","Step 30750: Gradients computed successfully\n","Step 30760: Loss: 0.3110, requires_grad: True\n","Step 30770: Loss: 0.1304, requires_grad: True\n","Step 30780: Loss: 0.4861, requires_grad: True\n","Step 30790: Loss: 0.1218, requires_grad: True\n","Step 30800: Loss: 0.1061, requires_grad: True\n","Step 30800: Gradients computed successfully\n","Step 30810: Loss: 0.0854, requires_grad: True\n","Step 30820: Loss: 0.0374, requires_grad: True\n","Step 30830: Loss: 0.2311, requires_grad: True\n","Step 30840: Loss: 0.2008, requires_grad: True\n","Step 30850: Loss: 0.2179, requires_grad: True\n","Step 30850: Gradients computed successfully\n","Step 30860: Loss: 0.1544, requires_grad: True\n","Step 30870: Loss: 0.2325, requires_grad: True\n","Step 30880: Loss: 0.5312, requires_grad: True\n","Step 30890: Loss: 0.0683, requires_grad: True\n","Step 30900: Loss: 0.1201, requires_grad: True\n","Step 30900: Gradients computed successfully\n","Step 30910: Loss: 0.1011, requires_grad: True\n","Step 30920: Loss: 0.2431, requires_grad: True\n","Step 30930: Loss: 0.1570, requires_grad: True\n","Step 30940: Loss: 0.0974, requires_grad: True\n","Step 30950: Loss: 0.3990, requires_grad: True\n","Step 30950: Gradients computed successfully\n","Step 30960: Loss: 0.3101, requires_grad: True\n","Step 30970: Loss: 0.2509, requires_grad: True\n","Step 30980: Loss: 0.0275, requires_grad: True\n","Step 30990: Loss: 0.2850, requires_grad: True\n","Step 31000: Loss: 0.1100, requires_grad: True\n","Step 31000: Gradients computed successfully\n","Step 31010: Loss: 0.1539, requires_grad: True\n","Step 31020: Loss: 0.1808, requires_grad: True\n","Step 31030: Loss: 0.2104, requires_grad: True\n","Step 31040: Loss: 0.1395, requires_grad: True\n","Step 31050: Loss: 0.2734, requires_grad: True\n","Step 31050: Gradients computed successfully\n","Step 31060: Loss: 0.2496, requires_grad: True\n","Step 31070: Loss: 0.1365, requires_grad: True\n","Step 31080: Loss: 0.2873, requires_grad: True\n","Step 31090: Loss: 0.1170, requires_grad: True\n","Step 31100: Loss: 0.2801, requires_grad: True\n","Step 31100: Gradients computed successfully\n","Step 31110: Loss: 0.1618, requires_grad: True\n","Step 31120: Loss: 0.3536, requires_grad: True\n","Step 31130: Loss: 0.0918, requires_grad: True\n","Step 31140: Loss: 0.2266, requires_grad: True\n","Step 31150: Loss: 0.4818, requires_grad: True\n","Step 31150: Gradients computed successfully\n","Step 31160: Loss: 0.1220, requires_grad: True\n","Step 31170: Loss: 0.1076, requires_grad: True\n","Step 31180: Loss: 0.1758, requires_grad: True\n","Step 31190: Loss: 0.3238, requires_grad: True\n","Step 31200: Loss: 0.1960, requires_grad: True\n","Step 31200: Gradients computed successfully\n","Step 31210: Loss: 0.3009, requires_grad: True\n","Step 31220: Loss: 0.0791, requires_grad: True\n","Step 31230: Loss: 0.1307, requires_grad: True\n","Step 31240: Loss: 0.1447, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-8300\n","Step 31250: Loss: 0.2081, requires_grad: True\n","Step 31250: Gradients computed successfully\n","Step 31260: Loss: 0.1244, requires_grad: True\n","Step 31270: Loss: 0.2891, requires_grad: True\n","Step 31280: Loss: 0.4583, requires_grad: True\n","Step 31290: Loss: 0.2212, requires_grad: True\n","Step 31300: Loss: 0.6469, requires_grad: True\n","Step 31300: Gradients computed successfully\n","Step 31310: Loss: 0.0802, requires_grad: True\n","Step 31320: Loss: 0.2294, requires_grad: True\n","Step 31330: Loss: 0.2248, requires_grad: True\n","Step 31340: Loss: 0.2333, requires_grad: True\n","Step 31350: Loss: 0.1766, requires_grad: True\n","Step 31350: Gradients computed successfully\n","Step 31360: Loss: 0.1283, requires_grad: True\n","Step 31370: Loss: 0.2295, requires_grad: True\n","Step 31380: Loss: 0.0988, requires_grad: True\n","Step 31390: Loss: 0.1123, requires_grad: True\n","Step 31400: Loss: 0.1590, requires_grad: True\n","Step 31400: Gradients computed successfully\n","Step 31410: Loss: 0.1199, requires_grad: True\n","Step 31420: Loss: 0.2715, requires_grad: True\n","Step 31430: Loss: 0.1150, requires_grad: True\n","Step 31440: Loss: 0.0919, requires_grad: True\n","Step 31450: Loss: 0.1810, requires_grad: True\n","Step 31450: Gradients computed successfully\n","Step 31460: Loss: 0.2166, requires_grad: True\n","Step 31470: Loss: 0.4191, requires_grad: True\n","Step 31480: Loss: 0.1867, requires_grad: True\n","Step 31490: Loss: 0.2422, requires_grad: True\n","Step 31500: Loss: 0.1416, requires_grad: True\n","Step 31500: Gradients computed successfully\n","Step 31510: Loss: 0.3216, requires_grad: True\n","Step 31520: Loss: 0.1015, requires_grad: True\n","Step 31530: Loss: 0.1681, requires_grad: True\n","Step 31540: Loss: 0.3552, requires_grad: True\n","Step 31550: Loss: 0.1748, requires_grad: True\n","Step 31550: Gradients computed successfully\n","Step 31560: Loss: 0.2452, requires_grad: True\n","Step 31570: Loss: 0.1825, requires_grad: True\n","Step 31580: Loss: 0.1522, requires_grad: True\n","Step 31590: Loss: 0.2992, requires_grad: True\n","Step 31600: Loss: 0.1147, requires_grad: True\n","Step 31600: Gradients computed successfully\n","Step 31610: Loss: 0.1843, requires_grad: True\n","Step 31620: Loss: 0.3639, requires_grad: True\n","Step 31630: Loss: 0.2505, requires_grad: True\n","Step 31640: Loss: 0.1396, requires_grad: True\n","Step 31650: Loss: 0.1145, requires_grad: True\n","Step 31650: Gradients computed successfully\n","Step 31660: Loss: 0.3998, requires_grad: True\n","Step 31670: Loss: 0.0902, requires_grad: True\n","Step 31680: Loss: 0.0698, requires_grad: True\n","Step 31690: Loss: 0.1443, requires_grad: True\n","Step 31700: Loss: 0.1472, requires_grad: True\n","Step 31700: Gradients computed successfully\n","Step 31710: Loss: 0.3049, requires_grad: True\n","Step 31720: Loss: 0.1446, requires_grad: True\n","Step 31730: Loss: 0.2459, requires_grad: True\n","Step 31740: Loss: 0.1859, requires_grad: True\n","Step 31750: Loss: 0.1517, requires_grad: True\n","Step 31750: Gradients computed successfully\n","Step 31760: Loss: 0.0857, requires_grad: True\n","Step 31770: Loss: 0.1290, requires_grad: True\n","Step 31780: Loss: 0.3039, requires_grad: True\n","Step 31790: Loss: 0.0678, requires_grad: True\n","Step 31800: Loss: 0.2675, requires_grad: True\n","Step 31800: Gradients computed successfully\n","Step 31810: Loss: 0.1816, requires_grad: True\n","Step 31820: Loss: 0.3528, requires_grad: True\n","Step 31830: Loss: 0.1801, requires_grad: True\n","Step 31840: Loss: 0.3561, requires_grad: True\n","Step 31850: Loss: 0.1093, requires_grad: True\n","Step 31850: Gradients computed successfully\n","Step 31860: Loss: 0.1783, requires_grad: True\n","Step 31870: Loss: 0.5038, requires_grad: True\n","Step 31880: Loss: 0.4108, requires_grad: True\n","Step 31890: Loss: 0.3506, requires_grad: True\n","Step 31900: Loss: 0.3198, requires_grad: True\n","Step 31900: Gradients computed successfully\n","Step 31910: Loss: 0.1780, requires_grad: True\n","Step 31920: Loss: 0.3004, requires_grad: True\n","Step 31930: Loss: 0.1697, requires_grad: True\n","Step 31940: Loss: 0.1881, requires_grad: True\n","Step 31950: Loss: 0.4909, requires_grad: True\n","Step 31950: Gradients computed successfully\n","Step 31960: Loss: 0.0834, requires_grad: True\n","Step 31970: Loss: 0.2561, requires_grad: True\n","Step 31980: Loss: 0.1636, requires_grad: True\n","Step 31990: Loss: 0.2728, requires_grad: True\n","Step 32000: Loss: 0.2131, requires_grad: True\n","Step 32000: Gradients computed successfully\n","Step 32010: Loss: 0.2848, requires_grad: True\n","Step 32020: Loss: 0.1122, requires_grad: True\n","Step 32030: Loss: 0.2234, requires_grad: True\n","Step 32040: Loss: 0.2390, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-8400\n","Step 32050: Loss: 0.1153, requires_grad: True\n","Step 32050: Gradients computed successfully\n","Step 32060: Loss: 0.1033, requires_grad: True\n","Step 32070: Loss: 0.3302, requires_grad: True\n","Step 32080: Loss: 0.3470, requires_grad: True\n","Step 32090: Loss: 0.1966, requires_grad: True\n","Step 32100: Loss: 0.1837, requires_grad: True\n","Step 32100: Gradients computed successfully\n","Step 32110: Loss: 0.2345, requires_grad: True\n","Step 32120: Loss: 0.1821, requires_grad: True\n","Step 32130: Loss: 0.1049, requires_grad: True\n","Step 32140: Loss: 0.0905, requires_grad: True\n","Step 32150: Loss: 0.0765, requires_grad: True\n","Step 32150: Gradients computed successfully\n","Step 32160: Loss: 0.2913, requires_grad: True\n","Step 32170: Loss: 0.1337, requires_grad: True\n","Step 32180: Loss: 0.4614, requires_grad: True\n","Step 32190: Loss: 0.3139, requires_grad: True\n","Step 32200: Loss: 0.1686, requires_grad: True\n","Step 32200: Gradients computed successfully\n","Step 32210: Loss: 0.2065, requires_grad: True\n","Step 32220: Loss: 0.2959, requires_grad: True\n","Step 32230: Loss: 0.1721, requires_grad: True\n","Step 32240: Loss: 0.2997, requires_grad: True\n","Step 32250: Loss: 0.0594, requires_grad: True\n","Step 32250: Gradients computed successfully\n","Step 32260: Loss: 0.1684, requires_grad: True\n","Step 32270: Loss: 0.2094, requires_grad: True\n","Step 32280: Loss: 0.2640, requires_grad: True\n","Step 32290: Loss: 0.2103, requires_grad: True\n","Step 32300: Loss: 0.3308, requires_grad: True\n","Step 32300: Gradients computed successfully\n","Step 32310: Loss: 0.2043, requires_grad: True\n","Step 32320: Loss: 0.2606, requires_grad: True\n","Step 32330: Loss: 0.2819, requires_grad: True\n","Step 32340: Loss: 0.1779, requires_grad: True\n","Step 32350: Loss: 0.2078, requires_grad: True\n","Step 32350: Gradients computed successfully\n","Step 32360: Loss: 0.1438, requires_grad: True\n","Step 32370: Loss: 0.1692, requires_grad: True\n","Step 32380: Loss: 0.0751, requires_grad: True\n","Step 32390: Loss: 0.2329, requires_grad: True\n","Step 32400: Loss: 0.1525, requires_grad: True\n","Step 32400: Gradients computed successfully\n","Step 32410: Loss: 0.2526, requires_grad: True\n","Step 32420: Loss: 0.1449, requires_grad: True\n","Step 32430: Loss: 0.1156, requires_grad: True\n","Step 32440: Loss: 0.1727, requires_grad: True\n","Step 32450: Loss: 0.2672, requires_grad: True\n","Step 32450: Gradients computed successfully\n","Step 32460: Loss: 0.1973, requires_grad: True\n","Step 32470: Loss: 0.2376, requires_grad: True\n","Step 32480: Loss: 0.2188, requires_grad: True\n","Step 32490: Loss: 0.3326, requires_grad: True\n","Step 32500: Loss: 0.1055, requires_grad: True\n","Step 32500: Gradients computed successfully\n","Step 32510: Loss: 0.3541, requires_grad: True\n","Step 32520: Loss: 0.1050, requires_grad: True\n","Step 32530: Loss: 0.0941, requires_grad: True\n","Step 32540: Loss: 0.0740, requires_grad: True\n","Step 32550: Loss: 0.1604, requires_grad: True\n","Step 32550: Gradients computed successfully\n","Step 32560: Loss: 0.1842, requires_grad: True\n","Step 32570: Loss: 0.1164, requires_grad: True\n","Step 32580: Loss: 0.4643, requires_grad: True\n","Step 32590: Loss: 0.4316, requires_grad: True\n","Step 32600: Loss: 0.2001, requires_grad: True\n","Step 32600: Gradients computed successfully\n","Step 32610: Loss: 0.3675, requires_grad: True\n","Step 32620: Loss: 0.2968, requires_grad: True\n","Step 32630: Loss: 0.1443, requires_grad: True\n","Step 32640: Loss: 0.1514, requires_grad: True\n","Step 32650: Loss: 0.1712, requires_grad: True\n","Step 32650: Gradients computed successfully\n","Step 32660: Loss: 0.2347, requires_grad: True\n","Step 32670: Loss: 0.2508, requires_grad: True\n","Step 32680: Loss: 0.0528, requires_grad: True\n","Step 32690: Loss: 0.2515, requires_grad: True\n","Step 32700: Loss: 0.1637, requires_grad: True\n","Step 32700: Gradients computed successfully\n","Step 32710: Loss: 0.2720, requires_grad: True\n","Step 32720: Loss: 0.0561, requires_grad: True\n","Step 32730: Loss: 0.1886, requires_grad: True\n","Step 32740: Loss: 0.0851, requires_grad: True\n","Step 32750: Loss: 0.1562, requires_grad: True\n","Step 32750: Gradients computed successfully\n","Step 32760: Loss: 0.1546, requires_grad: True\n","Step 32770: Loss: 0.1051, requires_grad: True\n","Step 32780: Loss: 0.2456, requires_grad: True\n","Step 32790: Loss: 0.1538, requires_grad: True\n","Step 32800: Loss: 0.4805, requires_grad: True\n","Step 32800: Gradients computed successfully\n","Step 32810: Loss: 0.2228, requires_grad: True\n","Step 32820: Loss: 0.2137, requires_grad: True\n","Step 32830: Loss: 0.2145, requires_grad: True\n","Step 32840: Loss: 0.1272, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-8500\n","Step 32850: Loss: 0.0972, requires_grad: True\n","Step 32850: Gradients computed successfully\n","Step 32860: Loss: 0.0485, requires_grad: True\n","Step 32870: Loss: 0.2178, requires_grad: True\n","Step 32880: Loss: 0.1168, requires_grad: True\n","Step 32890: Loss: 0.0849, requires_grad: True\n","Step 32900: Loss: 0.1471, requires_grad: True\n","Step 32900: Gradients computed successfully\n","Step 32910: Loss: 0.2805, requires_grad: True\n","Step 32920: Loss: 0.3001, requires_grad: True\n","Step 32930: Loss: 0.0922, requires_grad: True\n","Step 32940: Loss: 0.0654, requires_grad: True\n","Step 32950: Loss: 0.1164, requires_grad: True\n","Step 32950: Gradients computed successfully\n","Step 32960: Loss: 0.1841, requires_grad: True\n","Step 32970: Loss: 0.1547, requires_grad: True\n","Step 32980: Loss: 0.1999, requires_grad: True\n","Step 32990: Loss: 0.2774, requires_grad: True\n","Step 33000: Loss: 0.3768, requires_grad: True\n","Step 33000: Gradients computed successfully\n","Step 33010: Loss: 0.3485, requires_grad: True\n","Step 33020: Loss: 0.2644, requires_grad: True\n","Step 33030: Loss: 0.2041, requires_grad: True\n","Step 33040: Loss: 0.3410, requires_grad: True\n","Step 33050: Loss: 0.3228, requires_grad: True\n","Step 33050: Gradients computed successfully\n","Step 33060: Loss: 0.1550, requires_grad: True\n","Step 33070: Loss: 0.4494, requires_grad: True\n","Step 33080: Loss: 0.2752, requires_grad: True\n","Step 33090: Loss: 0.1503, requires_grad: True\n","Step 33100: Loss: 0.1008, requires_grad: True\n","Step 33100: Gradients computed successfully\n","Step 33110: Loss: 0.1146, requires_grad: True\n","Step 33120: Loss: 0.1458, requires_grad: True\n","Step 33130: Loss: 0.2896, requires_grad: True\n","Step 33140: Loss: 0.3410, requires_grad: True\n","Step 33150: Loss: 0.1584, requires_grad: True\n","Step 33150: Gradients computed successfully\n","Step 33160: Loss: 0.4013, requires_grad: True\n","Step 33170: Loss: 0.1445, requires_grad: True\n","Step 33180: Loss: 0.1562, requires_grad: True\n","Step 33190: Loss: 0.1256, requires_grad: True\n","Step 33200: Loss: 0.1017, requires_grad: True\n","Step 33200: Gradients computed successfully\n","Step 33210: Loss: 0.1901, requires_grad: True\n","Step 33220: Loss: 0.0337, requires_grad: True\n","Step 33230: Loss: 0.1052, requires_grad: True\n","Step 33240: Loss: 0.1210, requires_grad: True\n","Step 33250: Loss: 0.2411, requires_grad: True\n","Step 33250: Gradients computed successfully\n","Step 33260: Loss: 0.0910, requires_grad: True\n","Step 33270: Loss: 0.0977, requires_grad: True\n","Step 33280: Loss: 0.1723, requires_grad: True\n","Step 33290: Loss: 0.1362, requires_grad: True\n","Step 33300: Loss: 0.1425, requires_grad: True\n","Step 33300: Gradients computed successfully\n","Step 33310: Loss: 0.4312, requires_grad: True\n","Step 33320: Loss: 0.1743, requires_grad: True\n","Step 33330: Loss: 0.4230, requires_grad: True\n","Step 33340: Loss: 0.2297, requires_grad: True\n","Step 33350: Loss: 0.1988, requires_grad: True\n","Step 33350: Gradients computed successfully\n","Step 33360: Loss: 0.0906, requires_grad: True\n","Step 33370: Loss: 0.4863, requires_grad: True\n","Step 33380: Loss: 0.1040, requires_grad: True\n","Step 33390: Loss: 0.1353, requires_grad: True\n","Step 33400: Loss: 0.1059, requires_grad: True\n","Step 33400: Gradients computed successfully\n","Step 33410: Loss: 0.4734, requires_grad: True\n","Step 33420: Loss: 0.4019, requires_grad: True\n","Step 33430: Loss: 0.2866, requires_grad: True\n","Step 33440: Loss: 0.1681, requires_grad: True\n","Step 33450: Loss: 0.0984, requires_grad: True\n","Step 33450: Gradients computed successfully\n","Step 33460: Loss: 0.2074, requires_grad: True\n","Step 33470: Loss: 0.2144, requires_grad: True\n","Step 33480: Loss: 0.3361, requires_grad: True\n","Step 33490: Loss: 0.4496, requires_grad: True\n","Step 33500: Loss: 0.2454, requires_grad: True\n","Step 33500: Gradients computed successfully\n","Step 33510: Loss: 0.3142, requires_grad: True\n","Step 33520: Loss: 0.0922, requires_grad: True\n","Step 33530: Loss: 0.1906, requires_grad: True\n","Step 33540: Loss: 0.1311, requires_grad: True\n","Step 33550: Loss: 0.1111, requires_grad: True\n","Step 33550: Gradients computed successfully\n","Step 33560: Loss: 0.1379, requires_grad: True\n","Step 33570: Loss: 0.0562, requires_grad: True\n","Step 33580: Loss: 0.1737, requires_grad: True\n","Step 33590: Loss: 0.2420, requires_grad: True\n","Step 33600: Loss: 0.0718, requires_grad: True\n","Step 33600: Gradients computed successfully\n","Step 33610: Loss: 0.1384, requires_grad: True\n","Step 33620: Loss: 0.1590, requires_grad: True\n","Step 33630: Loss: 0.1594, requires_grad: True\n","Step 33640: Loss: 0.1769, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-8600\n","Step 33650: Loss: 0.1040, requires_grad: True\n","Step 33650: Gradients computed successfully\n","Step 33660: Loss: 0.2390, requires_grad: True\n","Step 33670: Loss: 0.1074, requires_grad: True\n","Step 33680: Loss: 0.2159, requires_grad: True\n","Step 33690: Loss: 0.0501, requires_grad: True\n","Step 33700: Loss: 0.1249, requires_grad: True\n","Step 33700: Gradients computed successfully\n","Step 33710: Loss: 0.2325, requires_grad: True\n","Step 33720: Loss: 0.1074, requires_grad: True\n","Step 33730: Loss: 0.1764, requires_grad: True\n","Step 33740: Loss: 0.3828, requires_grad: True\n","Step 33750: Loss: 0.1988, requires_grad: True\n","Step 33750: Gradients computed successfully\n","Step 33760: Loss: 0.2013, requires_grad: True\n","Step 33770: Loss: 0.1292, requires_grad: True\n","Step 33780: Loss: 0.5011, requires_grad: True\n","Step 33790: Loss: 0.2802, requires_grad: True\n","Step 33800: Loss: 0.1996, requires_grad: True\n","Step 33800: Gradients computed successfully\n","Step 33810: Loss: 0.0979, requires_grad: True\n","Step 33820: Loss: 0.3407, requires_grad: True\n","Step 33830: Loss: 0.4546, requires_grad: True\n","Step 33840: Loss: 0.3531, requires_grad: True\n","Step 33850: Loss: 0.0853, requires_grad: True\n","Step 33850: Gradients computed successfully\n","Step 33860: Loss: 0.1875, requires_grad: True\n","Step 33870: Loss: 0.0496, requires_grad: True\n","Step 33880: Loss: 0.3400, requires_grad: True\n","Step 33890: Loss: 0.3646, requires_grad: True\n","Step 33900: Loss: 0.1052, requires_grad: True\n","Step 33900: Gradients computed successfully\n","Step 33910: Loss: 0.1580, requires_grad: True\n","Step 33920: Loss: 0.1550, requires_grad: True\n","Step 33930: Loss: 0.1148, requires_grad: True\n","Step 33940: Loss: 0.2949, requires_grad: True\n","Step 33950: Loss: 0.1560, requires_grad: True\n","Step 33950: Gradients computed successfully\n","Step 33960: Loss: 0.1078, requires_grad: True\n","Step 33970: Loss: 0.2437, requires_grad: True\n","Step 33980: Loss: 0.2250, requires_grad: True\n","Step 33990: Loss: 0.1468, requires_grad: True\n","Step 34000: Loss: 0.2020, requires_grad: True\n","Step 34000: Gradients computed successfully\n","Step 34010: Loss: 0.2063, requires_grad: True\n","Step 34020: Loss: 0.1408, requires_grad: True\n","Step 34030: Loss: 0.3601, requires_grad: True\n","Step 34040: Loss: 0.0748, requires_grad: True\n","Step 34050: Loss: 0.3639, requires_grad: True\n","Step 34050: Gradients computed successfully\n","Step 34060: Loss: 0.2128, requires_grad: True\n","Step 34070: Loss: 0.1966, requires_grad: True\n","Step 34080: Loss: 0.1102, requires_grad: True\n","Step 34090: Loss: 0.3646, requires_grad: True\n","Step 34100: Loss: 0.2646, requires_grad: True\n","Step 34100: Gradients computed successfully\n","Step 34110: Loss: 0.1995, requires_grad: True\n","Step 34120: Loss: 0.2270, requires_grad: True\n","Step 34130: Loss: 0.4433, requires_grad: True\n","Step 34140: Loss: 0.2306, requires_grad: True\n","Step 34150: Loss: 0.1191, requires_grad: True\n","Step 34150: Gradients computed successfully\n","Step 34160: Loss: 0.3399, requires_grad: True\n","Step 34170: Loss: 0.1661, requires_grad: True\n","Step 34180: Loss: 0.2241, requires_grad: True\n","Step 34190: Loss: 0.3795, requires_grad: True\n","Step 34200: Loss: 0.1180, requires_grad: True\n","Step 34200: Gradients computed successfully\n","Step 34210: Loss: 0.1400, requires_grad: True\n","Step 34220: Loss: 0.1400, requires_grad: True\n","Step 34230: Loss: 0.3203, requires_grad: True\n","Step 34240: Loss: 0.0811, requires_grad: True\n","Step 34250: Loss: 0.2222, requires_grad: True\n","Step 34250: Gradients computed successfully\n","Step 34260: Loss: 0.3367, requires_grad: True\n","Step 34270: Loss: 0.1822, requires_grad: True\n","Step 34280: Loss: 0.0986, requires_grad: True\n","Step 34290: Loss: 0.3925, requires_grad: True\n","Step 34300: Loss: 0.2554, requires_grad: True\n","Step 34300: Gradients computed successfully\n","Step 34310: Loss: 0.2309, requires_grad: True\n","Step 34320: Loss: 0.0742, requires_grad: True\n","Step 34330: Loss: 0.2434, requires_grad: True\n","Step 34340: Loss: 0.1181, requires_grad: True\n","Step 34350: Loss: 0.1319, requires_grad: True\n","Step 34350: Gradients computed successfully\n","Step 34360: Loss: 0.1375, requires_grad: True\n","Step 34370: Loss: 0.2293, requires_grad: True\n","Step 34380: Loss: 0.1296, requires_grad: True\n","Step 34390: Loss: 0.2453, requires_grad: True\n","Step 34400: Loss: 0.1643, requires_grad: True\n","Step 34400: Gradients computed successfully\n","Step 34410: Loss: 0.0747, requires_grad: True\n","Step 34420: Loss: 0.1032, requires_grad: True\n","Step 34430: Loss: 0.1241, requires_grad: True\n","Step 34440: Loss: 0.3522, requires_grad: True\n","Saved checkpoint to Checkpoints/mistral_complex_sql_training_4090/checkpoint-8700\n","Step 34450: Loss: 0.0782, requires_grad: True\n","Step 34450: Gradients computed successfully\n","Step 34460: Loss: 0.1376, requires_grad: True\n","Step 34470: Loss: 0.1752, requires_grad: True\n","Step 34480: Loss: 0.2042, requires_grad: True\n","Step 34490: Loss: 0.2685, requires_grad: True\n","Step 34500: Loss: 0.0999, requires_grad: True\n","Step 34500: Gradients computed successfully\n","Step 34510: Loss: 0.2423, requires_grad: True\n","Step 34520: Loss: 0.2839, requires_grad: True\n","Step 34530: Loss: 0.1708, requires_grad: True\n","Step 34540: Loss: 0.4445, requires_grad: True\n","Step 34550: Loss: 0.3437, requires_grad: True\n","Step 34550: Gradients computed successfully\n","Step 34560: Loss: 0.1968, requires_grad: True\n","Step 34570: Loss: 0.2180, requires_grad: True\n","Step 34580: Loss: 0.1515, requires_grad: True\n","Step 34590: Loss: 0.3996, requires_grad: True\n","Step 34600: Loss: 0.1123, requires_grad: True\n","Step 34600: Gradients computed successfully\n","Step 34610: Loss: 0.3915, requires_grad: True\n","Step 34620: Loss: 0.0709, requires_grad: True\n","Step 34630: Loss: 0.2275, requires_grad: True\n","Step 34640: Loss: 0.0991, requires_grad: True\n","Step 34650: Loss: 0.2032, requires_grad: True\n","Step 34650: Gradients computed successfully\n","Step 34660: Loss: 0.1520, requires_grad: True\n","Step 34670: Loss: 0.4130, requires_grad: True\n","Step 34680: Loss: 0.3007, requires_grad: True\n","Step 34690: Loss: 0.1100, requires_grad: True\n","Step 34700: Loss: 0.2352, requires_grad: True\n","Step 34700: Gradients computed successfully\n","Step 34710: Loss: 0.5990, requires_grad: True\n","Step 34720: Loss: 0.2762, requires_grad: True\n","Step 34730: Loss: 0.1709, requires_grad: True\n","Step 34740: Loss: 0.3709, requires_grad: True\n","Step 34750: Loss: 0.1387, requires_grad: True\n","Step 34750: Gradients computed successfully\n","Step 34760: Loss: 0.1710, requires_grad: True\n","Step 34770: Loss: 0.3445, requires_grad: True\n","Step 34780: Loss: 0.0772, requires_grad: True\n","Step 34790: Loss: 0.1517, requires_grad: True\n","Step 34800: Loss: 0.1291, requires_grad: True\n","Step 34800: Gradients computed successfully\n","Step 34810: Loss: 0.0441, requires_grad: True\n","Step 34820: Loss: 0.5296, requires_grad: True\n","Step 34830: Loss: 0.0934, requires_grad: True\n","Step 34840: Loss: 0.1856, requires_grad: True\n","Step 34850: Loss: 0.1064, requires_grad: True\n","Step 34850: Gradients computed successfully\n","Step 34860: Loss: 0.3849, requires_grad: True\n","Step 34870: Loss: 0.1299, requires_grad: True\n","Step 34880: Loss: 0.2818, requires_grad: True\n","Step 34890: Loss: 0.0364, requires_grad: True\n","Step 34900: Loss: 0.2106, requires_grad: True\n","Step 34900: Gradients computed successfully\n","Step 34910: Loss: 0.4102, requires_grad: True\n","Step 34920: Loss: 0.1387, requires_grad: True\n","Step 34930: Loss: 0.3874, requires_grad: True\n","Step 34940: Loss: 0.2301, requires_grad: True\n","Step 34950: Loss: 0.1001, requires_grad: True\n","Step 34950: Gradients computed successfully\n","Step 34960: Loss: 0.2882, requires_grad: True\n","Step 34970: Loss: 0.0954, requires_grad: True\n","Step 34980: Loss: 0.2901, requires_grad: True\n","Step 34990: Loss: 0.1226, requires_grad: True\n","Step 35000: Loss: 0.1565, requires_grad: True\n","Step 35000: Gradients computed successfully\n","Step 35010: Loss: 0.0924, requires_grad: True\n","Step 35020: Loss: 0.1085, requires_grad: True\n","Step 35030: Loss: 0.3852, requires_grad: True\n","Step 35040: Loss: 0.3005, requires_grad: True\n","Step 35050: Loss: 0.1288, requires_grad: True\n","Step 35050: Gradients computed successfully\n","Step 35060: Loss: 0.2802, requires_grad: True\n","Step 35070: Loss: 0.1680, requires_grad: True\n","Step 35080: Loss: 0.2411, requires_grad: True\n","Step 35090: Loss: 0.1220, requires_grad: True\n","Step 35100: Loss: 0.2451, requires_grad: True\n","Step 35100: Gradients computed successfully\n","Step 35110: Loss: 0.1779, requires_grad: True\n","Step 35120: Loss: 0.1358, requires_grad: True\n","Step 35130: Loss: 0.1157, requires_grad: True\n","Step 35140: Loss: 0.2890, requires_grad: True\n","Epoch 2 completed. Average loss: 0.2129\n","Saved model at end of epoch 2 to Checkpoints/mistral_complex_sql_training_4090/epoch-2\n","\n","Training completed. Average loss: 0.2392\n"]}],"source":["# Set model to training mode\n","model.train()\n","\n","# Training loop\n","print(\"\\n Starting training \")\n","global_step = 0\n","total_loss = 0\n","\n","# Progress tracking\n","from tqdm.auto import tqdm\n","\n","for epoch in range(num_epochs):\n","    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n","    epoch_loss = 0\n","    optimizer.zero_grad()  # Reset gradients at start of epoch\n","\n","    # Progress bar for this epoch\n","    progress_bar = tqdm(total=len(train_dataloader), desc=f\"Epoch {epoch+1}\")\n","\n","    for step, batch in enumerate(train_dataloader):\n","        # Move batch to GPU\n","        batch = {k: v.cuda() for k, v in batch.items()}\n","\n","        # Forward pass\n","        outputs = model(\n","            input_ids=batch[\"input_ids\"],\n","            attention_mask=batch[\"attention_mask\"],\n","            labels=batch[\"labels\"],\n","            use_cache=False  # Disable KV cache for training\n","        )\n","\n","        # Get loss and scale for gradient accumulation\n","        loss = outputs.loss / grad_accumulation_steps\n","\n","        # Log the loss value\n","        if step % 10 == 0:\n","            print(f\"Step {step}: Loss: {loss.item() * grad_accumulation_steps:.4f}, requires_grad: {loss.requires_grad}\")\n","\n","        # Backward pass\n","        loss.backward()\n","\n","        # Check for gradient computation\n","        has_grad = any(p.grad is not None for p in model.parameters() if p.requires_grad)\n","        if step % 50 == 0:\n","            if has_grad:\n","                print(f\"Step {step}: Gradients computed successfully\")\n","            else:\n","                print(f\"Step {step}: WARNING - No gradients computed!\")\n","\n","        # Update weights after accumulating gradients\n","        if (step + 1) % grad_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n","            # Clip gradients to prevent exploding gradients\n","            torch.nn.utils.clip_grad_norm_(\n","                [p for p in model.parameters() if p.requires_grad],\n","                max_norm=1.0\n","            )\n","\n","            # Update weights\n","            optimizer.step()\n","            scheduler.step()\n","            optimizer.zero_grad()\n","\n","            # Increment global step\n","            global_step += 1\n","\n","            # Save checkpoint periodically\n","            if global_step % 100 == 0:\n","                checkpoint_dir = os.path.join(output_dir, f\"checkpoint-{global_step}\")\n","                model.save_pretrained(checkpoint_dir)\n","                print(f\"Saved checkpoint to {checkpoint_dir}\")\n","\n","        # Update tracking metrics\n","        epoch_loss += loss.item() * grad_accumulation_steps\n","        total_loss += loss.item() * grad_accumulation_steps\n","\n","        # Update progress bar\n","        progress_bar.update(1)\n","        progress_bar.set_postfix({\"loss\": epoch_loss / (step + 1)})\n","\n","        # Clear CUDA cache periodically\n","        if step % 500 == 0 and step > 0:\n","            gc.collect()\n","            torch.cuda.empty_cache()\n","\n","    # End of epoch\n","    progress_bar.close()\n","    avg_epoch_loss = epoch_loss / len(train_dataloader)\n","    print(f\"Epoch {epoch+1} completed. Average loss: {avg_epoch_loss:.4f}\")\n","\n","    # Save model at end of epoch\n","    epoch_dir = os.path.join(output_dir, f\"epoch-{epoch+1}\")\n","    model.save_pretrained(epoch_dir)\n","    tokenizer.save_pretrained(epoch_dir)\n","    print(f\"Saved model at end of epoch {epoch+1} to {epoch_dir}\")\n","\n","# End of training\n","print(f\"\\nTraining completed. Average loss: {total_loss / (len(train_dataloader) * num_epochs):.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"29HGm06QeBIb"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"widgets":{"application/vnd.jupyter.widget-state+json":{"000fbab5c4264019a01be791335fc28d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"00afba2659c84efeb0394178d369ce2c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"030b649008bb451694d77d5741b6f1d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd2b51237efe4d3da125ffd83ae71696","placeholder":"​","style":"IPY_MODEL_112cb17bfd85490aba0571bf0b5ee143","value":"Loading checkpoint shards: 100%"}},"0566c57a70b845cda3feafc8174a0104":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"07f9ed2963664b99a85228a31bc13b0a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08053ea2f8df444c9df1d0822c2efb25":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0842ecbcd5c34a34ab66a666bc837434":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d24166ec711435ca53e0f6abf6dde3c","max":116,"min":0,"orientation":"horizontal","style":"IPY_MODEL_17ed2d961a3f44b0a8a68421d0b3ef9b","value":116}},"0a06e2fb938c4317a0871d64c0195ce7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0ae343eb44c14d5b9c0d72aa1e1485af":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0cbcad0a37f74db6b020ba59e5032ed8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_00afba2659c84efeb0394178d369ce2c","placeholder":"​","style":"IPY_MODEL_e2570f5c57d24733a9398856f330b502","value":"tokenizer.model: 100%"}},"0f21ad596f35469382c9cb5a34afa5f6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9694e9d89614df6bd6594d6cca75893","placeholder":"​","style":"IPY_MODEL_b4e962cde98a472f8f4777101055dd7d","value":"config.json: 100%"}},"111630b83f38422f87fd95ef74d2180c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"112cb17bfd85490aba0571bf0b5ee143":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"12903c76c2ed414188c49c7c85973854":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"12e1b1799bfb473a99813719acd57636":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_030b649008bb451694d77d5741b6f1d0","IPY_MODEL_cb984e030eda435ea8b8a1d7d0e58ee5","IPY_MODEL_bac8ed097fcf4d9a9dbaf2307bbaec07"],"layout":"IPY_MODEL_ce644f40f2744f018aff76fc8c202c5d"}},"173b91dc9b244237866692da1a7452c7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"17ed2d961a3f44b0a8a68421d0b3ef9b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"19871a03a2824d01b4a054c3964daf7f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ab7fbbe6b2f495a9f12b2eb3851d76c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0cbcad0a37f74db6b020ba59e5032ed8","IPY_MODEL_2c9b98ef8888475b807c4aaf5ba5263b","IPY_MODEL_d2e6433f5c5042c38216a3056355730e"],"layout":"IPY_MODEL_e60e10a138f3490da1b6cd569ae491d3"}},"1b83ba1506f74950a4f9d6aa136cf034":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f2ec9c292c349bebcb1364c2479ed6b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fd90877809c4d3fa188208d482d471e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0f21ad596f35469382c9cb5a34afa5f6","IPY_MODEL_71f67fb5ef374e968430265697902a2d","IPY_MODEL_43ccb3d88a704af3bb2c5279e35a25f0"],"layout":"IPY_MODEL_9cb4db9595774712a95d7bf07d36c5b9"}},"202bf597fca244eeb1bc1cd07c9bc61e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"22e29a8f9b4c48f8b51146af9e60c2ff":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22fc35b725ed4f0787f6525c198a1ef2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"262d6c0f7d564e2889d7037ec910b459":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"264b2dc6c0074b1ab91a0093183dc8b0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_594fbb00ed1d489a8f092d5f67deb9ae","IPY_MODEL_c9d61939b2af43f383ff75185d300b39","IPY_MODEL_7e3bbb555d524d31b8ad8cc2f8f46619"],"layout":"IPY_MODEL_f2c019dc2037458492c3e321a404e1c2"}},"2770df2a72eb46cba7f6a3f84c66cfee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_71e3f59480ff4dc5ac4f0a2e793ee131","placeholder":"​","style":"IPY_MODEL_50ea2f0b62674f118e0617479729fe96","value":"model-00002-of-00002.safetensors: 100%"}},"282ab77adf624e6487f5b9faf2f145c9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"297a0837f39045319fe339a53b8a7152":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_47c4ad4741d74fdb8eaf1e2323915107","placeholder":"​","style":"IPY_MODEL_e7efd3833a6d4e71863df2ae481f2bbe","value":"model.safetensors.index.json: 100%"}},"2aaa370015fb49e5ac8328353e0bdbfa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ad6cb8b52ad422d8d49bd4f1197f964":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ad7308f3f8e4122be1ed2c6a5ed1343":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2bd2bca8d7ad4804ac2057c650cfecec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2bdf89de641b4515a12970f3bc565463":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2c013ad55c8849d79e4d02f293d94412":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a06e2fb938c4317a0871d64c0195ce7","placeholder":"​","style":"IPY_MODEL_b03dca684c6b4fe2894178f833916d22","value":"model-00002-of-00002.safetensors: 100%"}},"2c9b98ef8888475b807c4aaf5ba5263b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9f2653d840448959b890c7c21089e45","max":493443,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6d1e4d70f4824ee28ae8b596783e904f","value":493443}},"2d0eddb9678e497193bd63f9dbba6fd7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d4777b6a6ac407ea0770d05ca2a0463":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2fbca29b97494d3da19730fa1674d4b6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6895e8451a6f40f2b812b5fb902d9108","placeholder":"​","style":"IPY_MODEL_53130449395d4db88f80464d2a8c10b5","value":" 2/2 [00:59&lt;00:00, 59.15s/it]"}},"305b27e4512343e89cb7dae487a35bab":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"349d455be24b4d439828b7034f073f9a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3566f3fe91c14e37984c27ddbf9005a3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40283a9320a9452bb89bf67633ce4f10","placeholder":"​","style":"IPY_MODEL_12903c76c2ed414188c49c7c85973854","value":"Loading checkpoint shards: 100%"}},"385a3517b7694055a2d546a920702d63":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3961b5a9b2194fae804f74f473d7570a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_403e61620ca6478a86d308d6ae1bb511","IPY_MODEL_a8c27af0ba2446b7badb36141c433678","IPY_MODEL_4298287b165a4c6a8c7618bcbd077990"],"layout":"IPY_MODEL_68a031a5321c4f8aab675d743e0b4194"}},"3bee6fa43d0d43f08317f4d06fa978b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3db18f2bb64d468e8e2030c5e2bc4489":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ed7cfc48a4c4e8c98b506e9f132cdb6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40283a9320a9452bb89bf67633ce4f10":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"403e61620ca6478a86d308d6ae1bb511":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_22e29a8f9b4c48f8b51146af9e60c2ff","placeholder":"​","style":"IPY_MODEL_d7caff773345470fae32963b2d57bbd5","value":"Fetching 2 files: 100%"}},"405049a9c262426994b0b26f21aac54b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4298287b165a4c6a8c7618bcbd077990":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5aab09e802d64408afbd2dc41909f41c","placeholder":"​","style":"IPY_MODEL_8add316c69d54eadb81b4b29dfda9834","value":" 2/2 [00:51&lt;00:00, 51.38s/it]"}},"43c4463f9d5d49a0a21e90a2922134fb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f603f3cc30084b8baa3ab63dd6b13cd4","placeholder":"​","style":"IPY_MODEL_61d1db653a804bcbad06a1aaa30c314a","value":"Loading checkpoint shards: 100%"}},"43ccb3d88a704af3bb2c5279e35a25f0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ae9445a351a4f808289edf526ade9f0","placeholder":"​","style":"IPY_MODEL_a61712fe056d469bb2b2ee0cbe551405","value":" 571/571 [00:00&lt;00:00, 71.3kB/s]"}},"47c4ad4741d74fdb8eaf1e2323915107":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49971dfc54f3474fbabd7643f95e3aa2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_666057b15e2f46d5b107a313d42d6ae4","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7908972bc9884675b584e71a90a73e38","value":2}},"49fbe299f4054b6f9b1697a30c9a254d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bb0acead0dbc4054bf1d0d5cfe0bee53","IPY_MODEL_4cac8726bc1441689305d629aced7432","IPY_MODEL_68894f169f224c76893717dc96acbcea"],"layout":"IPY_MODEL_a97c761270b14fbd956b80a0c617824b"}},"4af55c63d38c42889c3bc368c01331f1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4cac8726bc1441689305d629aced7432":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_668f216dc79845fc90082c8598bafda3","max":2103,"min":0,"orientation":"horizontal","style":"IPY_MODEL_08053ea2f8df444c9df1d0822c2efb25","value":2103}},"4efcc1d3020f4470b266cf71644bad6f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"502d64ae43b5449fac7fb0a616d34b22":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_297a0837f39045319fe339a53b8a7152","IPY_MODEL_fb2a7c9a0fb549dd9f872a317acb95e5","IPY_MODEL_ef6ad329d90345c3908409eb085b3cca"],"layout":"IPY_MODEL_a9eb869104d7405e96ce40abd2ff91da"}},"50ea2f0b62674f118e0617479729fe96":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52b0617b9364437485879fed5f37139b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53130449395d4db88f80464d2a8c10b5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"537476b6435a4c1a9a7ed2f236528e57":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5865ff53ad204e398fcc31d353661e29","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a17733980ea8472c8878a56187a26ea1","value":2}},"549e968c25034051a0604328e9279e9a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"56f504d9ee964609b42f0b56df0b0dd2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_43c4463f9d5d49a0a21e90a2922134fb","IPY_MODEL_e23b4fb1841a43d6ae2d4acd3c663465","IPY_MODEL_6897b5710b9a446bb9d752c2fa4263b8"],"layout":"IPY_MODEL_3ed7cfc48a4c4e8c98b506e9f132cdb6"}},"5865ff53ad204e398fcc31d353661e29":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"589240a331fc4f99ad8263492911573d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"58cd6ff3599d4bddb49a9d3ecd23225b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"594fbb00ed1d489a8f092d5f67deb9ae":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_07f9ed2963664b99a85228a31bc13b0a","placeholder":"​","style":"IPY_MODEL_8a15dbc9558847cfb1c1d64b6a61a4a1","value":"generation_config.json: 100%"}},"5aab09e802d64408afbd2dc41909f41c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b282bc24299412c9898678abb572bed":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f18378affc0b4a36ad62fe46bbf0cbe0","placeholder":"​","style":"IPY_MODEL_2d4777b6a6ac407ea0770d05ca2a0463","value":"model.safetensors.index.json: 100%"}},"5f72a381666c4c9bb6458e4cf0f63298":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_746daa80ded442ecb03865b932b08f2b","placeholder":"​","style":"IPY_MODEL_173b91dc9b244237866692da1a7452c7","value":" 4.54G/4.54G [00:33&lt;00:00, 248MB/s]"}},"61d1db653a804bcbad06a1aaa30c314a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"62930ee9985a46fcab640762a28a1a0d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"659ad925b9e14dc587d8cd54383a468f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"666057b15e2f46d5b107a313d42d6ae4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"668f216dc79845fc90082c8598bafda3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67ec3e58fe6d40eb954d61a824f88764":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68894f169f224c76893717dc96acbcea":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_75b35b9e20cc42f189ad40cb1f0fc7e1","placeholder":"​","style":"IPY_MODEL_cee9a0dde2ec4ef7b42e6dad83e122e0","value":" 2.10k/2.10k [00:00&lt;00:00, 257kB/s]"}},"6895e8451a6f40f2b812b5fb902d9108":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6897b5710b9a446bb9d752c2fa4263b8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4a6353654834f45a01aa324b97d6c3c","placeholder":"​","style":"IPY_MODEL_bced6d64ab8449a98a3d9751db89bdd1","value":" 2/2 [00:02&lt;00:00,  1.29s/it]"}},"68a031a5321c4f8aab675d743e0b4194":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69ddcd6fff75492fbfa0631913b3ecff":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b4a50af84204149a3b33820f248a0b6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bcc7ad499a34d2dbc00e2d045efacce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19871a03a2824d01b4a054c3964daf7f","placeholder":"​","style":"IPY_MODEL_202bf597fca244eeb1bc1cd07c9bc61e","value":" 2/2 [00:17&lt;00:00,  8.10s/it]"}},"6d1e4d70f4824ee28ae8b596783e904f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6d6f11a2854d4cfb87f002cd75d8c8d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6e1901e5291c4ab189349487cf3a7561":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c087096f16874b4290b7766816de3178","IPY_MODEL_91b8a5fc38954c7c81dfc9365cb9972f","IPY_MODEL_c1bb0e4f92cf4827821869c8e84a451d"],"layout":"IPY_MODEL_6b4a50af84204149a3b33820f248a0b6"}},"6f7ae8a0187d4108bc0ed0c4f3e3c625":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71cb62f9b44146f4b2c62f0f913d83dd":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71e3f59480ff4dc5ac4f0a2e793ee131":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71f67fb5ef374e968430265697902a2d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ae343eb44c14d5b9c0d72aa1e1485af","max":571,"min":0,"orientation":"horizontal","style":"IPY_MODEL_74383b60180f4bd79183cafeef38b69f","value":571}},"73d286ecc7754374a76742282182abe6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_67ec3e58fe6d40eb954d61a824f88764","max":25125,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b4dfc7afa28e44f8b0b67c01f0ba8d48","value":25125}},"74383b60180f4bd79183cafeef38b69f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"743c8b2360574e61a6b29d811619a14a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7444377c63994813880f0f470f10c430":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"746daa80ded442ecb03865b932b08f2b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7490b1d2d1bd44359b3260117e5f86d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2770df2a72eb46cba7f6a3f84c66cfee","IPY_MODEL_87972a49b6834c708eedc8eb5957398f","IPY_MODEL_5f72a381666c4c9bb6458e4cf0f63298"],"layout":"IPY_MODEL_de35ef99e93c4e4ab6eb484474f3c5f7"}},"75212ec16971472ab3951d05761f453b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75b35b9e20cc42f189ad40cb1f0fc7e1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"771f598928e3423f931b0a2adf0d07be":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7908972bc9884675b584e71a90a73e38":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"791433f1ee67498aab8604c4f4bb2a3d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a894847dc1643078751dc8b9c564181":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2bda070080146aea8a248a34737690b","placeholder":"​","style":"IPY_MODEL_cbf3344ee6a54075a647e73e28506d93","value":"tokenizer.json: 100%"}},"7ae9445a351a4f808289edf526ade9f0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b1a8018a9d3445c92973aae1e297710":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a7dac64f02bd4715ba21d8ebee16456f","max":9942981696,"min":0,"orientation":"horizontal","style":"IPY_MODEL_589240a331fc4f99ad8263492911573d","value":9942981696}},"7ba72f221a0648959029042fcd66907d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3566f3fe91c14e37984c27ddbf9005a3","IPY_MODEL_49971dfc54f3474fbabd7643f95e3aa2","IPY_MODEL_6bcc7ad499a34d2dbc00e2d045efacce"],"layout":"IPY_MODEL_3db18f2bb64d468e8e2030c5e2bc4489"}},"7e3bbb555d524d31b8ad8cc2f8f46619":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_111630b83f38422f87fd95ef74d2180c","placeholder":"​","style":"IPY_MODEL_6d6f11a2854d4cfb87f002cd75d8c8d9","value":" 116/116 [00:00&lt;00:00, 16.0kB/s]"}},"844fb2dbe0724d9d9838e6b13788e791":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8633d1a8f29241c3a35faa8e534cd0a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"87972a49b6834c708eedc8eb5957398f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_771f598928e3423f931b0a2adf0d07be","max":4540516344,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c4c3a800295544498a98699d73dfef5b","value":4540516344}},"8984bcb2dfd147fb80038daa8498ac27":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a15dbc9558847cfb1c1d64b6a61a4a1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8add316c69d54eadb81b4b29dfda9834":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8c29bf3515f44aa59b90628591145d08":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8d24166ec711435ca53e0f6abf6dde3c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d89a3b943c6467682f4537f916dd23f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f73b0966a54f4b7db04117257c61646c","placeholder":"​","style":"IPY_MODEL_c37f4e128ec34f01b7ba1189eaeecd41","value":" 571/571 [00:00&lt;00:00, 72.4kB/s]"}},"8db3e662f139486b994788bb8dc3bf3c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7a894847dc1643078751dc8b9c564181","IPY_MODEL_ac7638fe7bb54bf9884020d16675aff9","IPY_MODEL_92c92d55fcac4969829e42b7554074d0"],"layout":"IPY_MODEL_2ad6cb8b52ad422d8d49bd4f1197f964"}},"8e91b5d4c95b44faa9130b413a51ed58":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"91a56fba160641d38cb4ad430f2e4024":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c6fd0be956bb4e67b15d4660f9b1a43f","IPY_MODEL_b4d5879255f8459398ba0d7ac9be4452","IPY_MODEL_8d89a3b943c6467682f4537f916dd23f"],"layout":"IPY_MODEL_1b83ba1506f74950a4f9d6aa136cf034"}},"91b8a5fc38954c7c81dfc9365cb9972f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_22fc35b725ed4f0787f6525c198a1ef2","max":9942981696,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2bdf89de641b4515a12970f3bc565463","value":9942981696}},"91bd7169bc54413a935d5a129ebb2384":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_405049a9c262426994b0b26f21aac54b","placeholder":"​","style":"IPY_MODEL_efa540d6d9434a2fbc3dc9da75524ec5","value":" 9.94G/9.94G [00:58&lt;00:00, 243MB/s]"}},"92c92d55fcac4969829e42b7554074d0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d0eddb9678e497193bd63f9dbba6fd7","placeholder":"​","style":"IPY_MODEL_8c29bf3515f44aa59b90628591145d08","value":" 1.80M/1.80M [00:00&lt;00:00, 7.63MB/s]"}},"9399851214fb40799909a0c5eb7e07eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f2ec9c292c349bebcb1364c2479ed6b","placeholder":"​","style":"IPY_MODEL_96ff2624aa6c45f2bcb7e06662d01d38","value":"special_tokens_map.json: 100%"}},"96ff2624aa6c45f2bcb7e06662d01d38":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"99891fee5dea4a0c828a284c156aecef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c2c72f3d53440e3b7fb0f7c2f04b3e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9399851214fb40799909a0c5eb7e07eb","IPY_MODEL_e81ae95c48d54895b77748d145887f8f","IPY_MODEL_c8ee929e264142868a8776cafed21b17"],"layout":"IPY_MODEL_9ed02748d22a43aa8121b17093974312"}},"9cb4db9595774712a95d7bf07d36c5b9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ed02748d22a43aa8121b17093974312":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a17733980ea8472c8878a56187a26ea1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a364ac3947ae4a5abc0ed61e27adbe84":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bee6fa43d0d43f08317f4d06fa978b5","max":4540516344,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2bd2bca8d7ad4804ac2057c650cfecec","value":4540516344}},"a61712fe056d469bb2b2ee0cbe551405":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a7dac64f02bd4715ba21d8ebee16456f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8c27af0ba2446b7badb36141c433678":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f7ae8a0187d4108bc0ed0c4f3e3c625","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c83cc202629f4698adf2f6efb2fa7719","value":2}},"a97c761270b14fbd956b80a0c617824b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9eb869104d7405e96ce40abd2ff91da":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa78ded68d144fbcae450dd5230783b1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac7638fe7bb54bf9884020d16675aff9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_52b0617b9364437485879fed5f37139b","max":1795188,"min":0,"orientation":"horizontal","style":"IPY_MODEL_844fb2dbe0724d9d9838e6b13788e791","value":1795188}},"b03dca684c6b4fe2894178f833916d22":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b26b0f4320a24496a7c16a6368c67950":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2bda070080146aea8a248a34737690b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2dc6d84489c4ac6a390be8baa51c16d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cf1ee0f2698c40bdbd85ab6f1f94edf1","IPY_MODEL_0842ecbcd5c34a34ab66a666bc837434","IPY_MODEL_e50889d4d8a4425680a57d404bd2d86b"],"layout":"IPY_MODEL_385a3517b7694055a2d546a920702d63"}},"b4969fe334c343d6b19eb446414dc164":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b4d5879255f8459398ba0d7ac9be4452":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d8f7a4946eb04b0d949c20afae1ab6a1","max":571,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0566c57a70b845cda3feafc8174a0104","value":571}},"b4dfc7afa28e44f8b0b67c01f0ba8d48":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b4e962cde98a472f8f4777101055dd7d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b52b9a20f2b545c689d9d4cd7c94a531":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b64247d2a080466e9349222342cc9c13":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b8027b18e6f544e1bb267b93f20a09ad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b9694e9d89614df6bd6594d6cca75893":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9f2653d840448959b890c7c21089e45":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba8f12662e44493f913c00efbb1a040f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5b282bc24299412c9898678abb572bed","IPY_MODEL_73d286ecc7754374a76742282182abe6","IPY_MODEL_c13f40c1b3064c16be1ea8edee33ccf8"],"layout":"IPY_MODEL_4af55c63d38c42889c3bc368c01331f1"}},"bac8ed097fcf4d9a9dbaf2307bbaec07":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b52b9a20f2b545c689d9d4cd7c94a531","placeholder":"​","style":"IPY_MODEL_c6170d9782d5417d96885e88151c1e11","value":" 3/3 [00:01&lt;00:00,  2.16it/s]"}},"bb0acead0dbc4054bf1d0d5cfe0bee53":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_75212ec16971472ab3951d05761f453b","placeholder":"​","style":"IPY_MODEL_b64247d2a080466e9349222342cc9c13","value":"tokenizer_config.json: 100%"}},"bb9a970b351c48d5b2b6ff0d217c378d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2c013ad55c8849d79e4d02f293d94412","IPY_MODEL_a364ac3947ae4a5abc0ed61e27adbe84","IPY_MODEL_bc3ef9c61be4481babd58e28d46504c1"],"layout":"IPY_MODEL_b26b0f4320a24496a7c16a6368c67950"}},"bc2be97ff9e6412db9f42b6feb213a1d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc3ef9c61be4481babd58e28d46504c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eeb12acb25fa4dcab7429df8aabf3d9e","placeholder":"​","style":"IPY_MODEL_99891fee5dea4a0c828a284c156aecef","value":" 4.54G/4.54G [00:27&lt;00:00, 265MB/s]"}},"bc78026910314080873bd06b0b4934ce":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bced6d64ab8449a98a3d9751db89bdd1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c087096f16874b4290b7766816de3178":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa78ded68d144fbcae450dd5230783b1","placeholder":"​","style":"IPY_MODEL_b8027b18e6f544e1bb267b93f20a09ad","value":"model-00001-of-00002.safetensors: 100%"}},"c0e14ad7daed451cafe97f8ac1cc9d46":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c13f40c1b3064c16be1ea8edee33ccf8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_305b27e4512343e89cb7dae487a35bab","placeholder":"​","style":"IPY_MODEL_8633d1a8f29241c3a35faa8e534cd0a2","value":" 25.1k/25.1k [00:00&lt;00:00, 2.86MB/s]"}},"c1986e65eba64f008ffdae264c92f904":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c1bb0e4f92cf4827821869c8e84a451d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f897f199b876470c8a11ef6aef3aec4d","placeholder":"​","style":"IPY_MODEL_8e91b5d4c95b44faa9130b413a51ed58","value":" 9.94G/9.94G [00:51&lt;00:00, 253MB/s]"}},"c32b752bc45d44d080be128d0d94ed82":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c37f4e128ec34f01b7ba1189eaeecd41":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c4c3a800295544498a98699d73dfef5b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c532da6460c94acf88b07eb88b37687f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4969fe334c343d6b19eb446414dc164","placeholder":"​","style":"IPY_MODEL_fbc92ca12e69442eb2f58f9b028cd945","value":"Fetching 2 files: 100%"}},"c6170d9782d5417d96885e88151c1e11":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c6fd0be956bb4e67b15d4660f9b1a43f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_69ddcd6fff75492fbfa0631913b3ecff","placeholder":"​","style":"IPY_MODEL_282ab77adf624e6487f5b9faf2f145c9","value":"config.json: 100%"}},"c83cc202629f4698adf2f6efb2fa7719":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c8ee929e264142868a8776cafed21b17":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff1dc317a75c4b418e67433b29f8ca7f","placeholder":"​","style":"IPY_MODEL_58cd6ff3599d4bddb49a9d3ecd23225b","value":" 414/414 [00:00&lt;00:00, 51.0kB/s]"}},"c95e6b746cf1463783536f0f5dd84713":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9d61939b2af43f383ff75185d300b39":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_743c8b2360574e61a6b29d811619a14a","max":116,"min":0,"orientation":"horizontal","style":"IPY_MODEL_262d6c0f7d564e2889d7037ec910b459","value":116}},"cb984e030eda435ea8b8a1d7d0e58ee5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc2be97ff9e6412db9f42b6feb213a1d","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_659ad925b9e14dc587d8cd54383a468f","value":3}},"cbf3344ee6a54075a647e73e28506d93":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ce31bf68bdbc4597959d8cb603ee6744":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c532da6460c94acf88b07eb88b37687f","IPY_MODEL_537476b6435a4c1a9a7ed2f236528e57","IPY_MODEL_2fbca29b97494d3da19730fa1674d4b6"],"layout":"IPY_MODEL_71cb62f9b44146f4b2c62f0f913d83dd"}},"ce644f40f2744f018aff76fc8c202c5d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cee9a0dde2ec4ef7b42e6dad83e122e0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cf1ee0f2698c40bdbd85ab6f1f94edf1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_62930ee9985a46fcab640762a28a1a0d","placeholder":"​","style":"IPY_MODEL_c32b752bc45d44d080be128d0d94ed82","value":"generation_config.json: 100%"}},"d2e6433f5c5042c38216a3056355730e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8984bcb2dfd147fb80038daa8498ac27","placeholder":"​","style":"IPY_MODEL_2aaa370015fb49e5ac8328353e0bdbfa","value":" 493k/493k [00:00&lt;00:00, 5.68MB/s]"}},"d7caff773345470fae32963b2d57bbd5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d8f7a4946eb04b0d949c20afae1ab6a1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de35ef99e93c4e4ab6eb484474f3c5f7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e23b4fb1841a43d6ae2d4acd3c663465":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4efcc1d3020f4470b266cf71644bad6f","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_549e968c25034051a0604328e9279e9a","value":2}},"e2570f5c57d24733a9398856f330b502":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e480fee0c6ea46f4bfa21d5e67b10b2d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c95e6b746cf1463783536f0f5dd84713","placeholder":"​","style":"IPY_MODEL_349d455be24b4d439828b7034f073f9a","value":"model-00001-of-00002.safetensors: 100%"}},"e50889d4d8a4425680a57d404bd2d86b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea2ec46b51e04a4a8e17415e99c2d1f9","placeholder":"​","style":"IPY_MODEL_000fbab5c4264019a01be791335fc28d","value":" 116/116 [00:00&lt;00:00, 14.5kB/s]"}},"e60e10a138f3490da1b6cd569ae491d3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7efd3833a6d4e71863df2ae481f2bbe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e81ae95c48d54895b77748d145887f8f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc78026910314080873bd06b0b4934ce","max":414,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c1986e65eba64f008ffdae264c92f904","value":414}},"ea2ec46b51e04a4a8e17415e99c2d1f9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eeb12acb25fa4dcab7429df8aabf3d9e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef6ad329d90345c3908409eb085b3cca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_791433f1ee67498aab8604c4f4bb2a3d","placeholder":"​","style":"IPY_MODEL_c0e14ad7daed451cafe97f8ac1cc9d46","value":" 25.1k/25.1k [00:00&lt;00:00, 2.88MB/s]"}},"efa540d6d9434a2fbc3dc9da75524ec5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f18378affc0b4a36ad62fe46bbf0cbe0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2c019dc2037458492c3e321a404e1c2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4a6353654834f45a01aa324b97d6c3c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f603f3cc30084b8baa3ab63dd6b13cd4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f73b0966a54f4b7db04117257c61646c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f897f199b876470c8a11ef6aef3aec4d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fad1bfa6ab1c4bbfa6a576b3e4d27ed2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e480fee0c6ea46f4bfa21d5e67b10b2d","IPY_MODEL_7b1a8018a9d3445c92973aae1e297710","IPY_MODEL_91bd7169bc54413a935d5a129ebb2384"],"layout":"IPY_MODEL_2ad7308f3f8e4122be1ed2c6a5ed1343"}},"fb2a7c9a0fb549dd9f872a317acb95e5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7444377c63994813880f0f470f10c430","max":25125,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fbe255c006664403811c3119c46cda0c","value":25125}},"fbc92ca12e69442eb2f58f9b028cd945":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fbe255c006664403811c3119c46cda0c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fd2b51237efe4d3da125ffd83ae71696":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff1dc317a75c4b418e67433b29f8ca7f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57e76f642aa1462286f414e7bea8fbc2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4ed764a3d85b47aca7efc5e211f4310b","IPY_MODEL_f43294d261ad44079e629ac7e6230fe2","IPY_MODEL_ff84ef2b27874632ad20e5c67f147642"],"layout":"IPY_MODEL_f3ebbaa8308448f59701026b6e3bb2be"}},"4ed764a3d85b47aca7efc5e211f4310b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b8feb8f2cd744a384b2209cd521a7f1","placeholder":"​","style":"IPY_MODEL_6aad7e933ade46208bc4f5d5e445189f","value":"config.json: 100%"}},"f43294d261ad44079e629ac7e6230fe2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_37cd8e0fc3d946d19e67f0837ddb0cfa","max":571,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fbecc65cb11e49e1b6a938c9c1d33495","value":571}},"ff84ef2b27874632ad20e5c67f147642":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c5001403647468baf40f3baf7e7de7f","placeholder":"​","style":"IPY_MODEL_1ff99957d87744e89b551e4591cc506b","value":" 571/571 [00:00&lt;00:00, 62.7kB/s]"}},"f3ebbaa8308448f59701026b6e3bb2be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b8feb8f2cd744a384b2209cd521a7f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6aad7e933ade46208bc4f5d5e445189f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"37cd8e0fc3d946d19e67f0837ddb0cfa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbecc65cb11e49e1b6a938c9c1d33495":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8c5001403647468baf40f3baf7e7de7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ff99957d87744e89b551e4591cc506b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"53ac75ce9e36472d9927f081bdea63cb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f7df2463aaf645af8272efb3b6005272","IPY_MODEL_dd8c6be9107341929774d33c1bb8eaab","IPY_MODEL_76f641deb3664f3bacae3db0207f354a"],"layout":"IPY_MODEL_550e741da721446d9054c8791fe80226"}},"f7df2463aaf645af8272efb3b6005272":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_869193acdbe64fef90244877113e8acf","placeholder":"​","style":"IPY_MODEL_9e06c025a31a4d618c046fe0ed346536","value":"model.safetensors.index.json: 100%"}},"dd8c6be9107341929774d33c1bb8eaab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7ebf961332e84f30a5ac2460e159f10f","max":25125,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a0ace417ec6546cc82b7a0707b0a7568","value":25125}},"76f641deb3664f3bacae3db0207f354a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e8350f7a6f6849c2b3ed6e554ac47f39","placeholder":"​","style":"IPY_MODEL_d5417346ff71485aa4a20ccab21fa863","value":" 25.1k/25.1k [00:00&lt;00:00, 2.57MB/s]"}},"550e741da721446d9054c8791fe80226":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"869193acdbe64fef90244877113e8acf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e06c025a31a4d618c046fe0ed346536":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7ebf961332e84f30a5ac2460e159f10f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0ace417ec6546cc82b7a0707b0a7568":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e8350f7a6f6849c2b3ed6e554ac47f39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5417346ff71485aa4a20ccab21fa863":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"171c55a07689492683d628186000ad9a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_efb94644f94c4b918288b446113d71e9","IPY_MODEL_ede67b2da89f496c8b8ea215381dd033","IPY_MODEL_516dba2f22574e9cbccf4a8805c2a1fc"],"layout":"IPY_MODEL_ae5fe91105a0415fa45a9d777b531561"}},"efb94644f94c4b918288b446113d71e9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b463641b455447039c543cd946e41e54","placeholder":"​","style":"IPY_MODEL_1516acf51bc44e51a95648f065cb11fa","value":"Fetching 2 files: 100%"}},"ede67b2da89f496c8b8ea215381dd033":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_05103c7ac83f4de3b77d3f3c6f35343b","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_079ed7cfb8314fceb0eb5ee97217dc93","value":2}},"516dba2f22574e9cbccf4a8805c2a1fc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f531854dc6b942b8ad7ada5212535615","placeholder":"​","style":"IPY_MODEL_79873f151e074c2ab0d9af4abc2b815e","value":" 2/2 [00:44&lt;00:00, 44.96s/it]"}},"ae5fe91105a0415fa45a9d777b531561":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b463641b455447039c543cd946e41e54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1516acf51bc44e51a95648f065cb11fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"05103c7ac83f4de3b77d3f3c6f35343b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"079ed7cfb8314fceb0eb5ee97217dc93":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f531854dc6b942b8ad7ada5212535615":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79873f151e074c2ab0d9af4abc2b815e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8110f7f5d424481dadc6c0293632109b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b4fbfeb442174b6395fd3a9d31e78b9d","IPY_MODEL_45f8616d6f4241f7931d27e45dc1477b","IPY_MODEL_7fbe3ab6050f4d4189e6fc08363d81a1"],"layout":"IPY_MODEL_18d2548f09fe4e888f6ef6bedaa72aea"}},"b4fbfeb442174b6395fd3a9d31e78b9d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6db982977a4f4fafbb8e890cdb17b68f","placeholder":"​","style":"IPY_MODEL_dd87f2dfbda74d939fe2679b5e190e80","value":"model-00002-of-00002.safetensors: 100%"}},"45f8616d6f4241f7931d27e45dc1477b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_934d24ceb6b14326a973d7da34d56e14","max":4540516344,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6081b74a7a124e36aca740e89012a14b","value":4540516344}},"7fbe3ab6050f4d4189e6fc08363d81a1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a7076f701da41eeaf41dacc0a9181f2","placeholder":"​","style":"IPY_MODEL_99791a5b668b473c9d89b81c1d5e0d61","value":" 4.54G/4.54G [00:25&lt;00:00, 143MB/s]"}},"18d2548f09fe4e888f6ef6bedaa72aea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6db982977a4f4fafbb8e890cdb17b68f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd87f2dfbda74d939fe2679b5e190e80":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"934d24ceb6b14326a973d7da34d56e14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6081b74a7a124e36aca740e89012a14b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2a7076f701da41eeaf41dacc0a9181f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99791a5b668b473c9d89b81c1d5e0d61":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"00d2fde3440e4ce9ab7bb192844df86c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a235c24fc0a54a8ca862fad5cd2628d7","IPY_MODEL_06e5a1455128437680f9ddcc4f6d5d8f","IPY_MODEL_4521c387d5d14183b8f9d12622e5babd"],"layout":"IPY_MODEL_2d26084c66a1406b822a75b7762f93e9"}},"a235c24fc0a54a8ca862fad5cd2628d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_208402e4aa9c43518bfde6a8c7fd5c8e","placeholder":"​","style":"IPY_MODEL_06ee60dd2d004ad4a7323eafef2b61f6","value":"model-00001-of-00002.safetensors: 100%"}},"06e5a1455128437680f9ddcc4f6d5d8f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd1a1637768443b880f85f043fbf0b09","max":9942981696,"min":0,"orientation":"horizontal","style":"IPY_MODEL_82e321797fe54f329a86625abe5dd659","value":9942981696}},"4521c387d5d14183b8f9d12622e5babd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd773445f75c45e68cef6b31639957af","placeholder":"​","style":"IPY_MODEL_84603fa02e6646ebba3e8d1c1f9a22cc","value":" 9.94G/9.94G [00:44&lt;00:00, 399MB/s]"}},"2d26084c66a1406b822a75b7762f93e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"208402e4aa9c43518bfde6a8c7fd5c8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06ee60dd2d004ad4a7323eafef2b61f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dd1a1637768443b880f85f043fbf0b09":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82e321797fe54f329a86625abe5dd659":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dd773445f75c45e68cef6b31639957af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84603fa02e6646ebba3e8d1c1f9a22cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bbb40760b25842eba2faf41f092429f2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_db74ed0735c44376a8ad184236a9b862","IPY_MODEL_a3ae599d752242c9a047012cc725606a","IPY_MODEL_9e129efce33246f4b52ec887f61956aa"],"layout":"IPY_MODEL_8bb957cc36d5445b8090e8450ab436db"}},"db74ed0735c44376a8ad184236a9b862":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6fef3e69e38435bac7d7887c0fb17ef","placeholder":"​","style":"IPY_MODEL_ef53355a1c5648279c4bfc987f9164be","value":"Loading checkpoint shards: 100%"}},"a3ae599d752242c9a047012cc725606a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9eda7d6c71224b63871b5e8ec6ea3d0f","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f907fae1c2e74dab8439be26d0e46138","value":2}},"9e129efce33246f4b52ec887f61956aa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2b1c4797827417aba95652af02970c2","placeholder":"​","style":"IPY_MODEL_ce619f7c363743a1a1f851dc4aca58d5","value":" 2/2 [00:04&lt;00:00,  1.93s/it]"}},"8bb957cc36d5445b8090e8450ab436db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6fef3e69e38435bac7d7887c0fb17ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef53355a1c5648279c4bfc987f9164be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9eda7d6c71224b63871b5e8ec6ea3d0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f907fae1c2e74dab8439be26d0e46138":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e2b1c4797827417aba95652af02970c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce619f7c363743a1a1f851dc4aca58d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0c13050e213f420587733f5c48ff6c45":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a91caf45c46d486fabed3bc98b660584","IPY_MODEL_9c5d23db00d14c7885f59dfe5f261f25","IPY_MODEL_5bf1d2d4dbd04ea4aa98f00fc23a480f"],"layout":"IPY_MODEL_fb1ab124747c4f33a0bd9bd0b2d09cec"}},"a91caf45c46d486fabed3bc98b660584":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_23083db9243546e2b8fbefb1113a826b","placeholder":"​","style":"IPY_MODEL_b12aa2ed67d04c1c8c31e624081cb16c","value":"generation_config.json: 100%"}},"9c5d23db00d14c7885f59dfe5f261f25":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_414480d2fd6c44ec943882b87fcaa71d","max":116,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f2d64957e8c3424d831e1233b70b2062","value":116}},"5bf1d2d4dbd04ea4aa98f00fc23a480f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e2175fc59cf458ab55cc0c5f7d940c0","placeholder":"​","style":"IPY_MODEL_d4d5ba6783994e57805cb0f81bcd12d3","value":" 116/116 [00:00&lt;00:00, 14.8kB/s]"}},"fb1ab124747c4f33a0bd9bd0b2d09cec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23083db9243546e2b8fbefb1113a826b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b12aa2ed67d04c1c8c31e624081cb16c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"414480d2fd6c44ec943882b87fcaa71d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2d64957e8c3424d831e1233b70b2062":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6e2175fc59cf458ab55cc0c5f7d940c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4d5ba6783994e57805cb0f81bcd12d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4a3f359de7f544079ffe42da9391171f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ab9aefd400a148ec81bbc07be130c8ff","IPY_MODEL_ed8aab732d934226ae5c2d771d4fadf6","IPY_MODEL_b66771093eff4c7590a216d55f14b5e5"],"layout":"IPY_MODEL_3ce709d2010f40fcbbcf9c4193668f61"}},"ab9aefd400a148ec81bbc07be130c8ff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_16520616b28943728f42c4bdc80e415c","placeholder":"​","style":"IPY_MODEL_1ac6c7ec63c94ca1b59d34bfda2e85d5","value":"tokenizer_config.json: 100%"}},"ed8aab732d934226ae5c2d771d4fadf6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0dfe83bcd9b044979f8c4a204ab7ea17","max":2103,"min":0,"orientation":"horizontal","style":"IPY_MODEL_35f1587df63f49e795627c80c3da998d","value":2103}},"b66771093eff4c7590a216d55f14b5e5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_06f4439d8056410981fce1d63bb0dc2d","placeholder":"​","style":"IPY_MODEL_c0fb6d4abf8f40bcae015c9df32297f9","value":" 2.10k/2.10k [00:00&lt;00:00, 248kB/s]"}},"3ce709d2010f40fcbbcf9c4193668f61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16520616b28943728f42c4bdc80e415c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ac6c7ec63c94ca1b59d34bfda2e85d5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0dfe83bcd9b044979f8c4a204ab7ea17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35f1587df63f49e795627c80c3da998d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"06f4439d8056410981fce1d63bb0dc2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0fb6d4abf8f40bcae015c9df32297f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd748f04f3bd48f28de2146c328fb9d7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_91434fd6bc3e4080b7c801b9d9a20bfb","IPY_MODEL_acdd42f714fe4fa288b7ae18963e1a47","IPY_MODEL_e85c5698c8ca4139a77fc42f8b8513ff"],"layout":"IPY_MODEL_d51026308aa042f7804dbccb14717bbe"}},"91434fd6bc3e4080b7c801b9d9a20bfb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9b92a25ccc149ad8357246e60e8acee","placeholder":"​","style":"IPY_MODEL_0bd83c226c53485ab87a503370462de7","value":"tokenizer.model: 100%"}},"acdd42f714fe4fa288b7ae18963e1a47":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c140af28974443e80b077821928691f","max":493443,"min":0,"orientation":"horizontal","style":"IPY_MODEL_53000cde2e3e40ebb27925ec54f5fd54","value":493443}},"e85c5698c8ca4139a77fc42f8b8513ff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_32c68e9ef8c941b08a4a0ee021b7161a","placeholder":"​","style":"IPY_MODEL_7540f59215ca4d589cad8bc876bf59d3","value":" 493k/493k [00:00&lt;00:00, 18.5MB/s]"}},"d51026308aa042f7804dbccb14717bbe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9b92a25ccc149ad8357246e60e8acee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bd83c226c53485ab87a503370462de7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6c140af28974443e80b077821928691f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53000cde2e3e40ebb27925ec54f5fd54":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"32c68e9ef8c941b08a4a0ee021b7161a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7540f59215ca4d589cad8bc876bf59d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"816a9c5ce7f147e2829a8d797a827222":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_184fa66ad80540c3951e9de0b3f8376e","IPY_MODEL_4e84dc56235341b49c9c2bed8a1d86e4","IPY_MODEL_6ca722bf80cd403eb689d0302f52a28b"],"layout":"IPY_MODEL_d1a452030d63469e85b34a7aff108435"}},"184fa66ad80540c3951e9de0b3f8376e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b14d65eb4b1d4b72b085b9d5e36359ec","placeholder":"​","style":"IPY_MODEL_945d37cdf60743559ae91a2940b130ba","value":"tokenizer.json: 100%"}},"4e84dc56235341b49c9c2bed8a1d86e4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3223638fcb44f3880d4c4b20c0eda27","max":1795188,"min":0,"orientation":"horizontal","style":"IPY_MODEL_41b23cff945b44c4a89ba37188e34090","value":1795188}},"6ca722bf80cd403eb689d0302f52a28b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_588f201fd5cc4090af398a1091f21be5","placeholder":"​","style":"IPY_MODEL_0430e059cf2e4871a8a983b4195d51f6","value":" 1.80M/1.80M [00:00&lt;00:00, 44.2MB/s]"}},"d1a452030d63469e85b34a7aff108435":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b14d65eb4b1d4b72b085b9d5e36359ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"945d37cdf60743559ae91a2940b130ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3223638fcb44f3880d4c4b20c0eda27":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41b23cff945b44c4a89ba37188e34090":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"588f201fd5cc4090af398a1091f21be5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0430e059cf2e4871a8a983b4195d51f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e33c008d387a4741bbe6b1d3fd1c1f54":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_469bf1a1ff7f403fa090db0c29677784","IPY_MODEL_bf7052dd23e945889ae09367e07b7d1f","IPY_MODEL_9bb5bac46f8b4b7c853010b5b529ebd1"],"layout":"IPY_MODEL_c11c416041e54bc1abb0250fb754d78f"}},"469bf1a1ff7f403fa090db0c29677784":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_76397838ca244ebbb101fef83b921190","placeholder":"​","style":"IPY_MODEL_d56d2d84b4f44e03b23b4e9d1928d07c","value":"special_tokens_map.json: 100%"}},"bf7052dd23e945889ae09367e07b7d1f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b4b31fc1a0d4270a90c98b498feb09b","max":414,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c30ce2d604cf4b10a6abeb86055463ac","value":414}},"9bb5bac46f8b4b7c853010b5b529ebd1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc3ca9901d504a29b9ed19b570f11d61","placeholder":"​","style":"IPY_MODEL_37e497f27de44846a34ae3bd46fed312","value":" 414/414 [00:00&lt;00:00, 41.4kB/s]"}},"c11c416041e54bc1abb0250fb754d78f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76397838ca244ebbb101fef83b921190":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d56d2d84b4f44e03b23b4e9d1928d07c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8b4b31fc1a0d4270a90c98b498feb09b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c30ce2d604cf4b10a6abeb86055463ac":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fc3ca9901d504a29b9ed19b570f11d61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37e497f27de44846a34ae3bd46fed312":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}